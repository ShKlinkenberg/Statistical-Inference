# Moderation with Regression Analysis {#moderation}
> Key concepts: interaction variable, covariate, outcome, regression equation, dummy variables, normally distributed residuals, linearity, homoscedasticity, independent observations, statistical diagram, common support, simple slope, conditional effect, mean-centering.

### Summary {-}

The linear regression model is a powerful and very popular model for predicting a numeric outcome variable from one or more predictor variables. Predictor variables must be numeric or dichotomies. Regression coefficients show the predicted difference in the outcome for a one unit difference in the predictor. 

But what if this predictive effect is not the same in all contexts? For example, exposure to an anti-smoking campaign may generally generate a more negative attitude towards smoking. The effect, however, is probably different for people who smoke than for people who do not smoke. The effect of campaign exposure on attitude towards smoking is moderated by the context: Whether or not the person exposed to the campaign is a smoker.

Different effect sizes for different contexts require different regression coefficients: regression lines with different slopes for different groups of people. We can use an interaction variable as a predictor in a regression model to accommodate for moderation as different slopes. An interaction variable is just the product of the predictor and moderator variables.

As a predictor in the model, an interaction variable has a confidence interval and a p value. The confidence interval tells us the plausible values for the size of the interaction effect in the population. The p value tests the null hypothesis that there is no interaction effect at all in the population.

To interpret the interaction effect, we must determine the size of the effect of the predictor on the outcome variable for several interesting values of the moderator. If the moderator is categorical, we want to know the effect (simple slope) within each category of the moderator. For example, the effect of campaign exposure on smoking attitude for smokers and the effect for non-smokers. If the moderator is a continuous variable, we may look at the effect for the mean value of the moderator (moderate score level) and one standard deviation below (low level) or above (high level) the mean.

```{r SPSS-PROCESS, eval=FALSE, echo=FALSE}
# TERMINOLOGY: predictor (X, cause), moderator (M, represents context), covariate (C, control), outcome (Y).

# SPSS versus PROCESS:
# + SPSS: visual checks on residuals: normal distribution and zpred by zresid (linearity, homoscedasticity)
# + SPSS: scatterplot (X, Y) with regression lines per group (Moderator) with original variable and value labels, showing common support ; add reference line for each group manually specifying the regression equation, setting covariates to their mean values (with categorical moderator and no covariates or covariates that are not correlated with the predictor, Regression Variable Plots can be used)
# - SPSS: manual entering of regression equation with selected values for covariates (and a continuous moderator; lines can only be labeled with the equation text)
# - SPSS: interaction predictors have to be created by hand (also multiple interaction variables for a categorical predictor; Transform>Create Dummy Variables, taught in RMCS?)
# - SPSS: mean-centering must be done by hand
# - SPSS: statistical inference for non-zero moderator values requires separate regression models where the low category requires ADDING one SD instead of subtracting.
# + PROCESS: must be used anyway for mediation models
# - PROCESS: no visual checks on assumptions
# - PROCESS: no visual impression of common support of predictor for different values of the moderator (requires additional work with continuous moderator also in SPSS)
# - PROCESS: data list for visualization of results must be copied from output to syntax file, variable and value labels must be added, lines must be added (and this requires that the moderator has no decimal places in SPSS?) in chart editor
# - PROCESS: model number must be remembered
# - PROCESS: because the student need not create the interaction variables, mean-center or "re-center" for probing the interaction, PROCESS output is more mysterious (but the estimated slopes for different moderator values are directly linked to the graph)
# - PROCESS: dichotomies are automatically treated as indicator variables but categorical predictors/moderators are treated as numeric ; it is not possible to use more than one moderator variable, so PROCESS cannot handle a categorical moderator.
# DECISION: Use PROCESS for results and interpretation. Check assumptions with SPSS (interaction variables and, possibly, dummies must be created but no need for mean-centering) or forget about assumptions.
```

### Test your intuition and understanding {-}

```{r moderator-overview, fig.cap="How does moderation work in a regression model?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="440px"}
# Use app continuous-moderator.
knitr::include_app("http://82.196.4.233:3838/apps/continuous-moderator/", height="550px")
```

1. What does the red line in Figure \@ref(fig:moderator-overview) mean?
```{r eval=FALSE}
* The (red) regression line represents the (estimated) predictive effect of
exposure on attitude for a particular value of contact (with smokers).
*Contact is a moderator of the effect of exposure on attitude. In the initial
plot after loading the app, the value of contact is zero, so the regression
line expresses the predictive effect of exposure on attitude for respondents
who have no contact with smokers.
```

2. What happens if you change the position on the slider? Explain you answer.
```{r eval=FALSE}
* A change of the slider changes the moderator value, so the regression line
is re-estimated for respondents with another number of contacts with smokers.
As a result, the regression line is redrawn (the previous regression line is
shown in grey).
* Because the regression line represents respondents with a different
moderator score, the regression line is based on other observations (dots in
the plot). The observations with scores closest to the selected moderator
value are coloured blue. Changing the moderator value changes the relevant
observations.
* If the moderator value increases, the regression line's decrease is less
steep and at some point changes into an increase from left to right.
```

3. Why does _contact_ (with smokers) appear in between brackets together with the regression coefficient for exposure in the regression equation?
```{r eval=FALSE}
* Due to the interaction effect between exposure and contact in the model, the
predictive effect of exposure depends on the respondent's contact score. For
this reason, the respondent's contact score and its interaction regression
coefficient are included in the (conditional) predictive effect of exposure.
Thus, contact adds to (or subtracts from) the predictive effect of exposure.
* The slope of the regression line becomes less negative or more positive for
higher moderator (contact) values because the interaction effect is positive
(0.04). Every addition unit on the moderator adds 0.04 to the regression slope
of the conditional effect of exposure.
```

4. Which of the regression coefficients represent(s) a partial effect and which a conditional effect? Explain your answer.
```{r eval=FALSE}
* In this regression equation, all regression coefficients represent a partial
and a conditional effect.
* The effects are partial because the multiple regression model controls each
effect for all other effects.
* The effects are conditional because the effect of exposure on attitude
represents the effect for one value of the moderator variable contact.
* But moderation is symmetrical in the sense that we can also see exposure as
moderator of the effect of contact (join the interaction effect between
brackets with the contact effect), so the contact effect (b2) is the
predictive effect of contact for respondents scoring zero on the exposure
predictor.
```

5. What is the null hypothesis of a significance test on the interaction effect ($b_3$)? 
```{r eval=FALSE}
* The null hypothesis of an interaction effect in a (multiple) regression
model is that there is no interaction effect between these predictors at all
in the population.
* In other words, the null hypothesis states that the effect of a predictor is
the same at all levels of the other predictor(s) (included in the interaction
effect) in the population.
```

## The Regression Equation {#regression-equation}

In the social sciences, we usually expect that a particular outcome has several causes. Investigating the effects of an anti-smoking campaign, for example, we would not assume that a person's attitude towards smoking depends only on exposure to a particular anti-smoking campaign. It is easy to think of other and perhaps more influential causes such as personal smoking status, contact with people who do or do not smoke, susceptibility to addiction, and so on.

```{r concept-smoke, echo=FALSE, fig.asp=0.4, fig.cap="A conceptual model with some hypothesized causes of attitude towards smoking."}
# Draw conceptual diagram: Attitude towards smoking predicted by Exposure, Smoking status, and Contact with smokers.
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(0.3, 0.3, 0.3, 0.7), 
                        y = c(.1, .3, .5, .3),
                        label = c("Exposure", "Smoking Status", "Contact with Smokers", "Attitude"))
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$x[4] - 0.04, yend = variables$y[4] - 0.02), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$x[4] - 0.04, yend = variables$y[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[3], y = variables$y[3], xend = variables$x[4] - 0.04, yend = variables$y[4] + 0.02), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.05, 0.55)) +
  theme_void()
# Cleanup.
rm(variables)
```

Figure \@ref(fig:concept-smoke) summarizes some hypothesized causes of the attitude towards  smoking. A regression model translates this conceptual diagram into a statistical model. The statistical regression model is a mathematical function with the outcome variable (also known as the dependent variable, usually referred to with the letter $y$) as the sum of a constant ($a$), the effects ($b$) of predictors ($x$), which are _predictive effects_, and an error term ($e$), which is also called the _residuals_, see Equation \@ref(eq:regression).

$$ 
\small
\begin{align}
  y = a + b_1*x_1 + b_2*x_2 + b_3*x_3 + e 
  (\#eq:regression) 
\end{align}
\normalsize
$$

### Interpretion of a regression equation

```{r multiple-regression, fig.cap="What are the function and meaning of the constant and regression weights?", eval=FALSE, echo=FALSE}
#DROPPED.

# Goal: Refresh function and interpretation of constant and regression weights in a multiple regression model.
# Generate data (N = 40) for a regression model predicting attitude with the parameters as displayed in the inputs, e.g., constant = 0.4, b1 (exposure) = -0.6, b2 (smoking status) = 1.6, b3 (smoker contact) = 0.2 and an error term rnorm(0, 1). Range for attitude is [-5, +5], range for exposure and contact is [0, 10], values for smokings status is 0 (non-smoker) and 1 (smoker). Draw a scatterplot for each attitude (Y) by each of the predictors with the regression line for the predictor (use the parameter setting for the constant and its slope and set the other predictors to zero). Allow the user to change each parameter within a reasonable range (slider?) and update all three scatterplots (don't update the simulated data). It would be nice if the original regression lines remain visible (gray) for comparison.

1. What does the constant mean and how do the regression lines change if you change the value of the constant? Use the regression equation to explain your answer.

2. What does a regression weight ($b$) mean and how does the regression line change if you change its value? Again, explain your answer.
```

```{r regression-coefficients, eval=FALSE, fig.cap="What is the meaning of the regression equation?"}
# REPLACED BY STATIC IMAGE

# Goal: Refresh interpretation of the unstandardized and standardized regression coefficient by manipulating the standard deviation of the outcome variable (minimum value is sd(x) * |b|).
# Generate data (N = 40) from a regression model (see below). Plot attitude (y) against (x), add regression line (fixed slope), and vertical lines from x = 5 and x = 6 to regression line and horizontal lines from where they meet the regression lines to the Y axis. The sd of the error term can be scaled (s) in the range [.5, 2] (larger error produces larger SD of y). Calculate the standardized coefficient (beta) and display as label in plot. Display the regression equation (y = 2.4 + -0.6*x + e). If the scale of the error term is changed, update the dots in the scatterplot and beta.
# # Generate predictor.
# x <- seq(from = 0.5, to = 9.5, length.out = 20)
# # Generate random errors.
# set.seed(1272)
# e <- rnorm(20, mean = 0, sd = 0.3)
# e <- sign(e) * sqrt(abs(e*abs(e) - mean(e*abs(e)))) # center in squares
# # Set scale of error term.
# s <- 1
# # Generate outcome scaled by unstandardized regression coefficient.
# y <- mean(x)/2 + -0.6*x + s*e
# # Calculate (approximate) standardized regression coefficient.
# beta = round(-0.6 + sd(x) / sd(y), digits = 2)

  source("../apps/plottheme/styling.R", local = TRUE)
  #CREATE PREDICTOR
  n <- 100 #number of data points
  const = 1.4 #regression constant
  expo = -0.25 #regression coefficient
  set.seed(4932)
  exposure <- runif(n) * 10
  # Create outcome.
  set.seed(390)
  attitude <- expo * exposure + rnorm(n, mean = const, sd = 1.4)
  # Collect in data frame.
  scatter <- data.frame(attitude = attitude, exposure = exposure)
  #PLOT
  ggplot(scatter[scatter$attitude > -5,]) +
      geom_point(shape = 21,
                 size = 3,
                 aes(x = exposure,
                     y = attitude)) +
      geom_segment(aes(x = 0, xend = 10, y = const, yend = (const + (10 * expo))),
                   size = 1.6, colour = brewercolors["Blue"]) +
      geom_segment(aes(x = 5, xend = 5, y = -5, yend = (const + 5*expo)),
                   size = 1) +
      geom_segment(aes(x = 0, xend = 5, y = (const + 5*expo), yend = (const + 5*expo)),
                   size = 1) +
      geom_segment(aes(x = 4, xend = 4, y = -5, yend = (const + 4*expo)),
                   size = 1) +
      geom_segment(aes(x = 0, xend = 4, y = (const + 4*expo), yend = (const + 4*expo)),
                   size = 1) +
      scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, 1), expand = c(0,0)) +
      scale_y_continuous(limits = c(-5, 5), breaks = c(-5, (const + 5*expo), (const + 4*expo), const, 5), expand = c(0,0)) +
      geom_text(aes(x = 6, y = 4), label = "y = a + b * x") +
      geom_text(aes(x = 6, y = 3.5), label = paste0("attitude = ", const, " + ", expo, " * exposure")) +
      labs("Exposure", "Attitude") +
      theme_general()
  rm(n, attitude, exposure, scatter, const, expo)
```

1. How does the plot visualize the constant of the regression equation?

```{r eval=FALSE}
* The constant of a regression equation, represented by the symbol a, is the
predicted value of the outcome variable if all predictors are zero.
* Graphically, this is where the regression line cuts the vertical (y) axis.
* In the plot, a is 1.4 according to the equations and the regression line cuts
the vertical axis at 1.4 as it should.
```

2. Explain how the horizontal and vertical lines in the plot help to interpret the unstandardized regression coefficient $b$.

```{r eval=FALSE}
* An unstandardized regression coefficient, denoted by the symbol b, tells us
the predicted difference in the outcome for a difference of one unit in the
predictor variable.
* According to the equations, the predicted attitude decreases by 0.25 for a one
unit increase in exposure.
* This is the decrease from 0.40 to 1.15 signalled by the horizontal line
segments.
```

Good understanding of the regression equation is necessary for understanding moderation in regression models. So let us have a close look at an example equation (Eq. \@ref(eq:regrexample)). The outcome variable attitude towards smoking is predicted from a constant and three predictor variables.

$$ 
\small
\begin{align}
  attitude = constant + b_1*exposure + b_2*status + b_3*contact + e 
  (\#eq:regrexample) 
\end{align}
\normalsize
$$

The constant adds a fixed quantity to the predicted attitude for all subjects. It adjusts the overall level of the predicted attitude. As such, the constant is usually not interesting. 

More precisely, the constant is the predicted attitude if a person scores zero on all predictor variables. To see this, plug in zero for all predictors in the equation (Eq. \@ref(eq:regsmokedummy)) and remember that zero times something is zero. This reduces the equation to the constant and the _error term_ $e$. The error term is the error of our prediction, also known as the _residual_. It does not help to predict the outcome, so the constant is the only remaining predictor.

$$ 
\small
\begin{align}
  attitude &= constant + b_1*0 + b_2*0 + b_3*0 + e \\ 
  attitude &= constant + 0 + 0 + 0 + e \\
  attidude &= constant + e
  (\#eq:regsmokedummy) 
\end{align}
\normalsize
$$

For all persons scoring zero on exposure, smoking status, and contact with smokers, the predicted attitude equals the value of the regression constant. This interpretation only makes sense if the predictors can be zero. If they include, for example, scales ranging from one to seven, there are no persons with zero scores on all predictors and the constant has no meaning.

The regression coefficients $b$ represent the predicted difference in the outcome for a difference of one unit in the predictor. For example, plug in the values 5 and 4 for the _status_ predictor in the equation. If we take the difference of the two equations, we are left with $b_1$. All other terms in the two equations cancel out (except, perhaps, the error term $e$).

$$ 
\small
\begin{align}
  attitude = constant + b_1*5 + b_2*status + b_3*contact + e \\ 
  \underline{- \mspace{20mu} attitude = constant + b_1*4 + b_2*status + b_3*contact + e} \\
  attitude \mspace{4mu} difference = b_1*5 - b_1*4 = b_1*(5-4) =b_1
   (\#eq:regweight) 
\end{align}
\normalsize
$$

We will be plugging in values for predictors in the regression equation a lot in this chapter. It is necessary for understanding and interpreting moderation.

### Continuous predictors

In a linear regression, the outcome variable ($y$) must be numeric and in principle continuous. There are regression models for other types of outcomes, for example, logistic regression for a dichotomous (0/1) outcome and Poisson regression for a count outcome, but we will not discuss them.

The predictor variables must be either numeric or dichotomous. If exposure is measured as a scale, for example ranging from zero to ten, the interpretation of the effect of exposure ($b_1$) is the one that we have encountered in the preceding section: the predicted difference in the outcome for a one unit difference in exposure while all other predictor values do not change (are held constant). 

Whether this predicted difference is small or large depends on the practical context: Is a small decrease in attitude towards smoking worth the effort of the campaign? If we want to apply a rule of thumb for the strength of the effect, we usually look at the standardized regression coefficient ($b^*$ according to APA6, _Beta_ in SPSS output). See Section \@ref(assoc-size) for some rules of thumb for effect size interpretation.

Note that the regression coefficient is calculated for the predictor values that occur within the data set. As a consequence, we do not know the relation between exposure and anti-smoking attitude for predictor values outside the range that actually occur in the sample. For example, if sample exposure scores are within the range three to seven, we should not pretend to know the effects of exposure levels below three or above seven. It is good practice to check the actual range of predictor values.

### Dichotomous predictors {#dichpredictor}

```{r regression-dichotomy, eval=FALSE, fig.cap="What is the difference in attitude between non-smokers and smokers?"}
# Goal: Refresh interpretation of unstandardized regression weight for a dichotomous predictor by manipulating group averages.
# Generate attitude scores  with average values -0.6 for non-smokers and 1.0 for smokers (N = 20 per group). Draw horizontal and vertical lines from the axes to the group means and add a regression line (line through the two group means). Display the current regression equation beneath or in the plot. Allow user to change the average score per group. Update the scatterplot, regression line and equation, and the horizontal/vertical lines.
# Generate predictor.
x <- c(rep(0, 20), rep(1, 20))
# Generate random errors per group.
set.seed(1272)
e <- rnorm(20, mean = 0, sd = 1)
e <- sign(e) * sqrt(abs(e*abs(e) - mean(e*abs(e)))) # center in squares
e <- c(e, e)
# Set group means
nonsmoker <- -0.6
smoker <- 1.0
# Generate outcome scaled by unstandardized regression coefficient.
y <- nonsmoker + smoker*x + e
lm(y~x)
```

1. What is the relation between the regression line for the dichotomous predictor (smoking) status and group averages?

Apart from numeric predictors, we can use dichotomous predictors, that is, predictors with only two values, which are preferably coded as 0 and 1 (_dummy coding_). The interpretation of the effect of a dichotomous predictor in a regression model is quite different from the interpretation of a numeric predictor.

For example, let us assume that smoking status is coded as smoker (1) versus non-smoker (0). Because this predictor can only take two values, we effectively have two versions of the regression equation. The first equation \@ref(eq:regdicho1) represents all smokers, so their smoking status score is 1. This group has a fixed contribution to the predicted average attitude, namely $b_2$.

$$ 
\small
\begin{align}
  attitude &= constant + b_1*exposure + b_2*status + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_2*1 + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_2 + b_3*contact + e 
  (\#eq:regdicho1) 
\end{align}
\normalsize
$$

Regression equation \@ref(eq:regdicho0) represents all non-smokers. Their smoking status score is 0, so the smoking status effect drops from the model. 

$$ 
\small
\begin{align}
  attitude &= constant + b_1*exposure + b_2*status + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_2*0 + b_3*contact + e \\
  attitude &= constant + b_1*exposure + b_3*contact + e 
  (\#eq:regdicho0) 
\end{align}
\normalsize
$$

It makes no sense to interpret the regression coefficient of smoking status ($b_2$) as predicted difference in attitude for a difference of one in smoking status. After all, the 0 and 1 scores do not mean that there is a one unit difference. Instead, the coefficient indicates that we are dealing with different groups: smokers versus non-smokers. We have to interpret the effect as a difference between two groups. More specifically, as the difference between the group represented by the score 1 and the _reference group_ represented by score 0.

If you compare the final equations for smokers (Eq. \@ref(eq:regdicho1)) and non-smokers (Eq. \@ref(eq:regdicho0)), the only difference is $b_2$, which is present for smokers but absent for non-smokers. It is the difference between the average outcome score (attitude) for smokers and non-smokers. In this example, it is the average attitude of smokers minus the average attitude of non-smokers. Just like an independent-samples t test!

Imagine that $b_2$ equals 1.6. This indicates that the average attitude towards smoking among smokers is 1.6 units above the average attitude among non-smokers. Is this a small or large effect? In the case of a dichotomous predictor, we should __not__ use the standardized regression coefficient to evaluate effect size. The standardized coefficient depends on the distribution of 1s and 0s, that is, which part of the respondents are smokers. But this should be irrelevant to the size of the effect. 

Therefore, it is recommended to interpret only the unstandardized regression coefficient for a dichotomous predictor. Interpret it as the difference in average outcome scores for two groups as we have done in the preceding paragraph.

### A categorical predictor and dummy variables {#categorical-predictor}

```{r regression-categorical, fig.cap="What are the predictive effects of smoking status?"}
# Goal: Understanding effects of dummy variables by manipulating the reference group.
# Generate data for a categorical predictor (3 categories: (1) never smoked, (2) have smoked, (3) currently smoking) and a numerical outcome (attitude) in a random choice of one of the following scenarios: (1) < (2) = (3) {initial situation}, (1) = (2) < (3), (2) < (1) = (3) (1) < (2) < (3). Display scatterplot containing all three groups with group means indicated (line segment & value), regression lines through reference group mean and dummy group mean. Display b and p value of regression weights with the regression lines. Add input to select the reference group, initially set to group (1). Update regression lines and their p values on selection of a reference group. Add button to generate a new plot (with a new scenario).
```

1. Interpret the effects of smoking status in Figure \@ref(fig:regression-categorical).

2. Can you tell whether the attitude of smokers is significantly different from the attitude of former smokers? Select a different reference group to motivate your answer.

3. Select some new plots. For each plot, determine which reference group you think is most convenient for summarizing the results. 

What if smoking status was measured with three categories: (1) never smoked, (2) have smoked, (3) currently smoking? We can only include such a categorical predictor if we change it into a set of dichotomies. 

A _categorical variable_ contains three or more categories or groups. We can create a new dichotomous variable for each group, indicating whether (score 1) or not (score 0) the respondent is part of this group. In the example, we could create the variables _neversmoked_, _smokesnomore_, and _smoking_. Every respondent would score 1 on one of the three variables and 0 on the other two variables. These variables are called _dummy variables_ or _indicator variables_.

```{r dummytable, echo=FALSE, screenshot.opts=list(delay = 2)}
knitr::kable(rbind(c("1 - Never smoked", "1", "0", "0"), 
                   c("2 - Former smoker", "0", "1", "0"),
                   c("3 - Smoker", "0", "0", "1")), 
             col.names = c("Original categorical variable:", "neversmoked", "smokesnomore", "smoking"), caption = "Dummy variables for a categorical predictor: One dummy variable is superfluous.", align = c("l", "c", "c", "c"))
```

If we want to include a categorical predictor in a regression model, we must use all dummy variables as predictors except one. In the example, we must include two out of the three dummy variables. We cannot include all three dummy variables because the score on the third dummy variable is determined by the score on the first two dummy variables. 

If a respondent scores 1 on one of the first two dummy variables, her score must be 0 on the third dummy variable. Someone who is not a member of the Never Smoked or Smokes No More Groups, must be a member of the Smoking Group. A person scoring 0 on the first two dummy variables, then, must score 1 on the third.

We cannot include a predictor that is perfectly predictable from other predictors in the regression model. It is like including the same predictor twice: How can the estimation process decide which predictor is responsible for the effect? It can't decide, so the estimation process fails and no regression coefficients are estimated. If this happens, the predictors are said to be perfectly _multicollinear_. 

The category or group that is left out of the regression model is the reference group. Remember that non-smokers were the reference group in the preceding section because the regression equation did not include a dichotomous predictor on which the non-smokers scored 1. The two groups were represented by only one dichotomy.

The interpretation of the effects (regression coefficients) for the included dummies is the same as for a single dichotomous predictor such as smoker versus non-smoker. It is the difference between average outcome score of the group scoring 1 on the dummy variable and the average outcome score of the reference group.

If we exclude the dummy variable for the respondents who never smoked, the regression weight of the dummy variable for the Smokes No More Group gives the average difference between former smokers and non-smokers. If the regression weight is positive, for example -0.8, former smokers have a more negative attitude towards smoking than non-smokers. If the difference is positive, former smokers have a more positive attitude towards smoking. 

Which group should we use as reference category, that is, which dummy should not be used in the regression model? This is hard to say in general. If one group is of greatest interest to us, we could use this as the reference group, so all dummy variable effects express differences with this group. Alternatively, if we expect a particular ranking in average outcome scores, we may pick the group at the highest, lowest or middle rank as the reference group. If you can't decide, run the regression model several times with a different reference group.

### Sampling distributions and assumptions {#regr-inference}

```{r regression-sampling, fig.cap="What happens to regression lines from sample to sample?"}
# Goal: Understand that regression constant and coefficient(s) have sampling distributions.
# Generate a population with a weak negative effect (-0.6) of exposure on attitude and exposure, with a sizable error term (so a lot of variation in sample regression lines). Generate a sample (N = 10) and display it in a scatterplot with regression line, labelled with it's unstandardized regression coefficient value. Also plot sampling distribution for regression coefficient. Add button to allow drawing a new sample; display the new sample and new regression line but retain the existing regression lines. Add button (or change sampling button) to draw 1,000 samples: don't display samples, just update sampling distribution with normal (or t) distribution as superimposed curve.
```

1. Which estimates can change from sample to sample: the regression constant, the regression coefficient, or both? Check your answer by drawing new samples.

2. What is the shape of the sampling distribution if you draw a lot of samples?

3. What happens if you draw samples of larger size? Think of what you learned in preceding chapters. Formulate your answer before you change sample size in Figure \@ref(fig:regression-sampling).

If we are working with a random sample or we have other reasons to believe that our data could have been different due to chance (Section \@ref(no-random-sample)), we should not just interpret the outcomes for the data set that we collected. We should apply statistical inference---confidence intervals and significance tests---to our results. The confidence interval gives us bounds for the population value of the unstandardized regression coefficient. The p value is used to test the null hypothesis that the unstandardized regression coefficient is zero in the population.

Each regression coefficient as well as the constant may vary from sample to sample drawn from the same population, so we should devise a sampling distribution for each of them. Because the constant in a regression model is usually uninteresting, we will only discuss sampling distributions for regression coefficients. Their sampling distributions happen to have a t distribution under particular assumptions.

Chapters \@ref(param-estim) and \@ref(hypothesis) have extensively discussed how confidence intervals and p values are constructed and how they must be interpreted. So we may as well focus now on the assumptions under which the t distribution is a good approximation of the sampling distribution of a regression coefficient. 

#### Independent observations
The two most important assumptions require that the observations are _independent and identically distributed_. These requirements arise from probability theory. If they are violated, the statistical results should not be trusted.

Each observation, for example, a measurement on a respondent, must be independent of all other observations. This respondent's outcome variable score may not depend on outcome scores of other respondents.

It is hardly possible to check that our observations are independent. We usually have to assume that this is the case. But there are situations in which we should not make this assumption. In time series data, for example, the daily amount of political news, we usually have trends, cyclic movements, or issues that affect the amount of news over a period of time. As a consequence, the amount and contents of political news in one day may depend on the amount and contents of political news in the preceding days.

Clustered data should also not be considered as independent observations. Think, for example, of student evaluations of statistics tutorials. Students in the same tutorial group are likely to give similar evaluations because they had the same tutor and because of group processes: both enthusiasm and dissatisfaction can be contagious.

#### Identically distributed observations

```{r resid-normal, fig.cap="What are the residuals and how are they distributed?"}
# Goal: Understand the meaning of residuals by linking residuals in a scatterplot to the x values in a histogram.
# Generate a sample (N = 20?) with a weak negative effect (-0.6) of exposure on attitude and exposure, with a sizable error term to have residuals that are clearly visible. Generate either a sample with normally distributed residuals or uniformly distributed residuals. Display the sample as a scatterplot with the regression line and (red) line segments linking the dots vertically to the regression line. Display the residuals also as a histogram with normal curve. Hovering over/clicking a line segment (residual) in the scatterplot should highlight the corresponding bar in the histogram. Add a button to draw a new sample.
```

1. What do the lines between dots and regression line represent in the scatterplot of Figure \@ref(fig:resid-normal)?

2. What is the relation between the scatterplot and the histogram?

3. Draw some new samples. Are the residuals always normally distributed?

If we sample from a population where attitude towards smoking depends on exposure, smoking status, and contact with smokers, we will be able to predict attitude from the predictors in our sample. Our predictions will not be perfect, sometimes too high and sometimes to low. These are the residuals. 

If our sample is truly a random sample with independent and identically distributed observations, our predictions should be equally bad or equally well for each value of the outcome variable, that is, attitude in our example. More specifically, the sizes of our errors (residuals) should be normally distributed for each attitude level (according to the central limit theorem). 

So for all possible values of the outcome variable, we must collect the residuals for the observations that have this score on the outcome variable. For example, we should select all respondents who score 4.5 on the attitude towards smoking scale. Then, we select the residuals for these respondents and see whether they are approximately normally distributed.

Usually, we do not have more than a few observations for each single outcome score, so we cannot practically apply this check. Instead, we use a simple and coarse approach: Are all residuals normally distributed?

A histogram with an added normal curve helps us to evaluate the distribution of the residuals. If the curve more or less follows the histogram, we conclude that the assumption of identically distributed observations is plausible. If not, we conclude that the assumption is not plausible and we warn the reader that the results can be biased.

#### Linearity and prediction errors

```{r pred-linearity, fig.cap="How do residuals tell us whether the relation is linear?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="430px"}
# Goal: Understand the relation between linear model (scatterplot) and residuals plot by manipulating the shape of the association.
# Generate a sample (N = 20?) with a weak negative effect (-0.6) of exposure on attitude, with a sizable error term to have residuals that are clearly visible. Generate either a sample with (1) linear, (2) curved, (3) U-shaped association. Display the sample as a scatterplot with the regression line and (red) line segments linking the dots vertically to the regression line. Display the residuals also in a residuals (Y) by predicted values (X) plot. Hovering over/clicking a dot in the scatterplot should highlight the corresponding dot in the residuals plot. Add a button to select a different association shape. Upon selection of a shape, generate & display new sample data.
knitr::include_app("http://82.196.4.233:3838/apps/pred-linearity/", height="550px")
```

1. Think up which dot in the plot of residuals (Figure \@ref(fig:pred-linearity) bottom) corresponds with the left-most observation (dot) in the scatterplot of attitude by exposure (Figure \@ref(fig:pred-linearity) top). Drag your mouse around the left-most dot while pressing the left mouse button to check your choice. Repeat for more dots until you understand the relation between the two plots.

```{r eval=FALSE}
* The blue regression line in the top graph represents the predicted values on
the attitude variable from exposure scores. The predicted attitude value for
the left-most observation is the value at which the blue line intersects with
the vertical red line dropping down from that observation. The predicted value
is above zero.
* The left-most observation is the highest predicted value because the
regression line slopes down to the right. The highest predicted value is at
the far right of the residuals by predicted attitude graph (bottom graph)
because the predicted values are on the horizontal axis of this graph. The
left-most observation in the top grpah, then, must correspond with the
right-most observation in the bottom graph.
* Note that this is not always true. If the regression slope is positive, that
is, the regression line goes up from left to right, the left-most observation
in the top graph has the lowest predicted value, so it corresponds to the
left-most observation in the residuals plot.
```

2. Select a U-shaped curve in Figure \@ref(fig:pred-linearity). Explain how the plot of residuals tells you that the association is not linear. Do the same for a curved association.

```{r eval=FALSE}
* If the association between two variables is U-shaped, the regression line
underestimates the attitude for observations with low exposure, overestimates
the attitude for medium values of exposure, and underestimates the attitude
for high exposure values. As a result, the residuals have a marked pattern
from left to right: a set of positive residuals, followed by a set of negative
residuals, followed by a set of positive residuals.
* The same phenomenon occurs for a curved association.
* In contrast, a linear association yields residuals without a clear pattern.
At all levels of exposure and, hence, at all predicted levels of attitude, we
may encounter both positive and negative residuals. In the residuals by
predicted values plot, we have positive and negative residuals everywhere. We
may underestimate as well as overestimate the outcome variable everywhere.
```

The other two assumptions that we use tell us about problems in our model rather than problems in our statistical inferences. Our linear regression model assumes a linear effect of the predictors on the outcome variable (_linearity_) and it assumes that we can predict the outcome equally well or equally badly for all levels of the outcome variable (_homoscedasticity_). 

We can check the assumption of a linear model in a graph showing the (standardized) residuals (vertical axis) against the (standardized) predicted values of the outcome variable (on the horizontal axis). Note that the residuals represent prediction errors. If our regression predictions are systematically too low at some levels of the outcome variable and too high at other levels, the residuals are not nicely distributed around zero for all predicted levels of the outcome variable. This is what you see if the association is curved or U-shaped.

This indicates that our linear model does not fit the data. If it would fit, the average prediction error is zero for all predicted outcome levels. Graphically speaking, our linear model matches the data if at every horizontal position, positive prediction errors (residuals) are balanced by negative prediction errors.

#### Homoscedasticity and prediction errors

```{r pred-homoscedasticity, fig.cap="How do residuals tell us that we predict all values equally well?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="430px"}
# Goal: Understand the relation between linear model (scatterplot) and residuals plot by manipulating homoscedasticity.
# Generate a sample (N = 20) with a weak negative effect (-0.6) of exposure on attitude and exposure, with a sizable error term to have residuals that are clearly visible. Generate error terms with a dependency on the predictor ranging from -1 to +1. Display the sample as a scatterplot with the regression line and (red) line segments linking the dots vertically to the regression line. Display the residuals also in a residuals (Y) by predicted values (X) plot. Hovering over/clicking a dot in the scatterplot should highlight the corresponding dot in the residuals plot. Add a slider (range [-@, 0], initial value 0) to set the levl of heteroscedasticity. Upon slider change, generate & display new sample data.
knitr::include_app("http://82.196.4.233:3838/apps/pred-homoscedasticity/", height="550px")
```

1. What strikes you about the residuals in Figure \@ref(fig:pred-homoscedasticity)?

```{r, eval=FALSE}
* The residuals are higher for higher values of exposure.
* In the top graph, the residuals tend to become larger from left to right.
* As a consequence, the residuals are larger at one side of the residuals by
predicted values plot (bottom graph) then at the other side. Because the
regression line has a negative slope, we can predict high attitude levels
better than low attitude levels.
* The regression model seems to predict attitude better for subjects with low
exposure scores than for subjects with high exposure scores.
```

2. What happens if you move the slider to the far left?

```{r, eval=FALSE}
* The pattern reverses. Now, residuals are larger for low values of exposure
or, equivalently in this model with a negative slope, for higher predicted
values of attitude.
* In this situatio, we are better at predicting low attitude levels than high
attitude levels. Note that we prefer to predict all attitude levels equally
well.
```

3. At which slider position are all attitude levels predicted equally well or equally badly?

```{r eval=FALSE}
* If the slider is positioned at or around zero, the residuals are more or
less equally large for low, medium, or high predicted values of attitude.
Here, we can predict all levels of attitude equally well or equally badly.
* This is best seen in the bottom graph: The vertical spread of observations
is more or less the same at the left, middle, and right of the graph. The
vertical diameter of the dot cloud is more or less equal from left to right.
* This is how we like the plot to look.
```

The other assumption states that we can predict the outcome variable equally well at all outcome variable levels. In other words, the prediction errors (residuals) are more or less the same at all levels of the outcome variable. If we have large prediction errors at some levels of the outcome variable, we should also have large prediction errors at other levels. As a result, the vertical width of the residuals by predictions scatterplot should be more or less the same from left to right. 

If the prediction errors are not more or less equal for all levels of the predicted outcome, our model is better at predicting some values than other values. For example, low values can be predicted better than high values of the outcome variable. This may signal, among other things, that we need to include moderation in the model.


```{r eval=FALSE, echo=FALSE}
#DROPPED

Why do we use the residuals and predicted values instead of a scatterplot for each outcome-predictor variable pair to assess linearity and homoscedasticity? The reason is that some predictors may predict low outcome values and other predictors may predict high outcome values. This is perfectly OK if together they predict low and high outcome values equally well.
```

### Visualizing predictions 

```{r regression-predict, fig.cap="How do predictions based on exposure depend on values of smoking status and smoker contact?"}
# Goal: sensitize student to the notion that prediction in a multiple regression model requires selecting one predictor and fixed values for the other predictors. Without moderation, the fixed values only change the vertical position of the line (that is, the constant) but not the slope.
# Display a scatterplot with attitude towards smoking as Y and exposure as X with a negative more or less linear relation and a regression line for the currently selected values of the covariates (smoking status (dichotomy: smoker versus non-smoker) and (smoker)contact). Display the multiple regression equation: attitude = 0.25 - 0.20 * exposure + 0.50 * smoker +  0.15 * contact. 
# User can change smoking status (0 or 1) and contact (range 0-10), which triggers the app to redraw the regression line for attitude by exposure for these values of smoking status and (smoker)contact.
# The user must discover that the regression line moves up/down the covariate's regression coefficient times the difference between the selected covariate value and the covariate mean. 
```

1. What happens in Figure \@ref(fig:regression-predict) if you change smoking status and smoking contact score? Can you explain the size of changes?

2. Add the line for a simple regression of attitude on exposure (without covariates). Can you fit the original (multiple) regression line to the simple regression line? If so, at what covariate values? If not, why?

The regression equation without the error term $e$ predicts the outcome variable scores from the predictor scores. Plug in values for the predictor variables and you can calculate the predicted outcome score. Figure \@ref(fig:regression-predict), for example, shows a regression equation for the effects of exposure and contact with smokers on attitude towards smoking. If you plug in a score of 4 for exposure, 0 for smoking status (non-smoker), and 6 for contact with smokers, the predicted attitude is 0.25 + -0.20 * 4 + 0.50 * 0 + 0.15 * 6 = 0.41.

If we focus on the relation between one predictor and the outcome variable, the predicted values can be represented by a straight line in a scatterplot. By convention, we display the predictor on the horizontal axis and the outcome variable on the vertical axis of the scatterplot. 

In a simple regression, we only have one predictor, so we can only draw one regression line. In a multiple regression, however, we have several predictors. We can draw regression lines for each predictor and, importantly, we can draw different regression lines for the same predictor.

If we want to draw the regression line for the predictive effect of one predictor, we regard the other predictors as covariates. Let us define a _covariate_ as a variable that may predict the outcome but it is not our prime interest, so we mainly want to control for its effects. For example, if we focus on the predictive effect of exposure on attitude towards smoking, exposure is our predictor and smoking status and contact with smokers are covariates. 

Note that the distinction between predictor and covariates is temporary. As soon as we focus on another variable, that variable becomes the predictor and the other predictors become covariates. The distinction between predictor and covariate is just terminology to show on which variable we focus.

To draw the regression line for the effect of exposure on attitude towards smoking, we must select a value for the covariates. If we would not do so, we have more than one variable that is allowed to vary but we can only display one predictor on the horizontal axis of our scatterplot.

$$ 
\small
\begin{align}
  attitude &= 0.25 + -0.20*exposure + 0.5*status + 0.15*contact \\ 
  attitude &= 0.25 + -0.20*exposure + 0.5*0 + 0.15*3 \\ 
  attitude &= 0.25 + -0.20*exposure + 0 + 0.45 \\ 
  attitude &= 0.70 + -0.20*exposure 
  (\#eq:regsimpleslope) 
\end{align}
\normalsize
$$

Let us select non-smokers (_status_ equals 0) who score 3 on _contact_. If we plug in these values in the regression equation, we obtain a simple regression---just one predictor, namely _exposure_---with a higher constant: 0.70 instead of 0.25. The constant is the intercept of the regression line, that is, the value of the vertical axis where the regression line crosses it. 

The slope of the regression line, however, does not change no matter which values we select for the covariate. The regression coefficient of _exposure_ remains -0.20. The regression line only moves up or down if we choose different values for the covariates. To visualize the exposure effect, it does not matter which values we chose for the covariates. A popular choice is using their average scores. When we add a moderator, however, the slope also changes as we will see in Section \@ref(categoricalmoderator).

## Regression Analysis in SPSS {#SPSS_regression}

### Instructions

```{r SPSSregdummy2, echo=FALSE, out.width="640px", fig.cap="(ref:regdummy2SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/DKvt_d3GfVA", height = "360px")
# Creating dummy variables in SPSS.
# Goal: Understand creating dummy variables.
# Example: smokers.sav, respondent's smoking status ( 3 categories).
# SPSS menu: Transform > Create Dummy Variables
# Inspect results: new variables, coded 0/1.
```

----

```{r SPSSregdummy, echo=FALSE, out.width="640px", fig.cap="(ref:regdummySPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/AJ88dheUieY", height = "360px")
# Using dummy variables in a regression model in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status (dummies).
# SPSS menu: Transform > Create Dummy Variables
# Remember: Leave one dummy variable out.
# Interpret results: unstandardized regression coefficient as average difference with reference category. Don't interpret the standardized regression coefficient.
```

----

```{r SPSSreglines1, echo=FALSE, out.width="640px", fig.cap="(ref:reglines1SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/OQEylRkeVzI", height = "360px")
# Goal: Adding regression lines to a scattergram for a simple regression (Chart Editor: Add fit line at total), for one predictor in multiple regression for reference values of anther predictor - first one line, then two lines (Chart Editor: reference line from equation).
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status.
# SPSS menu: spell out the equation in notepad, Graph > Legacy Dialogs > Scatter/Dot, set markers by status3, Chart Editor, add a reference line from equation, just enter the equation with plugged in values, don't simplify ; color lines according to the smoking status group
# Interpret output: vertical difference between lines is average difference between categories, equals unstandardized regression coefficient.
```

----

```{r SPSSregassumpt, echo=FALSE, out.width="640px", fig.cap="(ref:regassumptSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/AtzXHORzxlA", height = "360px")
# Goal: Inspecting residuals.  (see Chapter 4 on hypothesis testing for video about regression basics.)
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status.
# Technique: regression analysis
# SPSS menu: linear regression, add plots
# Interpret output = check assumptions: Chart Editor of zresid * zpred plot: add reference line at 0 and perhaps at +2 and -2 to inspect shape of residual distribution.
```

### Exercises

1. Use the data in <a href="http://82.196.4.233:3838/data/smokers.sav" target="_blank">smokers.sav</a> to predict the attitude towards smoking from exposure to an anti-smoking campaign. Check the assumptions and interpret the results.

```{r eval=FALSE}
SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=exposure attitude
  /ORDER=ANALYSIS.
* Simple regression analysis with assumption checks.
REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT attitude
  /METHOD=ENTER exposure
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).

Check data:

There are no impossible values on the two variables.

Check assumptions:

* The distribution of the residuals is quite in line with a normal
distribution.
* The residuals are nicely centered around zero at all predicted levels of the
outcome variable, so the association seems to be linear.
* The residuals are evenly spread around zero at all predicted levels, so
prediction accuracy is the same at all levels (homoscedasticity).

Interpret the results:

* Campaign exposure predicts attitude towards smoking among adults reasonably
well, R2 = .17, F (1, 83) = 16.73, p < .001.
* One additional unit of exposure decreases the predicted attitude by 0.12 to
0.34 points, t = -4.09, p < .001, 95%CI[-0.34; -0.12]. This is a moderate to
strong effect (b* = -.41).
```

2. Add smoking status (variable _status3_), and contact with smokers as predictors to the regression model of Exercise 1. Compare the effect of exposure between the two regression models. What is the difference and why is there a difference?

```{r eval=FALSE}
SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=exposure status3 contact attitude
  /ORDER=ANALYSIS.
* Create dummy variables for status3.
* ENSURE THAT MEASUREMENT LEVEL I SET TO ORDINAL.
* Define Variable Properties.
*status3.
VARIABLE LEVEL  status3(ORDINAL).
EXECUTE.
SPSSINC CREATE DUMMIES VARIABLE=status3 
ROOTNAME1=status 
/OPTIONS ORDER=A USEVALUELABELS=YES USEML=YES OMITFIRST=NO.
* Multiple regression analysis with assumption checks.
REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT attitude
  /METHOD=ENTER exposure status_2 status_3 contact
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).

Check data:

All values on the variables seem to be valid.

Check assumptions:

* The residuals seem to be a little skewed rather than symmetrical as in a
normal distribution.
* The residuals by predicted values plot is not as it should be. At low
predicted levels, the residuals are below zero, so the average residual is not
zero. The associations do not seem to be linear.
* In addition, the spread of the residuals is larger at higher predicted
levels (right) than low predicted levels (left). This pattern suggests that
the effects are moderated (see next section).

Interpret the results:

* Include a table with regression coefficients, so we need not report all t
test results in our interpretation.

* The regression model predicts about sixty percent of the variation in
attitude towards smoking, which is very much for a social scientific model, R2
= .62, F (4, 80) = 32.02, p < .001.
* Exposure to the anti-smoking campaign predicts a more negative attitude
towards smoking, while contact with smokers is associated with a slightly more
positive attitude. Of the two, exposure (b* = -0.44) is a better predictor
than contact with smokers (b* = 0.20).
* Former smokers are on average much (2.85 points) more negative about smoking
than non-smokers and smokers, who are on average only 0.20 points below the
average attitude of non-smokers.
* The residuals suggest that the assumptions for using the theoretical
approximation of the sampling distributions may not have met and/or that the
current model is mispecified.
```

3. The data set <a href="http://82.196.4.233:3838/data/children.sav" target="_blank">children.sav</a> contains information about the media literacy of children and parental supervision of their media use. Are the two related? Check the assumptions and interpret the results.

```{r eval=FALSE}
* The analysis method you choose, depends on your substantive decision on the
direction of the association.
* If you assume no direction, a correlation coefficient is the most
appropriate choice. If you think one variable may depend on another, a
(simple) regression model is the best choice.

SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=medliter supervision
  /ORDER=ANALYSIS.
* Set supervision 25 to missing.
* Define Variable Properties.
*supervision.
MISSING VALUES supervision(25.00).
EXECUTE.
* Undirected: correlation (linear?).
* Check scatterplot.
GRAPH
  /SCATTERPLOT(BIVAR)=supervision WITH medliter
  /MISSING=LISTWISE.
* Correlations.
CORRELATIONS
  /VARIABLES=medliter supervision
  /PRINT=TWOTAIL NOSIG
  /MISSING=PAIRWISE.
* Simple regression: media literacy dependent.
REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT medliter
  /METHOD=ENTER supervision
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).
* Simple regression: parental supervision dependent.
REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT supervision
  /METHOD=ENTER medliter
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).

Check data:

Score '25' for parental supervision cannot be right because the scale runs to
10. Define this score as a missing value.

Check assumptions:

* In the regression models, the residuals are quite normally distributed,
nicely grouped around zero at all levels of the predicted outcome.
* The variation of residuals is more or less the same at different levels of
the predicted outcome but there are perhaps too few observations for low and
high predicted levels to decide.

Interpret the results:

Parental supervision and child media literacy are weakly correlated (r = .36,
p = .001). More supervision goes together with more media literacy, t = 3.54,
p = .001, 95%CI[0.19; 0.66]. One additional unit of parental supervision
predicts 0.31 additional units of media literacy. One additional unit of media
literacy predicts 0.42 additional units of parental supervision.

Note that the t test on the regression coefficient is the same in a simple
regression model if you use supervision or media literacy as outcome variable.
```

## Different Lines for Different Groups {#categoricalmoderator}

If we have a categorical independent variable, for example, smoking status, and we want to determine its effect on a numerical variable, for example, attitude towards smoking, we compare group means. The difference between group means is the main effect of the categorical variable. For example, the average attitude towards smoking is 0.5 points more positive among smokers than among non-smokers. 

In analysis of variance (Chapter \@ref(anova)), the main effect of smoking status is the average effect for all people regardless of their other characteristics or the contexts that they are in. In other words, a main effect is the overall difference in attitude between smokers and non-smokers.

What if the effect of smoking status on attitude may be different in different contexts, e.g., for people living among smokers versus those living among non-smokers? To model this, we added an interaction effect to the main effects in Chapter \@ref(anova).  

The interaction effect tells us whether the attitude difference between smokers and non-smokers differs between, on the one hand, people living among smokers and, on the other hand, people living among non-smokers. In a conceptual diagram, the interaction effect is represented by an arc pointing to another arc. The moderator (contact with smokers) changes the relation between the predictor (smoking status) and the outcome (attitude towards smoking).

```{r moderator-concept2, echo=FALSE, fig.cap="Conceptual diagram of moderation.", fig.asp=0.3}
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(0.3, 0.5, 0.7), 
                        y = c(.1, .3, .1),
                        label = c("Predictor", "Moderator", "Outcome"))
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$x[3] - 0.04, yend = variables$y[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$x[2], yend = variables$y[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) +
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0, 0.4)) +
  theme_void()
# Cleanup.
rm(variables)
```

Analysis of variance (ANOVA), as discussed in Chapter \@ref(anova), investigates the effects of categorical variables on a numeric outcome variable. It cannot handle numeric predictors or numeric covariates. Although there are ways to include numeric covariates in analysis of variance, for example, in the analysis of covariance (ANCOVA), we use regression analysis if we have at least one numerical predictor or covariate and a numerical outcome. 

In the current section, we discuss regression models with a numerical predictor and a categorical moderator. A later section (Section \@ref(cont-moderator-regression)), presents regression models in which both the predictor and moderator are numeric.

### A dichotomous moderator and continuous predictor

```{r dichotomous-moderator, fig.cap="Is the effect of exposure on attitude moderated by smoking status?"}
# Goal: Sensitize the student to the notion that moderation in a regression model means different slopes for different groups.
# In a graph with attitude as Y axis and exposure as X axis, generate two regression lines, one for smokers and one for non-smokers. Show the regression equation for each line (preferably in the plot). Systematically vary the slopes (the same, one more negative than the other, opposite signs) and the intercept difference (positive, nearly zero, negative). Add a Generate New button to replace the regression lines by a new pair of lines.
```

1. Is the effect of exposure on attitude moderated by the smoking status of respondents (smokers versus non-smokers) in Figure \@ref(fig:dichotomous-moderator)? Motivate your answer.

2. Press the Generate New button to practice some more with recognizing moderation. How can the regression equations help you to see whether the effect is moderated?

In Section \@ref(regression-equation), we have analyzed the predictive effects of exposure to an anti-smoking campaign and smoking status on a person's attitude towards smoking. We have found a negative effect for exposure and a positive effect for smoking. More exposure predicts a more negative attitude whereas smokers have a more positive attitude towards smoking.

Our current question is: Does exposure to the campaign have the same effect for smokers and non-smokers? We want to compare an effect (exposure on attitude) for different contexts (smokers versus non-smokers), so our current question involves moderation. Is the effect of exposure on attitude moderated by smoking status?

Our moderator (smoker vs. non-smoker) is a dichotomous variable but our predictor (exposure) is numeric, so we cannot use analysis of variance. Instead, we use regression analysis, which allows numeric predictors. 

In the context of a regression model, moderation means __different slopes for different groups__. The slope of the regression line is the regression coefficient, which expresses the effect of the predictor on the outcome variable. If we have different effects in different contexts (moderation), we must have different regression coefficients for different groups.

### Interaction variable {#interaction-variable}

```{r interaction-var-effect, fig.cap="What does an interaction variable do?"}
# Goal: Intuitive understanding of the effect of an interaction variable.
# Generate a dataset with 30 observations for the regression model y = 3 - 0.5x_1 + 1.5x_2 + 0.3x_1*x_2 with x_1 in the range [0, 10] and x_2 a dummy (0 or 1) with a random uniform component to each parameter in the range [-.1, .1]. In a scatterplot of attitude (Y) versus exposure (X), display the regression line (fat, grey) for the equation with x_2 = 0, labelled with the regression equation without the interaction variable. Display smokers and non-smokers with different colours/shapes. Add a select list labeled 'Add product of exposure and smoking status' with the values '--', '0 - Non-smokers', and '1 - Smokers'. Selection of a value adds the corresponding regression line to the plot with the category name and regression equation. (The line for non-smokers is parallel to the fat gray line.) Clicking/hovering over the newly created line shows the slope as a sum of the conditional and interaction effect, e.g., "Slope: 0.5 * exposure + 0.3 * 1 * exposure".
```

1. In Figure \@ref(fig:interaction-var-effect), does the fat grey line represent the effect of exposure on attitude in a simple regression model, a multiple regression model, or both?

2. Select an option under "Add product" and explain what the newly created regression line means.

3. Select the other option. Explain why the two regression lines that you created have different slopes.

How do we obtain different regression coefficients and lines for smokers and non-smokers? The statistical trick is quite easy: Include a new predictor in the model that is the product of the predictor (exposure) and the moderator (smoking status). This new predictor is the _interaction variable_. It must be included together with the original predictor and moderator variables, see Equation \@ref(eq:intvar). This is also visible in the statistical model (Figure \@ref(fig:moderator-statistical)) for moderation in a regression model.

$$
\small
\begin{align}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
  &+ b_4*exposure*smoker + e 
  (\#eq:intvar) 
\end{align}
\normalsize
$$

```{r moderator-statistical, fig.cap="Statistical diagram of moderation.", echo=FALSE, fig.asp=0.4}
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(rep.int(0.3, times = 4), 0.7), 
                        y = c(.4, .3, .2, .1, .25),
                        label = c("Exposure", "Smoker", "Contact", "Exposure*Smoker", "Attitude"))
# Add coordinates for arc endpoint.
x_diff <- 0.04
variables$xend <- variables$x[5] - x_diff #fixed translation to the left
variables$yend <- variables$y[5] + x_diff * (variables$y - variables$y[5]) / (variables$x[5] - variables$x)
rm(x_diff)
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$xend[1], yend = variables$yend[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$xend[2], yend = variables$yend[2]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[3], y = variables$y[3], xend = variables$xend[3], yend = variables$yend[3]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[4], y = variables$y[4], xend = variables$xend[4], yend = variables$yend[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.1, 0.5)) +
  theme_void()
```

The smoking status variable is coded 1 for smokers and 0 for non-smokers. For clarity, we name this variable smoker with score 1 for Yes and score 0 for No. Remember that we have two different regression equations, one for each group on the dichotomous predictor _status_. Just plug in the two possible values (1 and 0) for this variable. For non-smokers, the interaction variable drops from the model because multiplying with zero yields zero. For non-smokers, our reference group, $b_1$ represents the effect of exposure on attitude. It is called the _simple slope_ of exposure for non-smokers.

$$
\small
\begin{align}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
    &+ b_4*exposure*smoker + e \\
  attitude = &\ constant + b_1*exposure + b_2*0 + b_3*contact\\
    &+ b_4*exposure*0 + e \\
  attitude = &\ constant + b_1*exposure + b_3*contact + e
  (\#eq:intvarnonsmoker) 
\end{align}
\normalsize
$$

In contrast, the interaction variable remains in the model for the smokers, who score 1 on smoking status. Note what happens with the coefficient of the exposure effect if we rearrange the terms a little: The exposure effect equals the effect for the reference group of non-smokers ($b_1$) plus the effect of the interaction variable ($b_4$). The simple slope for smokers, then, is $b_1 + b_4$.

$$
\small
\begin{align}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
  &+ b_4*exposure*smoker + e \\
  attitude = &\ constant + b_1*exposure + b_2*1 + b_3*contact\\
  &+ b_4*exposure*1 + e \\
  attitude = &\ constant + b_1*exposure + b_4*exposure + b_2 + b_3*contact + e \\
  attitude = &\ constant + (b_1 + b_4)*exposure + b_2 + b_3*contact + e 
  (\#eq:intvarsmoker) 
\end{align}
\normalsize
$$

The interaction effect ($b_4$) shows the difference between the simple slope of the exposure effect for smokers ($b_1+b_4$) and the simple slope for non-smokers ($b_1$). This is the interpretation of the regression coefficient for a dichotomous interaction variable.

### Conditional effects, not main effects {#conditional-effects}

It is very important to note that the effects of exposure and smoking status in a model with exposure by smoking status interaction are __not__ main effects as in analysis of variance. As we have seen in the preceding section (Equation \@ref(eq:intvarnonsmoker)), the regression coefficient $b_1$ for exposure expresses the effect of exposure for the reference group of non-smokers. It is a _conditional effect_, namely the effect for non-smokers only. This is quite something different from a main effect, which is an average effect over all groups.

In a similar way, the regression coefficient $b_2$ for smoking status expresses the effect for persons who score zero on the exposure predictor. Simply plug in the value 0 for exposure in the regression equation (Equation \@ref(eq:simplestatus)).

$$
\small
\begin{align}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*contact\\
    &+ b_4*exposure*smoker + e \\
  attitude = &\ constant + b_1*0 + b_2*smoker + b_3*contact\\
    &+ b_4*0*smoker + e \\
  attitude = &\ constant + b_2*smoker + b_3*contact + e
  (\#eq:simplestatus) 
\end{align}
\normalsize
$$

Smoking status is a dichotomy, so it tells us the average difference in attitude between smokers and non-smokers. Due to the inclusion of the interaction variable, it now tells us the difference in average attitude between smokers and non-smokers who have zero exposure to the anti-smoking campaign. AS you see again, this is a conditional effect, not a main effect.

### Interpretation and statistical inference

```{r dich-moderator-output, echo=FALSE, message=FALSE, warning=FALSE}
# Table of regression coefficients for exposure moderated by status. Similar to SPSS output (with correct standardized coefficients).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
model_1 <- lm(attitude ~ exposure*status2, data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure"
attributes(results)$dimnames[[1]][3] <- "Status (smoker)"
attributes(results)$dimnames[[1]][4] <- "Exposure*Status (smoker)"
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Correctly standardized coefficients.
attach(smokers)
z_exposure <- (exposure - mean(exposure)/sd(exposure))
z_status2 <- (status2 - mean(status2)/sd(status2))
z_expostatus2 <- (exposure * status2 - mean(exposure * status2)/sd(exposure * status2))
z_attitude <- (attitude - mean(attitude)/sd(attitude))
model_2 <- lm(z_attitude ~ z_exposure + z_status2 + z_expostatus2)
results_2 <- coef(summary(model_2))
results <- cbind(results[, 1:2], results_2[, 1], results[, 3:6])
results[1, 3] <- NA
# Set column names.
attributes(results)$dimnames[[2]] <- c("B", "Std. Error", "Beta", "t", "Sig.", "Lower Bound", "Upper Bound")
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, caption = "Predicting attitude towards smoking: regression analysis results.")
# Helper function for displaying results within the text.
source("report_n.R")
#Cleanup (partial).
rm(smokers, model_1, smokers, ci, results_2, model_2, z_attitude, z_status2, z_expostatus2, z_exposure)
```

In Table \@ref(tab:dich-moderator-output), non-smokers are the reference group because they are coded 0 on the _Status_ variable. As a consequence, the regression coefficient for exposure gives us the effect of exposure on smoking attitude for non-smokers. It's value is `r report_n(results[2, 1], digits = 2)`, so an additional unit of exposure predicts a smoking attitude among non-smokers that is `r report_n(abs(results[2, 1]), digits = 2)` points more negative. More exposure to the campaign goes together with a more negative attitude towards smoking for non-smokers. The p value for this effect tests the null hypothesis that the effect is zero in the population. If we reject this null hypothesis, the exposure effect is statistically significant for non-smokers.

The effect of (smoking) status on attitude is conditional on exposure. The regression coefficient for status tells us the difference between smokers and non-smokers who have 0 exposure. So, without exposure to the campaign, smokers are on average `r report_n(results[3, 1], digits = 2)` more positive towards smoking than non-smokers. The p value tests the null hypothesis that the difference is zero for people without exposure to the anti-smoking campaign.

Smokers are coded 1 on the (smoking) status variable, so the regression coefficient for the interaction tells us that the slope of the exposure effect is `r report_n(abs(results[4, 1]), digits = 2)` lower for smokers than for non-smokers. In a preceding paragraph, we have seen that the estimated slope of the exposure effect is `r report_n(results[2, 1], digits = 2)` for non-smokers. We can add the regression coefficient of the interaction variable to obtain the estimated slope for smokers, which is `r report_n(results[2, 1] + results[4, 1], digits = 2)`. Now we can compare the two regression lines for the two groups, which gives good insight in the nature of moderation in this example. A graph of the two regression lines is probably the best way to communicate your results.

```{r dich-moderator-graph, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="The effects of exposure to the anti-smoking campaign on attitude towards smoking among smokers and non-smokers."}
# Graph of regression lines calculated in chunk dich-moderator-output.
# Plot.
ggplot() +
  geom_segment(aes(x = 0, xend = 10, 
                   y = results[1,1], yend = results[1,1] + 10*results[2,1],
                   colour = "Non-smokers"),
               size = 1
               ) +
  geom_segment(aes(x = 0, xend = 10, 
                   y = results[1,1] + results[3,1], 
                   yend = results[1,1] + results[3,1] + 10*(results[2,1] + results[4,1]),
               colour = "Smokers"),
               size = 1
               ) +
  scale_x_continuous(limits = c(0, 10), breaks = c(0, 10),
                     labels = c("No exposure", "Maximum\nexposure")) +
  scale_y_continuous(limits = c(-5, 5), breaks = c(-5, 0, 5), 
                     labels = c("negative", "neutral", "positive")) +
  scale_colour_manual(values = c("Non-smokers" = unname(brewercolors["Blue"]),
                                 "Smokers" = "Black")) + 
  labs(x = "Exposure to the anti-smoking campaign", y = "Attitude towards smoking") +
  #Legend definitions
  guides(colour = guide_legend(title = "Smoking status:"),
             fill = FALSE) + 
  theme_general() + 
  theme(legend.position = "top")
# Cleanup.
rm(results, report_n)
```

The interaction variable is treated as an ordinary predictor in the estimation process, so it receives a confidence interval and a p value. The null hypothesis is that the interaction effect is zero in the population. 

Remember that the regression coefficient for the interaction variable expresses the difference between the slope for the indicated group, e.g., smokers, and the slope for the reference group, e.g., non-smokers. If this difference is zero, as stated by the null hypothesis, the two groups have the same slope, so the effect is not moderated by the group variable.

So we know the confidence intervals and p values of the exposure effect for non-smokers and for the difference between their exposure effect and the exposure effect for smokers. We do not know, however, the confidence interval and statistical significance of the exposure effect for smokers. We cannot add confidence intervals or p values. 

If you want to know the confidence interval or p value of the exposure effect for smokers, you have to rerun the regression analysis using a different indicator variable for the moderator. You should create a dichotomous variable that assigns the 1 score to non-smokers and an interaction variable created with this dichotomy.

Interaction variables are used just like ordinary predictors, so the general assumptions or regression analysis apply. See Section \@ref(regr-inference) for a description of the assumptions and checks.

Let us conclude the interpretation with a warning. The standardized regression coefficients that SPSS reports must __not__ be used. They are calculated in the wrong way if the regression model includes an interaction variable. As a result, they are meaningless.

### Common support {#commonsupportdichotomous}

```{r common-support, fig.cap="Common support for the exposure predictor in each category of smoking status."}
# Goal: Sensitize students to the problem of lacking support for conditional effects by inspecting the coverage of the predictor for each moderator group.
# Generate a sample for smokers, for former smokers, and for non-smokers each of size 20. Give one or two randomly selected groups exposure values in the entire range [0, 10], but the remaining group(s) a restricted exposure range of 4 to 6 or 1 to 3 score points. Randomly assign a neutral, slightly negative, or moderately negative effect of exposure on attitude to the group. Display the three groups in a scatterplot (attitude by exposure) with different dot colours and their regression lines (coloured and labeled). Directly below the scatterplot, add a histogram of exposure, showing coverage. Allow the user to select groups 'All' (initial value), 'Smokers', 'Former smokers', or 'Non-smokers'. On selection, display the appropriate regression line and observations in the scatterplot and show their exposure scores in the histogram. Note that the scale of the histogram and the x axisof the scatterplot must be fixed to [0, 10]. Add a Generate New button to generate a new dataset.
```

1. What does the histogram represent in Figure \@ref(fig:common-support)?

2. Are the exposure values nicely spread for each smoke status group? Inspect each smoke status group separately with the "Show groups" option. 

3. What is the problem if exposure scores are not nicely spread within each smoke status group?

In a regression model with moderation, we have to interpret the effect of a predictor involved in the interaction at a particular value of the moderator (Section \@ref(conditional-effects)). The estimated effect at a particular value of the moderator can only be trusted if there are quite some observations at or near this value of the moderator. In addition, these observations should cover the full range of values on the predictor. After all, the effect that we estimate must tell us whether higher values on the predictor go together with higher (or lower) values on the outcome.

For example, we need quite some observations for smokers to estimate the conditional effect of exposure on attitude for smokers. If there are hardly any smokers in our sample, we cannot estimate the effect of exposure on attitude for them in a reliable way. Even if we have quite some observations for smokers but all smokers have low exposure, we cannot say much about the effect of exposure on attitude for them. If we cannot say much about the effect within this group, we cannot say much about the difference between this effect and effects for other groups. In short, the moderation model is problematic here.

The variation of predictor scores for a particular value of the moderator is called _common support_ [@RefWorks:3838]. If common support for predictors involved in moderation is bad, we should hesitate to draw conclusions from the estimated effects. Guidelines for good common support are hard to give. Common support is usually acceptable if there are observations over the entire range of the predictor. 

It is recommended to check the number of observations per value of the moderator. For a categorical moderator, such as smoking status, a scatterplot of outcome (vertical axis) by predictor (horizontal axis) with dots coloured according to the moderator category may do the job. Check that there are observations for more or less all values of the predictor. In Figure \@ref(fig:scatter-moderated), observations in each moderator category (dot colour) range from low to high predictor values.

```{r scatter-moderated, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="The effects of exposure to the anti-smoking campaign on attitude towards smoking among smokers and non-smokers."}
# Read data,
smokers <- haven::read_spss("data/smokers.sav")
# Graph of regression lines calculated in chunk dich-moderator-output.
# Plot.
ggplot(smokers, aes(x = exposure, y = attitude)) +
  geom_point(aes(colour = factor(status3, levels = c(0,1,2), 
                                 labels = c("Non-smoker", "Former smoker", "Smoker")))) +
  scale_x_continuous(limits = c(0, 10), breaks = c(0, 10),
                     labels = c("No exposure", "Maximum\nexposure")) +
  scale_y_continuous(limits = c(-5, 5), breaks = c(-5, 0, 5), 
                     labels = c("negative", "neutral", "positive")) +
  scale_colour_manual(values = c(unname(brewercolors["Blue"]), 
                                 unname(brewercolors["Orange"]), 
                                 unname(brewercolors["Red"]))) + 
  labs(x = "Exposure to the anti-smoking campaign", y = "Attitude towards smoking") +
  #Legend definitions
  guides(colour = guide_legend(title = "Smoking status:"),
             fill = FALSE) + 
  theme_general() + 
  theme(legend.position = "top")
# Cleanup.
rm(smokers)
```

```{r moderation-inference, fig.cap="Which null hypotheses are tested here?", eval=FALSE, echo=FALSE}
#DROPPED (added to Interpretation Section)

### Statistical inference for the interaction effect

# Goal: Understanding the null hypothesis for a conditional effect and an interaction effect by manipulating the population value of the conditional effects for (both) moderator groups.
# Generate a sample (N = 40?) from a population with exposure effect b = -.20 for non-smokers and b = .02 for smokers and an error term such that the former is significantly different from 0 but not the latter. Select a constant such that the predicted values are well within the [-5, 5] interval. Display a scatterplot (attitude by exposure) with the dots and regression lines for both groups. Add a table with the estimated regression coefficients for exposure, smoker, exposure-smoker interaction, and their standard error, t value, p value, and 95% confidence interval. Allow the user to change the conditional effects of exposure for both groups in the range [-.5, .5]. Then generate and display a new sample from a population with these values.

1. In Figure \@ref(fig:moderation-inference), what is the null hypothesis tested for the effect of exposure? 

2. How should you change the true values of the exposure effects to obtain a result that is not statistically significant?

3. What is the null hypothesis tested for the interaction effect of exposure with smoking status? 

4. How should you change the true values of the exposure effects to obtain a result that is not statistically significant?
```

### A categorical moderator
What if we have three or more groups in our moderator? For example, smoking status measured with three categories: (1) never smoked, (2) have smoked, (3) currently smoking? Does the effect of exposure on attitude vary between people who never smoked, stopped smoking, and are still smoking?

In Section \@ref(categorical-predictor), we learned that we must create dummy variables for all but one groups of a categorical predictor in a regression model. This is what we have to do also for a categorical moderator because we must include the (conditional) effects of the categorical variable in the model. If the effect of another predictor, such as exposure, is moderated by the categorical variable, we have to create an interaction variable for each dummy variable in the equation. To create the interaction variables, we multiply the predictor with the dummy variable as we have done before. 

```{r categorical-moderator, echo=FALSE, fig.cap="Statistical model with a moderator consisting of three groups. Non-smokers are the reference group", fig.asp=0.4}
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(rep.int(0.3, times = 5), 0.7), 
                        y = c(.5, .4, .3, .2, .1, .3),
                        label = c("Exposure", "Former smoker", "Smoker", "Expo*Former", "Expo*Smoker", "Attitude"))
# Add coordinates for arc endpoint.
x_diff <- 0.04
variables$xend <- variables$x[6] - x_diff #fixed translation to the left
variables$yend <- variables$y[6] + x_diff * (variables$y - variables$y[6]) / (variables$x[6] - variables$x)
rm(x_diff)
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = variables$x[1], y = variables$y[1], xend = variables$xend[1], yend = variables$yend[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[2], y = variables$y[2], xend = variables$xend[2], yend = variables$yend[2]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[3], y = variables$y[3], xend = variables$xend[3], yend = variables$yend[3]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[4], y = variables$y[4], xend = variables$xend[4], yend = variables$yend[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = variables$x[5], y = variables$y[5], xend = variables$xend[5], yend = variables$yend[5]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.1, 0.5)) +
  theme_void()
```

In the end, we have an interaction variable for all groups but one on the categorical moderator. Figure \@ref(fig:categorical-moderator) shows the statistical model. Estimation of the model yields point estimates (regression coefficients), confidence intervals, and p values for all variables (Figure \@ref(fig:cat-moderator-results)). 


```{r cat-moderator-results, echo=FALSE, message=FALSE, warning=FALSE}
# Table of regression coefficients for exposure moderated by status3. Similar to SPSS output (without correct standardized coefficients).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
model_1 <- lm(attitude ~ exposure*factor(status3, levels = c(0,1,2)), data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure"
attributes(results)$dimnames[[1]][3] <- "Former smoker"
attributes(results)$dimnames[[1]][4] <- "Smoker"
attributes(results)$dimnames[[1]][5] <- "Exposure*Former smoker"
attributes(results)$dimnames[[1]][6] <- "Exposure*Smoker"
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Set column names.
attributes(results)$dimnames[[2]] <- c("B", "Std. Error", "t", "Sig.", "Lower Bound", "Upper Bound")
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, caption = "Predicting attitude towards smoking for three smoking status groups: regression analysis results.")
# Cleanup.
rm(smokers, model_1, smokers, ci, results)
```

Remember that the effects of predictors that are included in interactions are conditional effects: effects for the reference group or reference value. The p value for _Exposure_ tests the hypothesis that the exposure effect for people who never smoked is zero in the population. For the two dummy variables _Former smoker_ and _Smoker_, the null hypothesis is tested that they have the same average attitude in the population as the non-smokers (reference group) if they are not exposed to the anti-smoking campaign.

Interaction predictors show effect differences. In Table \@ref(tab:cat-moderator-results), the interaction predictors test the null hypothesis that the effect of exposure is equal for non-smokers and former smokers (*Exposure\*Former smoker*) or for non-smokers and smokers (*Exposure\*Smoker*) in the population.

If we would like to know whether the exposure effect for former smokers is significantly different from zero, we have to rerun the regression model using the people who stopped smoking as reference group. This new model would also tell us whether the exposure effect for people who stopped smoking is significantly different from the exposure effect for people who are still smoking.

## A Dichotomous or Categorical Moderator in SPSS {#catmodSPSS}

### Instructions

```{r SPSSregpred, echo=FALSE, out.width="640px", fig.cap="(ref:regpredSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/pv6JnWL4nX8", height = "360px")
# Goal: Creating categorical by continuous interaction predictors (2 methods).
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable status2 and status3), use contact with smokers as a covariate.
# SPSS menu: Transform > Compute Variable for status2 (already 0/1 variable) ; Transform > Create Dummy Variables for status3 (3 categories)
# Interpret output: none.

# Create interaction predictors (also for cont*cont interaction variables in addition to interactions with dummies) and dummies for main effects in one go: 
#   - ensure that categorical variables are marked as Nominal or Ordinal in Variable View
#   - command Transform > Create Dummy Variables ; select (numeric) predictor and (categorical) moderator under 'Create Dummy Variables for:'
#   - under Create main-effect dummies (option checked by default) specify a short name for both variables, separated with a comma ; the name of the numeric variable is irrelevant but must be specified
#   - ensure that the option _Do not create dummies for scale variables values_ is selected under Measurement Level Usage
#   - select the _Create dummies for all two-way interactions_ option under Two-Way Interactions and give a short name, e.g., interact
#   - Note: this procedure can also be used to create a numeric by numeric interaction variable
```

```{r SPSSregcatmod, echo=FALSE, out.width="640px", fig.cap="(ref:regcatmodSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/A2VHIsYA66M", height = "360px")
# Goal: Estimating categorical by continuous moderation with regression in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# SPSS menu: linear regression, include descriptives for means and standard deviations of covariates for making reference lines.
# Interpret output: use unstandardized regression coefficients because they show the average difference in slope (effect size) ; do not use the standardized regression coefficients that SPSS reports: they are calculated in the wrong way if the regression model includes an interaction variable. As a result, they are meaningless ; better interpret the regression lines in a scatterplot, see another video
# Check assumptions: See other video.
``` 

```{r SPSSregmodlines, echo=FALSE, out.width="640px", fig.cap="(ref:regmodlinesSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/qZbdMdNoldI", height = "360px")
# Goal: Representing moderation by regression lines in a scatterplot in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# Technique: linear regression
# SPSS menu: 
# Interpret output: 

# * Visualize categorical moderator with reference lines in scatterplot:
#   - {skip} No covariates: Graphs > Regression Variable Plots: outcome on Y, predictor on X, categorical moderator in Color by, Options>Scatterplot Fit Lines > Linear and Grouping > Fit line for each categorical color group
#   - With covariates: 
#     - create scatterplot with Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter
#     - select outcome variable under Y Axis: and (numeric) predictor under X Axis: 
#     - select (original) categorical moderator under Set Markers by: (colours the observations according to moderator value, useful for inspecting common support)
#     - Paste & Run 
#     - display the mean values of covariates with Analyze > Descriptive Statistics > Fequencies ; select Statistics > Mean
#     - in SPSS Output, double-click the scatterplot to open it in the Chart Editor 
#     - add reference line (Options > Reference Line from Equation) ; in the Properties window under the Reference Line tab, add the regression equation for the first group in the moderator variable: use x for the predictor displayed on the X axis and use the category value (0/1) for dummies and the average values for numeric covariates
#     - repeat for other categories of the moderator variable
#     - change type (or colour) of the line in the Properties window under the Lines tab
#     - if you like, add label to lines describing the moderator group: Options > Text Box
#     - close the Chart Editor
```

```{r SPSSregSupport1, echo=FALSE, out.width="640px", fig.cap="(ref:regSupport1SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/d93Kvm0imio", height = "360px")
# Goal: Check common support for a predictor at different moderator values in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# Technique: 
# SPSS menu: 
# Interpret output: 
# Check assumptions: 
#
# * Check common support: 
#   - make histograms of the predictor panelled by the categorical grouping variable ; check that there are observations for more or less all values of the predictor (on the X axis) 
```

### Exercises

1. Use the data in <a href="http://82.196.4.233:3838/data/smokers.sav" target="_blank">smokers.sav</a> to predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_). Use contact with smokers as a covariate. Check the assumptions for regression analysis and interpret the results.

```{r eval=FALSE}
SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=exposure status2 contact attitude
  /ORDER=ANALYSIS.
* Compute interaction variable.
COMPUTE expo_status=exposure * status2.
VARIABLE LABELS  expo_status 'Interaction exposure * smoker'.
EXECUTE.
* Multiple regression.
REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT attitude
  /METHOD=ENTER exposure status2 expo_status contact
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).

Check data:

All values seem to be valid.

Check assumptions:

The residuals are skewed (long tail to the left). The residuals seem to
average to zero on most levels of the predicted outcome, so a linear model
seems to fit.
* However, the lower attitude values are predicted worse (more variation) than
the higher levels. The assumptions do not seem to be strongly violated but our
model may not be specified all well.

Interpret the results:

Add table with regression coefficients.

* We can predict smoking attitude for about 26 percent with the regression
model, R2 = .26, F (4, 80) = 7.10, p < .001.
* The predictive effect of exposure to the anti-smoking campaign on smoking
attitude for non-smokers is more probbaly negative than positive but it is not
significantly different from zero, b = -0.12, t = -1.79, p = .078,
95%CI[-0.25, 0,01]). More exposure tends to yield a more negative attitude.
* For smokers, the predictive effect of campaign exposure on smoking attitude
is more strongly negative. The moderation of the exposure effect by smoking
status is negative and statistically significant, b = -0.33, t = -2.35, p =
.021, 95%CI[-0.61, -0.05].
* More contact with smokers is associated with a more positive attitude
towards smoking rather than a negative attitude. The predictive effect is weak
(b* = 0.18) but not significantly different from zero, b = 0.15, t = 1.68, p =
.096, 95%CI[-0.03, 0.33].
* Smokers have a more positive attitude than non-smokers if they are not
exposed to the campaign, on average circa 2 (0.5 to 3.4) points more positive
the attitude, and this difference is statistically significant, b = 1.98, t =
2.72, p = .008, 95%CI[0.53, 3.43].

Remember: 
* In the presence of an interaction effect, the partial effect of a predictor
is the effect for the reference group or value. It is NOT an overall or
average effect as in analysis of variance.
* Standardized regresion coefficients reported by SPSS are not correct for
interaction effects or effects of predictors that are involved in interaction
effects. They can only be used for predictors that are not involved in
interaction effects.
```

2. Visualize the moderated effects of exposure on attitude (Exercise 1). Create a scatterplot with two regression lines. Colour the regression lines and the dots (respondents) according to their smoking status category. Interpret the moderation of the exposure effect by smoking status.

```{r eval=FALSE}
* First of all, you must write the regression equations for different values
of the moderator. Plug in the estimated values of the regression coefficients
and the means of covariates.

attitude = constant + -.118*exposure + 1.982*status +
  -.329*exposure*status + .152*contact

attitude = -.087 + -.118*exposure + 1.982*status + 
  -.329*exposure*status + .152*5.091

attitude = -.087 + -.118*exposure + 1.982*status + 
  -.329*exposure*status +.774

attitude = .687 + -.118*exposure + 1.982*status + 
  -.329*exposure*status

Non-smokers (0):

attitude = .687 + -.118*exposure + 1.982*0 + 
  -.329*exposure*0

attitude = .687 + -.118*exposure

Smokers (1):

attitude = .687 + -.118*exposure + 1.982*1 + -.329*exposure*1

attitude = .687 + 1.982 + -.118*exposure + -.329*exposure

attitude = 2.669 + (-.118 + -.329)*exposure

attitude = 2.669 + -.447*exposure

* Next, create a scatterplot of attitude by exposure, colouring the dots by
smoking status. Use the calculated two equations in the SPSS Chart Editor to
create two lines. Use the icon "Add a reference line from Equation" for each
line. Enter the the equation using x instead of exposure as the predictor.
Colour the lines with the colours of the dots in the scatterplot.

SPSS syntax:

* Scatterplot with dots coloured by smoking status.
GRAPH
  /SCATTERPLOT(BIVAR)=exposure WITH attitude BY status2
  /MISSING=LISTWISE.

Check data:

See Exercise 1.

Check assumptions:

See Exercise 1.

Interpret the results:

* For smokers, the predictive effect of campaign exposure on smoking attitude
is more strongly negative than for non-smokers.
```

3. Check the common support of the predictor (exposure) in all groups of the moderator (smoking status). Could you also check common support with the scatterplot you made for Exercise 2?

```{r eval=FALSE}
SPSS syntax:

* Histogram of predictor (exposure) for each smoking status.
GRAPH
  /HISTOGRAM=exposure
  /PANEL ROWVAR=status2 ROWOP=CROSS.

Interpret the results:

* Even for smokers, the much smaller group, we have exposure scores over
(almost) the entire range. We have good coverage both for smokers and
non-smokers. Do not mind the gap inscores around 6: we have plenty of
observations around 5 and 7.

* We could have seen this result in the scatterplot because we had green and
blue dots across the entire width of the plot.
```

4. Repeat the analyses of Exercises 1 through 3 but use smoking status with three categories (_status3_).

```{r eval=FALSE}
SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=exposure status3 contact attitude
  /ORDER=ANALYSIS.
* Create dummies and iteraction variables.
* ENSURE THAT MEASUREMENT LEVEL IS SET TO ORDINAL.
* Define Variable Properties.
*status3.
VARIABLE LEVEL  status3(ORDINAL).
EXECUTE.
SPSSINC CREATE DUMMIES VARIABLE=exposure status3 
ROOTNAME1=exposure, status ROOTNAME2=expo_status 
/OPTIONS ORDER=A USEVALUELABELS=YES USEML=YES OMITFIRST=NO.
* Multiple regression.
* Statistic Descriptives is added to get the means that we need
* to plug into the regression equation in the moderaiton plot.
REGRESSION
  /DESCRIPTIVES MEAN STDDEV CORR SIG N
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT attitude
  /METHOD=ENTER exposure status_3 status_4 expo_status_2_2 expo_status_2_3 contact
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).
* Scatterplot with dots coloured by smoking status.
GRAPH
  /SCATTERPLOT(BIVAR)=exposure WITH attitude BY status3
  /MISSING=LISTWISE.
* Histogram of predictor (exposure) for each smoking status.
GRAPH
  /HISTOGRAM=exposure
  /PANEL ROWVAR=status3 ROWOP=CROSS.

* Write out the regression equations for all three groups of the moderator.
Plug in the estimated values of the constant and the regression coefficients.

attitude = .550 + -.137*exposure + -1.139*former + 1.223*smoker +
  -.402*exposure*former + -.304*exposure*smoker + .171*contact

Plug in the means of covariates and add to constant:
  
attitude = .550 + -.137*exposure + -1.139*former + 1.223*smoker +
-.402*exposure*former + -.304*exposure*smoker + .171*5.091

attitude = 1.421 + -.137*exposure + -1.139*former + 1.223*smoker +
-.402*exposure*former + -.304*exposure*smoker

Non-smokers (former = 0, smoker = 0):

attitude = 1.421 + -.137*exposure + -1.139*0 + 1.223*0 + -.402*exposure*0 +
-.304*exposure*0

attitude = 1.421 + -.137*exposure

Former smokers (former = 1, smoker = 0):

attitude = 1.421 + -.137*exposure + -1.139*1 + 1.223*0 + -.402*exposure*1 +
-.304*exposure*0

attitude = 1.421 + -1.139 + -.137*exposure + -.402*exposure

attitude = 0.282 + (-.137 + -.402)*exposure

attitude = 0.282 + -.539*exposure

Smokers (former = 0, smoker = 1):

attitude = 1.421 + -.137*exposure + -1.139*0 + 1.223*1 + -.402*exposure*0 +
-.304*exposure*1

attitude = 1.421 + 1.223 + (-.137 + -.304)*exposure

attitude = 2.644 + -.441*exposure

* Create a scatterplot of attitude by exposure, colouring the dots by smoking
status.
* Use the calculated two equations in the SPSS Chart Editor to create two
lines. Use the icon "Add a reference line from Equation" for each line. Enter
the the equation using x instead of exposure as the predictor. Colour the
lines with the colours of the dots in the scatterplot.

Check data:

All values seem to be valid.

Check assumptions:

* The residuals seem to be skewed a little bit.
* The residuals by predicted values plot gives no reason to doubt the
linearity of the model but the problem of predicting higher values less
accurately than lower values seems to be worse than in Exercise 1. We should
warn the reader that the assumptions seem to be violated.

Interpret the results:

* Again, summarize the tests on the regression coefficients in a table.

* With three smoking status groups, we can predict attitude towards smoking
much better (R2 = 0.70) than with the two groups in Exercise 1 (R2 = .26).

* There is an important difference between non-smokers
and former smokers (they were lumped together in
the preceding exercises). On average, former smokers
have a more negative attitude than non-smokers,
which may range from a tiny difference (-0.1) to a
large difference (2 points on the scale from -5 to +5).

* In addition, exposure has a stronger negative predictive effect on smoking
attitude among former smokers than among non-smokers. Exposure also has a
stronger negative effect on attitude among smokers than among non-smokers. For
short, exposure to the campaign has less impact on attitude towards smoking
for non-smokers than for former smokers or smokers.

* The coverage of exposure is good for non-smokers and smokers but former
smokers with high exposure are rare.
```

## A Continuous Moderator {#cont-moderator-regression}

```{r continuous-moderator, fig.cap="How do contact values affect the conditional effect of exposure on attitude?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="440px"}
# Goal: Understand that there is a conditional effect for each value of the moderator by gradually changing the moderator value & understanding the linearity of the effect: a fixed slope change for a fixed difference in moderator values.
# Generate a data set with a linear interaction (attitude ~ exposure*contact). Display a scattergram with a regression line for the current value of the moderator (contact). Display regression equation as y = a + (b_1 + b_3*contact(5))*exposure + b_2*contact(5) with values for coefficients and for contact. Add slider allowing the user to change the moderator value (range [0, 10], initial value 0). Replace the previous regression line by a grey line, remove older regression lines, and add new regression line in black; also update regression equation.
# # Number of observations.
# n <- 85
# # Create predictor.
# set.seed(4932)
# exposure <- runif(n)*10
# # Create moderator.
# set.seed(4321)
# contact <- 0.12*(10 - exposure) + rnorm(n, mean = 4.5, sd = 2)
# # Create outcome.
# set.seed(390)
# attitude <- -0.26*exposure + 0.15*contact + 0.04*exposure*contact + rnorm(n, mean = 2, sd = 0.5)
knitr::include_app("http://82.196.4.233:3838/apps/continuous-moderator/", height="550px")
```

1. The regression line depicted in Figure \@ref(fig:continuous-moderator) represents the conditional effect of exposure on attitude for the value of contact with smokers selected with the slider. How many different conditional effects are there?

```{r eval=FALSE}
* In principle, there is an unlimited number of conditional effects because
(or if) the moderator is a continuous variable.
* In the app, however, the slider allows you to increase or decrease contact
score by 0.5. In the app, there are effectively 21 moderator values that you
can select, so there are 21 different regression lines that can be depicted.
```

2. Is the effect of campaign exposure on attitude towards smoking always negative? Or does more exposure lead to a more positive attitude (higher score) in some cases? If so, in which cases?
```{r eval=FALSE}
* Move the slider from left to right to find the moderator value at which the
regression line is horizontal. For higher moderator values, the slope of the
regression line is positive. Here, more exposure leads to a more positive
attitude.
* Or have a close look at the equations. The regression coefficient of the
(simple) slope of the exposure effect is zero if the part between brackets (b1
+ b3*contact) is zero. We know b1 and b3, so we must solve the equation:

> -0.26 + 0.04 * contact = 0

Your high-school algebra may help you:

> 0.04 * contact = 0.26

> contact = 0.26 / 0.04

> contact = 0.26 / 0.04 = 6.5
```

3. How much does the slope increase if the moderator value is changed from 0 to 1? And how much if it changes from 6 to 7? 
```{r eval=FALSE}
* Each increment of 1 unit of contact increases the (simple) slope of the
exposure effect on attitude by 0.04, that is, by the value of the interaction
effect.
* This is easy to see in the equation for the effect of exposure:

> (-0.26 + 0.04 * contact) * exposure

* Plug in 0 for contact: The simple slope is -0.26.
* Plug in 1 for contact: The simple slope is -0.26 + 0.04 * 1.
* The difference is 0.04. This difference is the same for every increase of 1
unit in contact, so it is also the difference between the slopes at contact
levels six and seven.
```

People hanging around a lot with smokers are likely to have a more positive attitude towards smokers than people who have little contact with smokers. After all, people who really hate smoking will avoid meeting smokers. This is a main effect of contact with smokers on attitude towards smoking.

In addition, the anti-smoking campaign may be less effective for people who spend a lot of time with smokers. Negative perceptions of smoking instilled by the campaign can be compensated by positive experiences of seeing people enjoy smoking. Contact with smokers would decrease the effect of campaign exposure on attitude. The effect of exposure is moderated by contact with smokers.

Our moderator, contact with smokers, is continuous. As a consequence, we can have an endless number of contact levels as groups for which the slope may change. This is the only difference with a categorical moderator. Other than that, we will analyze a continuous moderator in the same way as we analyzed a categorical moderator.

### Interaction variable {#interpret-cont-interaction}

We need one interaction variable to include a continuous moderator in a regression model. As before, the interaction variable is the product of the predictor and the moderator. 

Although we have an endless number of different moderator values or groups, we only need one interaction variable. It represents the gradual (linear) change of the effect of the predictor for higher values of the moderator. 

To see this, it is helpful to inspect the regression equation with rearranged terms (Equation \@ref(eq:simplecontact)). Every little bit of extra contact with smokers adds to the slope $(b_1 + b_3*contact)$ of the exposure effect. The addition is gradual---a little bit of additional contact with smokers changes the exposure effect a little bit---and it is linear: A unit increase in contact adds the same amount to the effect whether the effect is at a low or a high level.

$$
\small
\begin{align}
  attitude = &\ constant + b_1*exposure + b_2*contact + b_3*exposure*contact + e \\
  attitude = &\ constant + (b_1 + b_3*contact)*exposure + b_2*contact + e 
  (\#eq:simplecontact) 
\end{align}
\normalsize
$$

We can interpret the regression coefficient of the interaction effect ($b_3$) here as the predicted difference of the exposure effect (slope) for a one unit difference in contact (the moderator). A positive coefficient indicates that the exposure effect is more positive for higher levels of contact with smokers. A negative coefficient indicates that the effect is more negative for people with more contacts with smokers. 

Note that positive and negative are used here in there mathematical meaning, not in an appreciative way. A positive effect of exposure implies a more positive attitude towards smoking. Anti-smoking campaigners probably evaluate this as a negative result.

### Conditional effect {#conditional-effect-cont}

The regression coefficients for exposure and contact represent conditional effects (see Section \@ref(conditional-effects)). These are the effects for cases that score zero on the other variable. Plug in zero for the moderator and you see that all terms with a moderator drop from the equation and only $b_1$ is left as the effect of exposure.  

$$
\small
\begin{align}
  attitude = &\ constant + (b_1 + b_3*contact)*exposure + b_2*contact + e\\
  attitude = &\ constant + (b_1 + b_3*0)*exposure + b_2*0 + e\\
  attitude = &\ constant + b_1*exposure + e 
\end{align}
\normalsize
$$

The zero score on the moderator is the _reference value_ for the conditional effect of the predictor. Cases that score zero on the moderator are the _reference group_ just like cases scoring zero on the dummy variables are the reference group in a model with a categorical moderator (Section \@ref(dichpredictor)). 

### Mean-centering

```{r mean-centering-moderator, fig.cap="What happens if you mean-center the moderator variable?"}
# Goal: Understand how mean-centering affects the interpretion of the conditional effect of the predictor by seeing how the reference value changes with mean-centering (and centering on another value, e.g., M plus/minus 1 SD).
# Use same data set as in app continuous-moderator. Display scatterplot with conditional regression effect for predictor at moderator value = 0. Shade dots in scatterplot in accordance with distance of their moderator score to the moderator reference value. Add slider 'Status - x' (equal length as two x axes, range [0, 10], initial value 0), labeled with values M - SD, M, and M + SD. Adjusting the slider updates the regression line in the scatterplot.
```

1. What happens to the reference value of the moderator if you subtract mean (*M*) contact with smokers from the respondents' contact scores? Use the slider in Figure \@ref(fig:mean-centering-moderator) to check your answer.

2. What happens to the regression line if you mean-center (Question 1) the moderator (contact with smokers)?

3. What do you think expresses the shade of the dots in the scatterplot?

What if there are no people with zero contact? Then, the interpretation of the regression coefficient $b_1$ for exposure does not make sense. In this situation, it is better to mean-center the moderator (contact) before you add it to the regression equation and before you calculate the interaction variable. 

To _mean-center_ a variable, you subtract the variable's mean from all scores on the variable. As a result, a mean score on the original variable becomes a zero score on the mean-centered variable. 

$$
\small
\begin{align}
  contactcentered = contact - mean(contact)
\end{align}
\normalsize
$$

With mean-centered numerical moderators, a conditional effect in the presence of interaction always makes sense. It is the effect of the predictor for average score on the moderator. An average score always falls within the range of scores that actually occur. If we mean-center the contact with smokers moderator, the regression coefficient $b_1$ for exposure expresses the effect of exposure on attitude for people with average contacts with smokers. This makes sense.

Remember that the interaction variable is the product of the predictor and moderator (Section \@ref(interaction-variable)). If any or both of these are mean-centered, you should multiply the mean-centered variable(s) to create the interaction variable, see Sections \@ref(conditional-effects) and \@ref(conditional-effect-cont).

### Symmetry of predictor and moderator

```{r symmetry-predictor-moderator, fig.cap=""}
# Goal: Understand the advantages of mean-centering the predictor by seeing how the reference value changes with mean-centering (and centering on another value, e.g., M plus/minus 1 SD).
# Use same data set as in app continuous-moderator. Display scatterplot (x axis not labelled) with conditional regression effect for predictor (blue) at moderator value = 0 and conditional effect of moderator (red) for predictor = 0. Show two additional x axes marking the reference values of the predictor (blue) and moderator (red) (range [0, 10], initial value 0) that is currently the reference value. Add sliders 'Exposure - x' and 'Status - x' (equal length as two x axes, range [0, 10], initial value 0), labeled with values M - SD, M, and M + SD. Adjusting the sliders update the scale of the appropriate x axis (the marked point zero moves) and the regression lines in the scatterplot.
```

1. If you change the value on the slider 'Exposure - x', which regression line in the plot changes? Why this line?

2. Which variable is the predictor and which is the moderator if you adjust the value of the slider 'Exposure - x'?

If we want to interpret the conditional effect of contact on attitude ($b_2$), we must realize that this is the effect for people who score zero on the exposure variable. This is clear if we rearrange the regression equation as in Equation \@ref(eq:contactbyexposure).

$$
\small
\begin{align}
  attitude = &\ constant + b_1*exposure + b_2*contact + b_3*exposure*contact + e\\
  attitude = &\ constant + b_1*exposure + (b_2 + b_3*exposure)*contact + e\\
  attitude = &\ constant + b_1*0 + (b_2 + b_3*0)*contact + e\\
  attitude = &\ constant + b_2*contact + e(\#eq:contactbyexposure) 
\end{align}
\normalsize
$$

But wait a minute, this is what we would do if contact was the predictor and exposure the moderator. That is a completely different situation, is it not? No, technically it does not make a difference which variable is the predictor and which is the moderator. The predictor and moderator are symmetric. The difference is only in our theoretical expectations and in our interpretation.

The conditional effect of the moderator, as stated above, is the effect of the moderator if the predictor is zero. This interpretation makes sense only if there are cases with zero scores on the predictor. In the current example, the scores on exposure range from 0 to 10, so zero exposure is meaningful. But it represents an eccentric score with perhaps a very atypical effect of contact on attitude or few observations. For these reasons, it is recommended to *mean-center both the predictor and moderator if they are numeric*.

Mean centering does not change the interpretation of regression coefficients. An unstandardized regression coefficient still tells us the predicted difference in the outcome variable for a one unit difference in the predictor. Mean centering only changes the reference value, that is, the value of the other variable in the interaction to which the regression coefficient applies.

### Visualization of the interaction effect

```{r continuous-interaction-visualization, fig.cap="Which moderator values are helpful for visualizing moderation?"}
# Goal: Clarify the interpretation of the (unstandardized) interaction effect by showing regression lines at different (interesting) moderator scores (display slope value).
# Variant of the app continuous-moderator; ensure that there are few prdictor values at the minimum and maximum values of the moderator. Allow user to pick several values for the moderator from a list containing: minimum, maximum, first quartile, median, third quartile, 33% percentile, 67% percentile, M - 2SD, M - 1SD, M, M + 1SD, M + 2SD. Display the selected lines in different colours.
```

1. Which moderator values would you pick to communicate the results of moderation? Motivate your answer.

2. Which sets of options give similar results?

As we have seen in Section \@ref(interpret-cont-interaction), the regression coefficient of an interaction effect with a continuous moderator can be directly interpreted. It represents the predicted difference in the unstandardized effect size for a one unit increase in the moderator. For example, one more contact with a smoker increases the exposure effect by 0.04.

The size of the interaction effect tells us the moderation trend, for example, people who are more around smokers tend to be less opposed to smoking if they are exposed to the anti-smoking campaign. But we do not know how much an anti-smoking attitude is fostered by exposure to a campaign and whether exposure to the campaign increases anti-smoking attitude for everyone. Perhaps, people hanging out with smokers a lot may even get a more positive attitude towards smoking from campaign exposure.

We can be more specific about exposure effects at different levels of contact with smokers if we pick some interesting values of the moderator and calculate the conditional effects at these levels.

The minimum or maximum values of the moderator are usually not very interesting. We tend to have few observations for these values, so our confidence in the estimated effect at that level is low. Instead, the values one standard deviation below and above the mean of the moderator are popular values to be picked. One standard deviation below the mean (M - SD) indicates a low value, the mean (M) indicates a central value, and one standard deviation above the mean (M + SD) indicates a high value. 

Having picked these values, we can visualize moderation as different regression lines in a plot. We use exactly the same approach as in visualizing moderation by a categorical variable. AS a first step, we construct equations for conditional effects of the predictor at different levels of the moderator. Plug the selected value of the moderator into the regression equation. If there are covariates, we must also plug in a meaningful value for the covariates, usually the average for numeric covariates and zero or one for dichotomous covariates. As a second step, we use the equations to add regression lines to a scatterplot.

If contact (the moderator) is mean-centered, as in the current example, we simply plug in zero for the moderator to obtain the equation for the regression line at the mean of the moderator (contact with smokers). We plug in the value of the standard deviation of contact with smokers to get the regression equation for people who scored one standard deviation above the mean on the moderator. The standard deviation of contact is 2.0 in this example, so Equation \@ref(eq:regsimpleslopemoderated) replaces contact by 2.0 everywhere. Finally, we plug in minus the value of one standard deviation if we want the regression line for the moderator at the mean minus one standard deviation.

We also have to plug in a value for each covariate. This example contains one covariate, namely (smoking) status. We plug in the score for non-smokers (0). In the end, our predictor (exposure) should be the only variable in the right hand side of the regression equation (the last line in Equation \@ref(eq:regsimpleslopemoderated)). Note that we do not include the error term ($e$) in the equation if we predict values; the error term captures prediction errors.

$$ 
\small
\begin{align}
  attitude &= 3.6 + -0.1*exposure + 0.1*status + 0.1*contact \\
  &\ + 0.03*contact*exposure \\
  attitude &= 3.6 + -0.1*exposure + 0.1*(0) + 0.1*(2.0) + 0.03*(2.0)*exposure \\
    attitude &= 3.4 + -0.04*exposure (\#eq:regsimpleslopemoderated) 
\end{align}
\normalsize
$$

If the moderator is not mean-centered, we have to plug in the value of the mean of the moderator and the value of the mean plus or minus the standard deviation of the moderator. In this example, the mean score of contact with smokers is 5.1, so the moderator mean minus one standard deviation (2.0) equals 3.1 and the mean plus one standard deviation is 7.1.

### Statistical inference on conditional effects

```{r cont-moderator-output, echo=FALSE, message=FALSE, warning=FALSE}
# Table of regression coefficients for the effect of exposure moderated by contact with smokers. Similar to SPSS output (with standardized coefficients?).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
# Mean-center numerical predictors.
smokers$exposure_mc <- smokers$exposure - mean(smokers$exposure)
smokers$contact_mc <- smokers$contact - mean(smokers$contact)
# Unsandardized linear model.
model_1 <- lm(attitude ~ exposure_mc*contact_mc + status2, data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure (mean-centered)"
attributes(results)$dimnames[[1]][3] <- "Contact (mean-centered)"
attributes(results)$dimnames[[1]][4] <- "Status (smoker)"
attributes(results)$dimnames[[1]][5] <- "Exposure*Contact (mean-centered)"
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Correctly standardized coefficients.
attach(smokers)
z_exposure <- (exposure_mc - mean(exposure_mc)/sd(exposure_mc))
z_contact <- (contact_mc - mean(contact_mc)/sd(contact_mc))
z_status2 <- (status2 - mean(status2)/sd(status2))
z_expocontact <- (exposure_mc * contact_mc - mean(exposure_mc * contact_mc)/sd(exposure_mc * contact_mc))
z_attitude <- (attitude - mean(attitude)/sd(attitude))
model_2 <- lm(z_attitude ~ z_exposure + z_contact + z_status2 + z_expocontact)
results_2 <- coef(summary(model_2))
results <- cbind(results[, 1:2], results_2[, 1], results[, 3:6])
results[1, 3] <- NA
# Set column names.
attributes(results)$dimnames[[2]] <- c("B", "Std. Error", "Beta", "t", "Sig.", "Lower Bound", "Upper Bound")
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, caption = "Predicting attitude towards smoking: regression analysis results with exposure and contact mean-centered.")
# Helper function for displaying results within the text.
source("report_n.R")
# Partial cleanup.
rm(smokers, model_1, smokers, ci, results_2, model_2, z_attitude, z_contact, z_status2, z_expocontact, z_exposure)
```

The regression model yields a p value and confidence interval for the predictor at the reference value of the moderator. In the model estimated in Table \@ref(tab:cont-moderator-output), for example, we obtain a p value of `r report_n(results[2,5], 3)` and a 95% confidence interval of [`r report_n(results[2,6],2)`, `r report_n(results[2,7],2)`] for the effect of exposure on attitude. This is the conditional effect of exposure on attitude for cases that score zero on the moderator variable (contact with smokers) as we can verify in Equation \@ref(eq:statinfconditional).

$$
\small
\begin{align}
  attitude = &\ constant + b_1*exposure + b_2*contact + b_3*status + b_4*exposure*contact + e\\
    attitude = &\ constant + (b_1 + b_3*contact)*exposure + b_2*contact + e\\
        attitude = &\ constant + (b_1 + b_4*0)*exposure + b_2*0 + b_3*status + e\\
        attitude = &\ constant + b_1*exposure + b_3*status + e(\#eq:statinfconditional) 
\end{align}
\normalsize
$$

If the variable contact is mean-centered, the p value tests the null hypothesis that the effect of exposure is zero for people who have average contact with smokers. The confidence interval tells us that the effect of exposure on attitude for people with average contacts with smokers ranges between `r report_n(results[2,6],2)` and `r report_n(results[2,7],2)` with 95% confidence. If the moderator is not mean-centered, the results apply to people who have no contact with smokers.

Note that mean-centering of the moderator changes, so to speak, the regression line that we test from the effect of exposure for people with no smoker contact to the effect for people with average contact with smokers. If we would like to get the p value or confidence interval for the regression line at one standard deviation above or below the mean, we have to center the moderator at those values before we estimate the regression model.

```{r echo=FALSE}
#Cleanup.
rm(results, report_n)
```

### Common support

In Section \@ref(commonsupportdichotomous), we checked the support of the predictor in the data for different groups of the moderator. The basic idea is that we can only sensibly estimate and interpret a conditional effect at a moderator level if we have observations over the entire range of the predictor. For each moderator group, we checked the distribution of the predictor.

With a continuous moderator we can also do this if we group moderator scores. Hainmueller et all. [-@RefWorks:3838] recommend creating three groups, each containing one third of all observations. These low, medium, and high groups correspond more or less with the minus one standard deviation/mean/plus one standard deviation values that we used for visualizing and testing conditional effects. Create a histogram for the predictor in each of these groups to check common support of moderation in the data.

### Assumptions

The general assumptions for regression analysis (Section \@ref(regr-inference)) also apply to the interaction effect with a continuous moderator. The checks are the same: See if the residuals are more or less normally distributed and check the residuals by predicted values plot.

Note that the linearity assumption also applies to the interaction effect. If the interaction effect is positive, the exposure (predictor) effect must be higher for higher values of contact with smokers (moderator). More precisely, a unit difference on the moderator should result in a fixed increase (or decrease) of the effect of the predictor. You may have noticed this linear change in the effect size in Figure \@ref(fig:continuous-moderator) at the beginning of this section on continuous moderators.

If we would estimate a separate regression model for each selected moderator group, the linearity assumption says that the regression coefficients for the predictor should only go up (or down) if we progress from lower moderator scores to higher moderator scores. For example, the exposure effect for the 33% of all people with least contacts with smokers should be below the exposure effect for the 33% of all people with medium contact scores, which should be below the effect for people with most contact scores.

In principle, we can execute a separate regression analysis for each moderator group if we have a large sample, for example, over 200 cases. There are some complications, however, so let us not pursue this here.

```{r eval=FALSE, echo=FALSE}
### Comparing nested regression models

Discuss F Change test here with distinction between 'main' effects in model without interaction predictor and conditional effects in (nested?) model with interaction predictor ; additional SPSS clip?
Cf. Fam: Discuss a two-step approach to moderation? In the first model estimate effects without the interaction predictor, so we have the average or main effects (as in ANOVA). In the second model, add the interaction predictor. Now, the former main effect is the effect for the reference group or value (zero) on the moderator.
Pros/cons: Adds F Change test; F Change test does not add to significance test of interaction predictor?; highlights interpretation difference of seemingly the same effect (main effect becomes conditional effect); main effects are interesting only if there is no interaction effect?
```

### Higher-order interaction effects

An interaction effect with one moderator, albeit continuous or categorical, is called a _first-order interaction_. It is possible to have a moderated effect that is moderated itself by a second moderator. For example, the change in the exposure effect due to a person's contact with smokers may be different for smokers than for non-smokers. This is called a _second-order interaction_ or _higher order interaction_. We can include more moderators, yielding even higher higher-order interactions, such as three or four moderators.

An interaction variable that is the product of the predictor and two moderators can be used to include a second-order interaction in a regression model. If you include a second-order interaction, you must also include the effects of the variables involved in the interaction as well as all first-order interactions among these variables in the regression model. All in all, these models become very complicated to interpret, so we do not pay further attention to them.

## Reporting Regression Results with Moderation {#reportmoderation}

```{r report-moderation-table, echo=FALSE}
# Generate data with categorical*continuous and continuous*continuous moderation.
# Number of observations.
n <- 150
# Create predictors
set.seed(4932)
exposure <- runif(n)*10
set.seed(823)
former <- rbinom(n, 1, 0.40)
set.seed(401)
smoker <- rbinom(n, 1, 0.20)
smoker[former == 1] <- 0
set.seed(4321)
contact <- 0.12*(10 - exposure) + rnorm(n, mean = 4.5, sd = 2)
# Mean-centered predictors.
exposure_mc <- exposure - mean(exposure)
contact_mc <- contact - mean(contact)
# Create outcome for mean-centered numeric predictor and moderator.
set.seed(390)
attitude <- -0.26*exposure_mc + 0.25*contact_mc + 0.08*exposure_mc*contact_mc - 1.6*former + 0.06*smoker - 0.12*former*exposure_mc + 0.05*smoker*exposure_mc + rnorm(n, mean = -1, sd = 1)
# Regression.
regmodel_1 <- lm(attitude ~ exposure_mc*contact_mc + exposure_mc*former + exposure_mc*smoker)
# Collect model test results.
summ <- summary(regmodel_1)
resultsF <- cbind(c("1", "", ""),
                  c("Regression", "Residual", "Total"),
                  c(format(round(var(attitude)*(n-1) - sum(summ$residuals^2), digits = 3), nsmall = 3), 
                    format(round(sum(summ$residuals^2), digits = 3), nsmall = 3),
                    format(round(var(attitude)*(n-1), digits = 3), nsmall = 3)),
                  c(round(summ$fstatistic[2]), round(summ$fstatistic[3]), n - 1),
                  c(format(round((var(attitude)*(n-1) - sum(summ$residuals^2))/summ$fstatistic[2],digits=3), nsmall = 3), format(round((var(attitude)*(n-1))/summ$fstatistic[3],digits=3), nsmall = 3), ""),
                  c(format(round(summ$fstatistic[1], digits = 3), nsmall = 3), "", ""),
                  c(format(round(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE), digits = 3), nsmall = 3), "", "")
                )
# Table with coefficient results in SPSS style.
results <- coef(summary(regmodel_1))
# Confidence intervals
ci <- confint.lm(regmodel_1)
# Reorder for APA6 table.
table <- cbind(paste0(format(round(results[,1], digits=2), nsmall=2),
                      ifelse(results[,4] < 0.001, "***", 
                        ifelse(results[,4] < 0.01, "**",
                          ifelse(results[,4] < 0.05, "*", "")))),
                 paste0("[", format(round(ci[,1], digits=2), nsmall = 2),
                        ", ", 
                        format(round(ci[,2], digits=2), nsmall = 2), "]"))
# Add R2 and F
table <- rbind(table, c(format(round(summ$r.squared, digits=2), nsmall = 2), ""),
               c(paste0(format(round(summ$fstatistic[1], digits=2), nsmall=2),
                      ifelse(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE) < 0.001, "***", 
                        ifelse(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE) < 0.01, "**",
                          ifelse(pf(summ$fstatistic[1], summ$fstatistic[2], summ$fstatistic[3], lower.tail = FALSE) < 0.05, "*", "")))), ""))
# Adjust parameter names
rownames(table) <- c("Constant", "Exposure", "Contact", "Former smoker", "Smoker", "Exposure * Contact", "Exposure * Former smoker", "Exposure * Smoker", "R^2^", "F")
attributes(table)$dimnames[[1]][1] <- "Constant"
attributes(results)$dimnames[[1]][6] <- "exposure*contact"
attributes(results)$dimnames[[1]][7] <- "exposure*former smoker"
attributes(results)$dimnames[[1]][8] <- "exposure*smoker"
# Set column names.
colnames(table) <- c("B", "95% CI")
# Table.
options(knitr.kable.NA = '')
knitr::kable(table, align = c("l", "c"), caption = "Predicting attitude towards smoking. Results in APA6 style. Exposure and contact are mean-centered.")# Helper function for displaying results within the text.
source("report_n.R")
```

\small _Note_. _N_ = `r n`. CI = confidence interval.

\* _p_ < .05. \** _p_ < .01. \*** _p_ < .001.
\small

If we report a regression model, we first present the significance test and predictive power of the entire regression model. We may report that the regression model is statistically significant, F (`r resultsF[[1,4]]`, `r resultsF[[2,4]]`) = `r report_n(as.numeric(resultsF[[1,6]]),2)`, p `r ifelse(resultsF[[1,7]] == "0.000", "< 0.001", paste0("=", resultsF[[1,7]]))`, so the regression model very likely helps to predict attitude towards smoking in the population. Retrieve the test information from SPSS; the APA6-style table (Table \@ref(tab:report-moderation-table)) only reports the F value and its significance level.

How well does the regression model predict attitude towards smoking? The effect size of a regression model or its predictive power is summarized by $R^2$ (_R Square_), which is the proportion of the variation in the outcome variable scores (attitude towards smoking) that can be predicted with the regression model. In this example, $R^2$ is `r report_n(summ$r.squared, 2)`, so the regression model predicts `r report_n(summ$r.squared * 100, 0)`% of the variance in attitude towards smoking among the respondents. In communication research, $R^2$ is usually smaller. 

$R^2$ tells us how well the regression model predicts the outcome variable in the sample. Every predictor that we add to the regression model helps to predict results in the sample even if the predictor does not help to predict the outcome in the population. For a better idea of the predictive power of the regression model in the population, we may use _Adjusted R Square_. Adjusted R Square is usually slightly lower than R Square. In the example, Adjusted R Square is `r report_n(summ$adj.r.squared, 2)` (not reported in Table \@ref(tab:report-moderation-table))

As a next step, we discuss the size, statistical significance, and confidence intervals of the regression coefficients. If a predictor is involved in one or more interaction effects, we must be very clear about the reference value and reference group to which the effect applies.

Exposure, in our example, has a negative predictive effect on attitude towards smoking for non-smokers with average contacts with smokers, t = `r report_n(results[2,3])`, `r ifelse(results[2,4] < .0005, "p < .001", paste0("p = ", report_n(results[2,4], digits=3)))`, 95%CI[`r report_n(ci[2,1])`, `r report_n(ci[2,2])`]. Note that SPSS does not report the degrees of freedom for the t test on  regression coefficient, so we cannot report them.

Instead of presenting the numerical results in the text, we may summarize them in an APA6 style table, such as Table \@ref(tab:report-moderation-table). Note that t and p values are not reported in this table, the focus is on the confidence intervals. The significance level is indicated by stars.

A sizable and statistically significant interaction effect signals that an effect is moderated. In the example reported in Table \@ref(tab:report-moderation-table), the effect of exposure on attitude seems to be moderated by contact with smokers (_b_ = `r report_n(results[6,1])`, `r ifelse(results[6,4] < .0005, "p < .001", paste0("p = ", report_n(results[6,4], digits=3)))`) and by smoking status (_b_ = `r report_n(results[7,1])`, `r ifelse(results[7,4] < .0005, "p < .001", paste0("p = ", report_n(results[7,4], digits=3)))`). 

The regression coefficients for interaction effects must be interpreted as effect differences. For a categorical moderator, the coefficient describes the effect size difference between the category represented by the dummy variable and the reference group. Among former smokers, the negative effect of exposure is stronger for former smokers than for the reference group non-smokers. The average difference is (_b_ = `r report_n(results[7,1])`).

For a continuous moderator, we can interpret the general pattern reflected by the interaction effect. A positive interaction effect, such as `r report_n(results[6, 1])` for the interaction between exposure and smoker contact, signals that the effect of exposure is more strongly positive or less negative at higher levels of contact with smokers. 

This interpretation in terms of effect differences remains a difficult to understand. It is recommended to select some interesting values for the moderator and report the size of the effect for each value. For a categorical moderator, each category is of interest. For a continuous moderator, the mean and one standard deviation below and above the mean are usually interesting values. The regression coefficients show whether the effect is positive, negative, or nearly zero at different values of the moderator.

Visualize the regression lines for different values of the moderator rather than presenting the numerical results. If the regression model contains covariates, mention the values that you have used for the covariates. Select one of the categories for a categorical covariate. For numeric covariates, the mean is a good choice. If you are working with mean-centered predictors, be sure to use the mean-centered predictor for the horizontal axis (as in Figure \@ref(fig:report-moderator-visual)), not the original predictor.

```{r report-moderator-visual, fig.cap="The effect of exposure on attitude towards smoking. Left: Effects for groups with different smoking status (at average contact with smokers). Right: Effects at different levels of contact with smokers (effects for non-smokers).", out.width='50%', fig.asp=1, fig.show='hold', echo=FALSE}
# Create grouping variable. 
status <- rep(0, n)
status[former == 1] <- 1
status[smoker == 1] <- 2
status <- factor(status, labels = c("non-smoker", "former smoker", "smoker"))
df <- data.frame(attitude, contact, exposure, former, smoker, status)
ggplot(df, aes(x = exposure_mc, y = attitude, colour = status)) +
  geom_point(size = 4) +
  geom_abline(slope = results[2,1], intercept = results[1,1], colour = "red", size = 1) + #nonsmoker
  geom_abline(slope = (results[2,1] + results[7,1]), intercept = (results[1,1] + results[4,1]), colour = "green", size = 1) + #former smoker
  geom_abline(slope = (results[2,1] + results[8,1]), intercept = (results[1,1] + results[5,1]), colour = "blue", size = 1) + #smoker
  theme_classic(base_size = 18) +
  xlab("Exposure (mean-centered)") +
  ylab("Attitude towards smoking") +
  theme(legend.position = "bottom")
# define colours.
cl <- RColorBrewer::brewer.pal(5, "Blues")
ggplot(df, aes(x = exposure_mc, y = attitude, colour = contact)) +
  geom_point(size = 4) +
  geom_abline(slope = (results[2,1] + results[6,1]*sd(contact)), intercept = (results[1,1] + results[3,1]*(sd(contact))), colour = cl[3], size = 1) +
  geom_abline(slope = (results[2,1]), intercept = (results[1,1]), colour = cl[4], size = 1)  +
  geom_abline(slope = (results[2,1] - results[6,1]*sd(contact)), intercept = (results[1,1] - results[3,1]*sd(contact)), colour = cl[5], size = 1) +
  theme_classic(base_size = 18) +
  xlab("Exposure (mean-centered)") +
  ylab("Attitude towards smoking") +
  scale_color_continuous(breaks = c(mean(contact)-sd(contact), mean(contact), mean(contact)+sd(contact)), labels = c("M-SD", "M", "M+SD")) +
  theme(legend.position = "bottom", legend.key.size = unit(1.6, "cm") )

# It is possible to translate the regression equation for a mean-centered predictor back to the original scale of the predictor. The simple slope (of the conditional effects) remains the same. The intercept has to be adjusted: It is the intercept estimated for the mean-centered predictor minus the mean of the original predictor times the slope. Graphically speaking, the intercept must be moved from the mean of the original predictor, which is zero on the mean-centered predictor) to zero on the original predictor, which is minus the original mean on the mean-centered predictor. The inercept with the original predictor, then, is M steps to the left from zero on the regressio line for the mean-centered predictor.
# ggplot(df, aes(x = exposure, y = attitude, colour = status)) +
#   geom_point(size = 4) +
#   geom_abline(slope = results[2,1], intercept = (results[1,1] - results[2,1]*mean(exposure)), colour = "red", size = 1) + #nonsmoker
#   geom_abline(slope = (results[2,1] + results[7,1]), intercept = (results[1,1] + results[4,1] - (results[2,1] + results[7,1])*mean(exposure)), colour = "green", size = 1) + #former smoker
#   geom_abline(slope = (results[2,1] + results[8,1]), intercept = (results[1,1] + results[5,1] - (results[2,1] + results[8,1])*mean(exposure)), colour = "blue", size = 1) + #smoker
#   theme_classic(base_size = 18) +
#   xlab("Exposure") +
#   ylab("Attitude towards smoking") +
#   theme(legend.position = "bottom")
# ggplot(df, aes(x = exposure, y = attitude, colour = contact)) +
#   geom_point(size = 4) +
#   geom_abline(slope = (results[2,1] + results[6,1]*sd(contact)), intercept = (results[1,1] + results[3,1]*(sd(contact)) - (results[2,1] +  results[6,1]*sd(contact))*mean(exposure)), colour = cl[3], size = 1) + #M + SD
#   geom_abline(slope = (results[2,1]), intercept = (results[1,1] - results[2,1]*mean(exposure)), colour = cl[4], size = 1)  + #M
#   geom_abline(slope = (results[2,1] - results[6,1]*sd(contact)), intercept = (results[1,1] - results[3,1]*sd(contact) - (results[2,1] - results[6,1]*sd(contact))*mean(exposure)), colour = cl[5], size = 1) +
#   theme_classic(base_size = 18) + #M - SD
#   xlab("Exposure") +
#   ylab("Attitude towards smoking") +
#   scale_color_continuous(breaks = c(mean(contact)-sd(contact), mean(contact), mean(contact)+sd(contact)), labels = c("M-SD", "M", "M+SD")) +
#   theme(legend.position = "bottom", legend.key.size = unit(1.6, "cm") )

#Cleanup.
rm(ci, df, results, resultsF, table, attitude, cl, contact, exposure, former, n, regmodel_1, smoker, status, summ, report_n)
```

The left panel in Figure \@ref(fig:report-moderator-visual) clearly shows that the effect of exposure on attitude is more or less the same for non-smokers and smokers. The effect is different for former smokers, for whom the exposure effect is more strongly negative. It is more complicated to draw this conclusion from the table with regression coefficients.

Check that the predictor has good support at the selected values of the moderator. In the left-hand plot of Figure \@ref(fig:report-moderator-visual), the groups (colours) vary nicely over the entire range of the predictor _exposure_, so that is okay. It is more difficult to see good variation in the right-hand plot. 

Do not report that common support is good. If it is bad, try to find other reference groups or values with adequate support. If they cannot be found, warn the reader that we cannot fully trust the estimated moderation because we do not have a nice range of predictor values within each level of the moderator.

Finally, inspect the residual plots but do not include them in the report. Warn the reader if the assumptions of the linear regression model are not met. Do not mention the assumptions if they are met.

## A Continuous Moderator in SPSS {#RegressionContModSPSS}

### Instructions

```{r SPSSregcenter, echo=FALSE, out.width="640px", fig.cap="(ref:regcenterSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/2947blS-Dnc", height = "360px")
# Goal: Mean-centering numeric variables (both predictor and moderator) in SPSS.
# Example: smokers.sav, the effect of campaign exposure on attitude towards smoking moderated by contacts that people have with smokers.
# SPSS menu: 
#  1. determine average score on a variable: Analyze > Descriptive Statistics > Frequencies ; select Statistics > Mean (and Minimum, Maximum to check range) and unselect Display frequency tables
#  2. create a new variable with the average subtracted: Transform > Compute, select variable, give new name (indicating centering), and subtract value of average from Frequencies output
#  3. Calculate the interaction predictor from the two mean-centered variables.
# Inspect output: descriptives (and unstandardized coefficients) in regression analysis ;  never mind rounding errors or differences due to listwise deletion of missing values)
```

----

```{r SPSSreglines2, echo=FALSE, out.width="640px", fig.cap="(ref:reglines2SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/xp8tFvdb2EM", height = "360px")
# Goal: Graph regression lines for different moderator values in a scatterplot.
# Example: smokers.sav, the effect of campaign exposure on attitude towards smoking moderated by contacts that people have with smokers (both mean-centered), smoking status as covariate.
# Techniques: 
#   (1) using reference lines for M, M - SD, and M + SD ; with mean-centered moderator, add SD to obtain reference line for M - SD (- (M - SD) = - M + SD)
#   (2) {only mention} centering on (M - SD) and (M + SD) (in addition to centering on M)
# SPSS menu: {after having applied} regression analysis with descriptives, reconstruct regression equation, calculate mean of moderator minus one SD and plug into the equation ; use mean or reference value for covariat(s) ; Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter, in the Chart Editor, add reference line (Options > Reference Line from Equation)
# Interpret results. 
```

----

```{r SPSSregSupport2, echo=FALSE, out.width="640px", fig.cap="(ref:regSupport2SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/_OwIfQhOAxU", height = "360px")
# Goal: Checking common support with a continuous moderator; group moderator in 3 groups (terciles) and create (panelled) histograms for the predictor scores in each moderator group
# Example: smokers.sav, the effect of campaign exposure on attitude towards smoking moderated by contacts that people have with smokers.
# SPSS menu: Transform > Visual Binning
# Interpret output: 

# : (for enthusiasts?) don't interpret the standardized regression coefficients (Beta) for interaction variables in SPSS because they are calculated in the wrong way ; the predictor and moderator variables are multiplied to obtain the interaction variable and aferwards they are standardized ; instead, the predictor and moderator variables should be standardized before they are multiplied ; if you want to interpret the standardized regression coefficients, you have to standardize _all_ numeric variables yourself (Analyze > Descriptive Statistics > Descriptives with option 'Save standardized values as variables' checked) before you calculate the interaction variable and include them in the regression analysis ; in this situation, the output of the regression analysis lists the standardized regression weights in the column 'Unstandardized Coefficients'. 
```

### Exercises

1. With the data in <a href="http://82.196.4.233:3838/data/smokers.sav" target="_blank">smokers.sav</a>, check if the effect of campaign exposure on attitude towards smoking depends on the contacts that people have with smokers. For now, do not mean-center the variables. Control for the respondent's smoking status (_status2_). Interpret the regression coefficients and check the assumptions of the regression model.

```{r eval=FALSE}
SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=exposure status2 contact attitude
  /ORDER=ANALYSIS.
* Compute interaction variable.
COMPUTE expo_contact=exposure * contact.
VARIABLE LABELS  expo_contact 'Interaction exposure * contact'.
EXECUTE.
* Multiple regression.
* Statistic Descriptives is added to get the means that we need
* to plug into the regression equation in the moderaiton plot.
REGRESSION
  /DESCRIPTIVES MEAN STDDEV CORR SIG N
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT attitude
  /METHOD=ENTER exposure contact expo_contact status2
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).

Check data:

There are no impossible values on the variables.

Check assumptions:

* The residuals are skewed, so the assumption of a normal distribution can be
violated.
* The residuals seem to average to zero at all levels of the predicted
outcome. This supports a linear model. Note that it is not a problem that the
residuals tend to be further from zero if they are below zero.
* Prediction errors seem to be more or less of equal size at different levels
of the outcome variable, so the assumpion of homoscedasticity seems to be met.

Interpret the results:

* The regression model predicts 21 percent of the variation in the outcome
variable, F (4, 80) = 5.44, p = .001.
* None of the regression coefficients, however, is statistically significant.
We are not confident that the directions of the estimated effects are the true
directions; they may be the opposite in the population.
* But we should realize
* that the significance tests of the moderated partial effects apply to the
effect at one particular level of the moderator. At this level of the
moderator, the estimated coefficient is not sufficiently different from zero.
At other levels of the moderator, however, the coefficient can be sufficiently
different from zero for the test to be statistically significance.

* The effects of exposure and contact with smokers must be interpreted with
care due to their interaction effect.
* The estimated effect of exposure applies to adults who score zero on the
variable contact with smokers. In this context, exposure makes the predicted
attitude towards smoking more negative, b = -0.27, t = -1.54, p = .128.
* Contact with smokers makes the attitude more positive for adults who have no
exposure to the anti-smoking campaign, b = 0.07, t = 0.41, p = .682.
* The interaction effect is positive, b = .02, t = 0.53, p = .595, so exposure
seems to be less effective in making the attitude towards smoking more
negative if the adult has more contacts with smokers.
```

2. Visualize the moderating effect of contact with smokers on the exposure effect in a scatterplot with three regression lines. Explain the information conveyed by the plot to your reader.

```{r eval=FALSE}
SPSS syntax:

* Create scatterplot.
GRAPH
  /SCATTERPLOT(BIVAR)=exposure WITH attitude
  /MISSING=LISTWISE.


Manually add three regression lines:

* Write out the regression equations for different values of the moderator.
* Plug in the estimated values of the regression coefficients, the means of
covariates, and three values for the moderator using its M and SD.

The Equation for non-smokers (status = 0):

attitude = .648*constant + -.265*exposure + 
  .072*contact + .018*exposure*contact + 0.533*status

attitude = .648*1 + -.265*exposure +
  .072*contact + .018*exposure*contact + 0.533*0

attitude = .648 + (-.265 + .018*contact)*exposure +
  .072*contact

Contact: M - SD

attitude = .648 + (-.265 + .018*(5.091 - 1.974))*exposure +
  .072*(5.091 - 1.974)

attitude = .648 + (-.265 + .018*3.117)*exposure +
  .072*3.117

attitude = .648 + (-.265 + .056)*exposure + .224

attitude = .872 + -.209*exposure

Contact: M

attitude = .648 + (-.265 + .018*5.091)*exposure +
  .072*5.091

attitude = .648 + (-.265 + .092)*exposure + .367

attitude = 1.015 + -.173*exposure

Contact: M + SD

attitude = .648 + (-.265 + .018*(5.091 + 1.974))*exposure +
  .072*(5.091 + 1.974)

attitude = .648 + (-.265 + .018*7.065)*exposure +
  .072*7.065

attitude = .648 + (-.265 + .127)*exposure + .509

attitude = 1.157 + -.138*exposure

Interpret the results:
  
* The negative predictive effect of exposure on attitude towards smoking is
slightly stronger (more negative) for adults with fewer contacts with smokers
line.
```

3. Mean-center the predictor and moderator and repeat the regression analysis of Exercise 1. Explain the differences in the results.

```{r eval=FALSE}
SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=exposure status2 contact attitude
  /ORDER=ANALYSIS.
* Mean-center predictor and moderator.
* Ask for means of predictor and exposure.
FREQUENCIES VARIABLES=exposure contact
  /FORMAT=NOTABLE
  /STATISTICS=MEAN
  /ORDER=ANALYSIS.
* Subtract mean from variable.
COMPUTE exposure_c=exposure - 4.866.
VARIABLE LABELS  exposure_c 'Exposure (mean-centered)'.
COMPUTE contact_c=contact - 5.091.
VARIABLE LABELS  contact_c 'Contact (mean-centered)'.
EXECUTE.
* Compute new interaction variable.
COMPUTE expo_contact_c=exposure_c * contact_c.
VARIABLE LABELS  expo_contact_c 'Interaction exposure * contact  (mean-centered)'.
EXECUTE.
* Multiple regression.
* Statistic Descriptives is added to get the means that we need
* to plug into the regression equation in the moderaiton plot.
REGRESSION
  /DESCRIPTIVES MEAN STDDEV CORR SIG N
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT attitude
  /METHOD=ENTER exposure_c contact_c expo_contact_c status2
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).

Check data: See Exercise 1.

Check assumptions: See Exercise 1.

Interpret the results:

* The size and significance of the interaction effect has not changed at all.
Mean-centering only changes the reference values for the effects of the
predictor and the moderator.
* The coefficient for exposure now expresses the predictive effect of exposure
for adults with average contact with smokers. They have more contact with
smokers than the reference group in Exercise 1, who had no contact with
smokers. The interaction effect tells us that the effect of exposure becomes
less negative for higher levels of contact. This explains that we have a lower
value for the exposure coefficient now. It still is negative, so more exposure
to the anti-smoking campaign predicts a more negative attitude towards
smoking.
* In contrast, the positive effect of contact with smokers on attitude is
stronger now (b = 0.16) than in Exercise 1 (b = 0.07). The interaction effect
tells us that contact has a more positive effect on smoking attitude for
higher levels of campaign exposure. As a result, the effect of contact at
average exposure is stronger than at zero exposure.

* Why do we have a statistical significant result for the effect of exposure
now but not in Exercise 1?
* The size of the unstandardized effect is lower (b = -0.17) now than in
Exercise 1 (b = -0.27). It is closer to zero so we would not expect
statistical significance. However, the standard error is much smaller now: SE
= 0.06 against SE = 0.17. We have quite some observations with about average
contact score (the reference value if we mean center) but hardly any
observations with minimum (zero) contact score. With fewer observations, we
are less certain about estimates, so we have a larger standard error and it is
more difficult to be confident that the value is not zero in the population.
```

4. Check common support of the predictor for the moderator. Divide the moderator into three groups.

```{r eval=FALSE}
SPSS syntax:

* Group the moderator.
* Visual Binning.
*contact.
RECODE  contact (MISSING=COPY)
  (LO THRU 4.25076386584132=1)
  (LO THRU 5.83711577142397=2) 
  (LO THRU HI=3) (ELSE=SYSMIS) INTO contact_3.
VARIABLE LABELS  contact_3 'Contact with smokers (Binned)'.
FORMATS  contact_3 (F5.0).
VALUE LABELS  contact_3 1 '' 2 '' 3 ''.
VARIABLE LEVEL  contact_3 (ORDINAL).
EXECUTE.
* Histograms of the predictor for each moderator group.
GRAPH
  /HISTOGRAM=exposure
  /PANEL ROWVAR=contact_3 ROWOP=CROSS.

Interpret the results:

* Coverage of the exposure predictor is poor at low contact levels. Especially
low exposure hardly occurs at low contact level. This explains the the high
standard error for the exposure effect in Exercise 1 as explained in Exercise
3.
```

5. Let us hypothesize that children's media literacy depends on sex, age, and parental supervision. Is the effect of parental supervision moderated by the child's age? 

Use <a href="http://82.196.4.233:3838/data/children.sav" target="_blank">children.sav</a> to answer this research question and apply mean centering. 

Report the results as required in this course (APA6), include a moderation plot, and discuss coverage. 

```{r eval=FALSE}
SPSS syntax:
 Check data.
FREQUENCIES VARIABLES=medliter sex age supervision
  /STATISTICS=MEAN
  /ORDER=ANALYSIS.
* Set impossible values to missing.
* Define Variable Properties.
*sex.
MISSING VALUES sex(1).
*supervision.
MISSING VALUES supervision(25.00).
EXECUTE.
* Turn sex into a 0/1 variable.
RECODE sex (2=0) (3=1) INTO girl.
VARIABLE LABELS  girl 'The child is a girl.'.
EXECUTE.
* Mean-center predictor and moderator.
* Ask for means of predictor and exposure.
FREQUENCIES VARIABLES=age supervision
  /FORMAT=NOTABLE
  /STATISTICS=MEAN
  /ORDER=ANALYSIS.
* Subtract mean from variable.
COMPUTE age_c=age - 8.609.
VARIABLE LABELS  age_c 'Age (mean-centered)'.
COMPUTE supervision_c=supervision - 5.358.
VARIABLE LABELS  supervision_c 'Supervision (mean-centered)'.
EXECUTE.
* Check mean centering.
FREQUENCIES VARIABLES=age_c supervision_c
  /FORMAT=NOTABLE
  /STATISTICS=MEAN
  /ORDER=ANALYSIS.
* Compute interaction variable.
COMPUTE age_supervision_c=age_c * supervision_c.
VARIABLE LABELS  age_supervision_c 'Interaction age * supervision (mean-centered)'.
EXECUTE.
* Multiple regression.
* Statistic Descriptives is added to get the means that we need
* to plug into the regression equation in the moderation plot.
REGRESSION
  /DESCRIPTIVES MEAN STDDEV CORR SIG N
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT medliter
  /METHOD=ENTER girl age_c supervision_c age_supervision_c
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).
* Create scatterplot for moderation plot.
* Use the mean-centered variable.
GRAPH
  /SCATTERPLOT(BIVAR)=supervision_c WITH medliter
  /MISSING=LISTWISE.
* Manually add three regression lines.
  
* Write out the regression equations for different values of the moderator.
* Plug in the estimated values of the regression coefficients, the selected
category of the covariate, and three values for the moderator using its M and
SD.
* Create regression lines for the effect of media literacy at three levels of
parental supervision (M - SD, M, and M + SD) in the scatterplot of media
literacy by mean-centered parental supervision.

Estimated regression equation: 
  
medliter = 4.325 + 0.193 * girl + 0.176 * age_centered + 
  0.292 * supervision_centered + 0.025 * age_c * supervision_c

With rearranged terms and sex plugged in for boys:

medliter = 4.325 + 0.193 * 0 + 0.176 * age_centered + 
  (0.292  + 0.025 * age_c) * supervision_centered

Age at M - SD (mean-centered so M = 0, SD = 1.937) for boys:

medliter = 4.325 + 0.176*(0 - 1.937) +
  (0.292 + 0.025*(0 - 1.937))*supervision

medliter = 4.325 + -0.341 + (0.292 + -0.048)*supervision

medliter = 3.984 + 0.244*supervision
  
Age at M (M = 0) for boys:

medliter = 4.325 + .176*(0) +
  (0.292 + .025*(0))*supervision

medliter = 4.325 + 0.292*supervision

Age at M + SD (M = 0, SD = 1.937) for boys:

medliter = 4.325 + .176*(0 + 1.937) +
  (0.292 + 0.025*(0 + 1.937))*supervision

medliter = 4.325 + 0.341 + (0.292 + 0.048)*supervision

medliter = 4.666 + 0.340*supervision

Note: Age was mean-centered for all 87 cases but the regression model only
uses the 85 cases without a missing value on any of the variables. As a
result, the mean of age scores in the regression model is not exactly zero. We
can still use zero here because we merely mean to refer to an age value that
is in the center of the distribution. Both zero and nearly zero are in the
center.

Check data:

* Score '25' for parental supervision cannot be right because the scale runs to
10. Define this score as a missing value.
* The sex category '1' cannot be right either.

Check assumptions:

* The residuals are quite normally distributed, as they should.
* The residuals are centered around zero for all levels of the predicted
outcome (linearity) but the variation in residuals seems to be a bit larger at
higher predicted values (the residuals may not be homoscedastic).

Interpret the results:
  
* The regression model predicts 19 percent of the differences in media
litearcy among children, F (4, 80) = 4.58, p = .002.
* There is no remarkable difference between girls and boys, t = 0.58, p =
.566, 95%CI[-0.47; 0.86].
* Girls may be upto 0.86 more media literate on average than boys but we
cannot rule out that boys have on average more media literacy (up to 0.47).
* Age has a statistically significant positive effect on media literacy for
children at average parental supervision, t = 2.02, p = .047, 95%CI[0.003;
0.35].
* Parental supervision has a positive effect on media literacy, t = 3.36, p =
.001, 95%CI[0.12; 0.47.
* There is no statistically significant interaction effect between age and
parental supervision on media literacy, t = .50, p =.615, 95%CI[-0.07; 0.12].
If there is an interaction effect in the population, it can be positive or
negative.
* The regression lines in the moderation plot have quite similar slopes, which
illustrates the absence of a substantial interaction effect.
```

6. Is the effect of parental supervision moderated by sex? Use the data of Exercise 5 to answer this question. You may omit the age predictor from the model. Again, illustrate your answer with a moderation plot.

```{r eval=FALSE}
SPSS syntax:

* Check data.
FREQUENCIES VARIABLES=medliter sex supervision
  /STATISTICS=MEAN
  /ORDER=ANALYSIS.
* Set impossible values to missing.
* Define Variable Properties.
*sex.
MISSING VALUES sex(1).
*supervision.
MISSING VALUES supervision(25.00).
EXECUTE.
* Turn sex into a 0/1 variable.
RECODE sex (2=0) (3=1) INTO girl.
VARIABLE LABELS  girl 'The child is a girl.'.
EXECUTE.
* Mean-center the predictor.
* Ask for means of parental supervision.
FREQUENCIES VARIABLES=supervision
  /FORMAT=NOTABLE
  /STATISTICS=MEAN
  /ORDER=ANALYSIS.
* Subtract mean from variable.
COMPUTE supervision_c=supervision - 5.358.
VARIABLE LABELS  supervision_c 'Supervision (mean-centered)'.
EXECUTE.
* Compute interaction variable.
COMPUTE girl_supervision_c=girl * supervision_c.
VARIABLE LABELS  girl_supervision_c 'Interaction girl * supervision (mean-centered)'.
EXECUTE.
* Multiple regression.
* Statistic Descriptives is added to get the means that we need
* to plug into the regression equation in the moderation plot.
REGRESSION
  /DESCRIPTIVES MEAN STDDEV CORR SIG N
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS CI(95) R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT medliter
  /METHOD=ENTER girl supervision_c girl_supervision_c
  /SCATTERPLOT=(*ZRESID ,*ZPRED)
  /RESIDUALS HISTOGRAM(ZRESID).
* Scatterplot with dots coloured by sex.
* Use the mean-centered predictor.
GRAPH
  /SCATTERPLOT(BIVAR)=supervision_c WITH medliter BY girl
  /MISSING=LISTWISE.
* Note: This model does not contain a covariate, so SPSS can draw the lines.
* Command: Graphs > Regression Variable Plots; Color by: sex..
* With options: Scatterplot Fit Lines: Linear, Grouping: Fit Line for each categorical colour group.
* Use the mean-centered or not mean-centered predictor.
STATS REGRESS PLOT YVARS=medliter XVARS=supervision_c COLOR=sex 
/OPTIONS CATEGORICAL=BARS GROUP=1 INDENT=15 YSCALE=75 
/FITLINES LINEAR APPLYTO=GROUP.

* If you add the regression lines for boys and girls manually, use the
mean-centered supervision variable and the following equations:

Equation for boys (girl = 0):

medliter = 4.374 + 0.125*girl +
  0.389*supervision + -0.134*girl*supervision

medliter = 4.374 + 0.125*0 +
  (0.389 + -0.134*girl)*supervision

medliter = 4.374 + (0.389 + -0.134*0)*supervision

medliter = 4.374 + 0.389*supervision

Equation for girls (girl = 1):

medliter = 4.374 + 0.125*girl +
  0.389*supervision + -0.134*girl*supervision

medliter = 4.374 + 0.125*1 +
  (0.389 + -0.134*girl)*supervision

medliter = 4.374 + (0.389 + -0.134*1)*supervision

medliter = 4.374 + 0.255*supervision


Check data: See Exercise 5.

Check assumptions:

As with the regression model in Exercise 5.
* The residuals are quite normally distributed and centered around zero for
all levels of the predicted outcome (linearity).
* The variation in residuals seems to be a bit larger at higher predicted
values (the residuals may not be homoscedastic).

Interpret the results:

* The regression model predicts 15 percent of the variation in outcome scores,
F (3, 81) = 4.76, p = .004.
* There is no remarkable difference between girls and boys, t = 0.37, p =
.711, 95%CI[-0.55; 0.80] for children at average supervision level. Girls may
have up to 0.80 more media literacy on average than boys but we cannot rule
out that boys have on average more media literacy (up to 0.55).
* Parental supervision has a statistically significant positive effect on
media literacy for boys, b = 0.39, t = 2.99, p = .004, 95%CI[0.13; 0.65] that
is weaker than for girls, b = 0.26 but the difference between boys and girls
is not statstically significant, t = -0.76, p = .448, 95%CI[-0.48; 0.22].
```

## Take-Home Points  

* In a regression model, moderation means that there are different slopes (of the predictor) for different groups or contexts (moderator).

* Interaction variables represent moderation in a regression model. 

* An interaction variable is the product of the predictor and moderator. If the moderator is categorical, it is represented by one or more dummy variables. There is an interaction variable for each of the moderator's dummy variables. 

* Statistical inference for an interaction variable is exactly the same as for "ordinary" regression predictors.

* The effect of the predictor in a model with an interaction variable does _not_ represent a main or average effect. It is a conditional effect: The effect for cases that score zero on the moderator. The same applies to the effect of the moderator, which is the conditional effect for cases scoring zero on the predictor.

* To interpret moderation, describe the effects (slopes, unstandardized regression coefficients) and preferably visualize the regression lines for different groups or contexts. For a numerical variable, select some interesting levels of the moderator, such as the mean and one standard deviation below or above the mean.

* Interpret regression lines for groups or moderator levels only if the predictor scores are nicely distributed for this group or level (common support). 

* Don't use the standardized regression coefficients (Beta) for interaction variables in SPSS.