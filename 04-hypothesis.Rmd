# Testing a Null Hypothesis: Am I Right or Am I Wrong? {#hypothesis}
> Key concepts: research hypothesis, statistical null and alternative hypothesis, nil hypothesis, test statistic, p value, significance level (Type I error rate), Type I error, inflated Type I error, capitalization on chance, one-sided and two-sided tests and tests to which this distinction does not apply, rejection region.  

### Summary {-}

> Is my sample probable if the null hypothesis is true?

In the preceding chapter, we have learned that a confidence interval contains the population values that are plausible, given the sample that we have drawn. In the current chapter, we narrow this down to the question whether the expectation of the researcher about the population is plausible. 

The expectation is usually called a (research) hypothesis and it must be translated into statistical hypotheses about a population value (parameter): a null hypothesis and an alternative hypothesis. 

We test the null hypothesis in the following way. We construct a sampling distribution in one of the ways we have learned in Chapter \@ref(probmodels) using the value specified in the null hypothesis as the imaginary population value. In other words, we act as if the null hypothesis is true. 

Then, we calculate the probability of drawing a sample such as the one we have drawn or a sample that differs even more from a population for which the null hypothesis is true. If this p value is very low, say, below 5%, we reject the null hypothesis because our sample would be too unlikely if the null hypothesis is true. In this case, the test is statistically significant. The probability threshold that we use is called the significance level of the test.

## A Binary Decision

The overall goal of statistical inference is to increase our knowledge about a population, when we only have a random sample from that population. In Chapter \@ref(param-estim), we estimated population values that are plausible considering the sample that we drew. For instance, we looked for all plausible average weights of candies in the population using information about the weight of candies in our sample bag. This is what we do when we estimate a population value.  

Estimation is one of two types of statistical inference, the other being null hypothesis testing. When we estimate a population value, we do not use our previous knowledge about the world of candies or whatever other subject we are investigating. We can be completely ignorant about the phenomenon that we are investigating. This approach is not entirely in line with the conceptualization of scientific progress as an _empirical cycle_, in which scientists develop theories about the empirical world, test these theories against data collected from this world, and improve their theories if they are contradicted by the data.  

Hypothesis testing, however, is more in line with this conceptualization of scientific progress. It requires the researcher to formulate an expectation about the population, usually called a _hypothesis_. If the hypothesis is based on theory and previous research, the scientist uses previous knowledge. As a next step, the researcher tests the hypothesis against data collected for this purpose. If the data contradict the hypothesis, the hypothesis is rejected and the researcher has to improve the theory. If the data does not contradict the hypothesis, it is not rejected and, for the time being, the researcher need not change their theory.  

Hypothesis testing, then, amounts to choosing one of two options: reject or not reject the hypothesis. This is a binary decision between believing that the population is as it is described in the null hypothesis, or believing that it is not. This is quite a different approach from estimating a confidence interval as a range of plausible population values. Nevertheless, hypothesis testing and confidence intervals are tightly related as we will see later on in this chapter (Section \@ref(null-ci0)).

## Formulating Statistical Hypotheses {#formulating-hypotheses}

A _research hypothesis_ is a statement about the empirical world that can be tested against data. Communication scientists, for instance, may hypothesize that:  

* a television station reaches half of all households in a country,

* media literacy is below a particular standard (for instance, 5.5 on a 10-point scale) among children,  

* opinions about immigrants are not equally polarized among young and old voters,  

* the celebrity endorsing a fundraising campaign makes a difference to people's willingness to donate,  

* more exposure to brand advertisements increases brand awareness,

* and so on.  

As these examples illustrate, research hypotheses seldom refer to statistics such as means, proportions, variances, or correlations. Still, we need such statistics to test a hypothesis. The researcher must translate the hypothesis into a new hypothesis specifying a statistic in the population, for example, the population mean. The new hypothesis is called a _statistical hypothesis_.  

Translating the research hypothesis into a statistical hypothesis is perhaps the most creative part of statistical analysis, which is just a fancy way of saying that it is difficult to give general guidelines stating which statistic fits which research hypothesis. All we can do is give some hints.  

Research questions usually address shares, score levels, or score variation. If a research question talks about how frequent some characteristic occurs (How many candies are yellow?) or which part has a particular characteristic (Which percentage of all candies are yellow?), we are dealing with one or two categorical variables. Here, we need a binomial, chi-squared, or exact test (see Figure \@ref(fig:choice-diagram)).

If a research question asks how high a group scores or whether one group scores higher then another group, we are dealing with score levels. The variable of central interest usually is numerical (interval or ratio measurement level) and we are concerned with mean or median scores. There is a range of tests that we can apply, depending on the number of groups that we want to compare (one, two, three or more): t tests or analysis of variance. 

Instead of comparing mean scores of groups, a research question about score levels can address associations between numerical variables, for example, Are heavier candies more sticky? Here, the score level on one variable (candy weight) is linked to the score level on another variable (candy stickiness). This is where we use correlations or regression analysis.

Finally, a research question may address the size of the variation of numeric scores, for example, Does the weight of yellow candies vary more strongly than the weight of red candies? Variance is the statistic that we use to measure variation in numeric scores.

```{r choice-diagram, echo=FALSE, fig.cap="Flow chart for selecting the appropriate statistical test.", screenshot.alt = "figures/choice-diagram.png"}
source("flowchart.R")
p
rm(p)
```

1. For each of the research hypothesis examples (above), find the appropriate statistical test using Figure \@ref(fig:choice-diagram). Hint: Hover your mouse pointer over a node, click on a dot, drag with your (left) mouse button, and zoom with your mouse wheel.

```{r eval=FALSE}
* A television station reaches half of all households in a country: Half of  
all households refers to a category share. The variable is reaching a  
household, which is a dichotomy (yes/no), so we use a binomial test.  
 
* Media literacy is below a particular standard (for instance, 5.5 on a  
10-point scale) among children: "Standard" refers to an overall score level.  
We are testing a mean (not a median), so we can use a one-sample t test.  

* Opinions about immigrants are not equally polarized among young and old  
voters: "Polarization" refers to a stronger score variation in opinions. This  
brings us to Levene's F test.  

* The celebrity endorsing a fundraising campaign makes a difference to  
people's willingness to donate: "Difference" probably means that people seeing  
one endorser are on average more willing to donate than people seeing another  
(or no) celebrity endorser. This refers to score level differences between  
groups, which are tested with an indpendent-samples t test or analysis of  
variance, depending on the number of groups that we compare.  

* More exposure to brand advertisements increases brand awareness: "More ...  
increases ..." refers to concurrent score levels. People with more exposure  
are people with more brand awareness. If the association is more or less  
linear, we can use a correlation or regression analysis. If the association is  
clearly bent, we had better use Spearman's rank correlation.  
```

### Proportions: shares  
This book covers tests for four types of statistics: proportions, means, variance/standard deviations, and associations. A proportion is the statistic best suited to test research hypotheses addressing the share of a category or entity in the population. The hypothesis that a television station reaches half of all households in a country provides an example. All households in the country constitute the population. The share of the television station is the proportion or percentage of all households watching this television station.  

If we want to use a statistic, we need to know the variable and cases (units of analysis) for which the statistic must be calculated. In this example, a household does or does not watch the television station, so our variable is a dichotomy with the two categories ("No, does not watch this station", "Yes, watches this station") usually coded as 0 versus 1 or 1 versus 2.  

Each household provides an observation, namely either the score 0 or the score 1 on this variable or no score if there are missing values. To test the research hypothesis that a television station reaches half of all households in a country, we have to formulate a statistical hypothesis about the proportion---of households viewing this television station---in the population---all households in this country. For example, the researcher's statistical hypothesis could be that the proportion in the population is 0.5.  

We can also be interested in more than two categories, for instance, does the television station reach the same share of all households in the north, east, south, and west of the country? This translates into a statistical hypothesis containing three or more proportions in the population. If 30% of households in the population are situated in the west, 25 % in the south and east, and 20% in the north, we would expect these proportions in the sample if all regions are equally represented. Our statistical hypothesis is actually a relative frequency distribution, such as, for instance, in Table \@ref(tab:hypo-freq).

```{r hypo-freq, echo=FALSE}
knitr::kable(data.frame(Region = c("North", "East", "South", "West"), HP = c(0.20, 0.25, 0.25, 0.30)), digits = 2, caption = "Statistical hypothesis about four proportions as a frequency table.", col.names = c("Region", "Hypothesized Proportion"))
```

A test for this type of statistical hypothesis is called a one-sample chi-squared test. It is up to the researcher to specify the hypothesized proportions for all categories. This is not a simple task: What reasons do you have to expect particular values, say a region's share of thirty per cent of all households instead of twenty-five per cent?

The test is mainly used if the researcher has information on the proportions of the categories in the population. If we draw a sample from all citizens of a country, we usually know the frequency distribution of sex, age, educational level, and so on of all citizens from the national bureau of statistics. With the bureau's information, we can test if the respondents in our sample have the same distribution with respect to sex, age, or educational level as the population; just use the population proportions in the hypothesis. If they do, the sample is _representative_ (see Section \@ref(representative)) of the population with respect to sex, age, or educational level. This is an important check on the representativeness of our sample.

### Testing proportions in SPSS {#SPSSprops}

#### Instructions

```{r SPSSbinomial, echo=FALSE, out.width="640px", fig.cap="(ref:binomialSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/cA0idFxW-J4", height = "360px")

# A binomial test on a single proportion can be executed in SPSS with the command _Analyze > Nonparametric Tests > Binomial_. In the dialog , you have to enter a variable. 
# 
# If this variable is a dichotomy (it has only two values), you can leave the _Define Dichotomy_ option at "Get from data". SPSS will use the first category score that it encounters in the data set the test category. This is tricky. It will test the sample proportion of this value against the test proportion that you specify elsewhere in this dialog.
# 
# If the variable has more than two categories or you want to be sure about the category that you use for the test, use the "Cut point" option under _Define Dichotomy_ to divide all scores into two groups. The lowest score up to and including the cut point are used as the test category.
# 
# The statistics under Options are not interesting if you just want to test a proportion.
#
# Figure shows the output. The sample proportion does not differ significantly from 0.5. In this example, we would report: "We cannot reject the hypothesis that the TV station reaches half of all households, _p_ = .784."
```

----

```{r SPSSchisq1, echo=FALSE, out.width="640px", fig.cap="(ref:chisq1SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/S8I5o_3p6ro", height = "360px")
# If we want to test a frequency distribution against a known or hypothesized population distribution, we must use a one-sample chi-squared test. This test is available in SPSS with the command _ANALYZE > NONPARAMETRIC TESTS > LEGACY DIALOGS > CHI SQUARE_. Select the categorical variable for which you want to test the distribution under _Test variable List_.
# 
# Select the option _All categories equal_ under _Expected Values_ if you hypothesize that all categories have the same proportions in the population. In the example, we hypothesize that households are equally distributed over the four regions. This is a plausible hypothesis if the four regions are known to contain a quarter of all households in the country or if the sample was stratified by region, that is, every region was meant to deliver the same number of households to the sample.
# 
# If the hypothesized distribution is not equal over all categories, specify the expected proportions, percentages, or sample frequencies under _Values_. You must specify an expected value for each category in the exact order in which the categories are coded. Be careful not to make mistakes. 
# 
# Although the frequencies of the four regions are not exactly the same in the sample, the hypothesis of equal population frequencies cannot be rejected, Chi-square (3) = 3.27, _p_ = .352.

```

#### Exercises

1. Use the data set [households.sav](http://82.196.4.233:3838/data/households.sav) to test the hypothesis that the TV station does not reach 40 per cent of all households in the population.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=tv_reach  
  /ORDER=ANALYSIS.  
* Binomial test.  
* Note: The test is one-sided if the test proprtion is not 0.50.  
NPAR TESTS  
  /BINOMIAL (0.40)=tv_reach  
  /MISSING ANALYSIS.  
  
Check data & assumptions:   
  
Variable tv_reach is a dichotomy as it should be for this test.  
    
Interpret the results:  
  
We have to reject the null hypothesis that the TV station does not reach at
least fourty per cent of all households, p = .039 (one-sided).
```

2. Test the hypothesis that the TV station reaches 55 per cent of all households in the population.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=tv_reach  
  /ORDER=ANALYSIS.  
* Binomial test.  
* Hint: Test the proportion of households not reached because   
  this is the first category: 1 - 0.55 = 0.45.  
NPAR TESTS  
  /BINOMIAL (0.45)=tv_reach  
  /MISSING ANALYSIS.  
  
Check data & assumptions:   
  
Variable tv_reach is a dichotomy as it should be for this test.  
    
Interpret the results:  
  
We cannot reject the null hypothesis that the TV station reaches   
at least 55 per cent of all households, p = .260 (one-sided).  
```  
  
3. Does half of the households have an income of at most 40,000?   
  
```{r eval=FALSE}  
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=income  
  /ORDER=ANALYSIS.  
* Binomial test.  
* Use the cut off option in the binomial test.  
NPAR TESTS  
  /BINOMIAL (0.50)=income (40000)  
  /MISSING ANALYSIS.  
  
Check data:  
  
* There are no apparent impossible income values.  
  
Check assumptions:  
  
There are no assumptions for the binomial test.  
  
Interpret the results:  
  
The proportion of households with an income of at most 40,000   
is significantly less than fifty per cent, p = .022. It is   
39 per cent in the sample.  
```

4. According information from the National Bureau of Statistics, 20 per cent of all households have incomes up to 30,000, 50 per cent have incomes between 30,000 and 50,000, and 30 per cent has incomes over 50,000. Use a test to decide if our sample is representative with respect to income. Hint: recode income first.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=income  
  /ORDER=ANALYSIS.  
* Recoding income into groups.  
RECODE income (Lowest thru 30000=1) (30000  thru 50000=2)  
  (50000 thru Highest=3) INTO income_group.  
VARIABLE LABELS  income_group 'Grouped income'.  
EXECUTE.  
* Define Variable Properties.  
*income_group.  
VALUE LABELS income_group  
  1.00 'low'  
  2.00 'medium'  
  3.00 'high'.  
EXECUTE.  
* one-sample chi-squared test.  
NPAR TESTS  
  /CHISQUARE=income_group  
  /EXPECTED=20 50 30  
  /MISSING ANALYSIS.  
  
There are no apparent impossible income values.  
  
Check assumptions:  
  
Assumptions for the chi-squared test:

* Expected frequencies never below 1 and max 20% below 5: OK, the SPSS table
note says: "0 cells (0.0%) have expected frequencies less than 5. The minimum
expected cell frequency is 24.0."
* In this case, 0% of the cells have expected frequencies below 5, which is
less than the allowed maximum of 20%. In other words, at least 80% of the
cells have expected frequencies of 5 or more.
   
Interpret the results:  
  
The distribution of incomes over income groups in the sample does not differ
in a statistically significant way from the distribution in the population
according to the National Bureau of Statistics, chi-squared (2) = 3.40, p =
.182.
In other words, we have no reason to believe that our sample is not
representative of the population with regards to income.
```

### Mean and median: level
Research hypotheses that focus on the level of scores are usually best tested with the mean or another measure of central tendency such as the median value. For example, the hypothesis that media literacy is below a particular standard (e.g., 5.5 on a 10-point scale) among children refers to a level: the level of media literacy scores. 

The hypothesis probably does not argue that all children have a media literacy score below 5.5. Instead, it means to say that the overall level is below this standard. The center of the distribution offers a good indication of the general score level.

For a numeric (interval or ratio measurement level) variable such as the 10-point scale in the example, the mean is a good measure of the distribution's center. In this example, our statistical hypothesis would be that average media literacy score of all children in the population is (below) 5.5.  

### Testing one mean in SPSS  {#SPSS1mean}

#### Instructions

```{r SPSS1mean, echo=FALSE, out.width="640px", fig.cap="(ref:1meanSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/Bx0eiqfVfaw", height = "360px")
# To execute a one sample t test in SPSS, use the command _ANALYZE > COMPARE MEANS > ONE SAMPLE T TEST_. Select a numeric variable in the dialog and enter the hypothesized population mean under _Test Value_. 
# 
# Media literacy is measured on a ten point scale. Is average media literacy (in the population) equal to 5.5? A one sample t test tells us that average media literacy in our sample (_M_ = 4.47, _SD_ = 1.64) is statistically significantly different from 5.5, _t_ (86) = -5.87, _p_ < .001, 95%CI[-1.38, -0.68]. We are confident that the population average media literacy score is 0.68 to 1.38 below 5.5, so somewhere between 4.12 and 4.82.

```

#### Exercises

1. Use the data set [children.sav](http://82.196.4.233:3838/data/children.sav) to test the hypothesis that average parental supervision of the child's media use is 5.5 (on a scale from 1 to 10) in the population.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=supervision  
  /ORDER=ANALYSIS.  
* Set imposible value (25) to missing.  
* Define Variable Properties.  
*supervision.  
MISSING VALUES supervision(25.00).  
EXECUTE.  
* One-sample t test.  
T-TEST  
  /TESTVAL=5.5  
  /MISSING=ANALYSIS  
  /VARIABLES=supervision  
  /CRITERIA=CI(.95).  
  
Check data:  
      
There is one impossible value for parental supervision,  
namely 25. This value must be made missing.  
  
Check assumptions:  
  
Sample size (N = 86) is well over 30, so we need not worry  
about the shape of the variable distribution in the  
population.  
    
Interpret the results:  
  
We are rather confident that the true average supervision   
score in the population is around 5.5, more specifically,   
between 4.94 and 5.77, t (85) = -0.68, p = .498, 95%CI[4.94; 5.77].  
  
Note that we have to add the confidence interval limits  
to the test value (here: 5.5) to obtain the confidence  
interval for the population mean.  
```

2. Have a look at the confidence interval reported for Exercise 1. If you would test the hypothesis that average parental supervision in the population is 4.5, would the test be statistically significant? Check your answer by carrying out this test.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=supervision  
  /ORDER=ANALYSIS.  
* Set imposible value (25) to missing.  
* Define Variable Properties.  
*supervision.  
MISSING VALUES supervision(25.00).  
EXECUTE.  
* One-sample t test.  
T-TEST  
  /TESTVAL=4.5  
  /MISSING=ANALYSIS  
  /VARIABLES=supervision  
  /CRITERIA=CI(.95).  
  
Check data:  
  
There is one impossible value for parental supervision,  
namely 25. This value must be made missing.  
  
Check assumptions:  
  
Sample size (N = 86) is well over 30, so we need not worry  
about the shape of the variable distribution in the  
population.  
  
Interpret the results:  
  
The average parental supervision score in the sample   
(M = 5.39, SD = 1.95) makes us doubt that the true  
average supervision score in the population is 4.5 or  
thereabouts, t (85) = 4.10, p < .001, 95%CI[4.94; 5.77].  
  
Note that we obtain the same confidence interval as in  
Exercise 1. The confidence interval does not depend on   
the null hypothesis, whereas the significance test does.  
```

### Variance: (dis)agreement
Although rare, research hypotheses may focus on the variation in scores rather than on score level. The hypothesis about polarization provides an example. Polarization means that we have scores well above the center and well below the center rather than all scores concentrated in the middle. If voters' opinions about immigrants are strongly polarized, we have a lot of voters strongly in favour of admitting immigrants as well as many voters strongly opposed to admitting immigrants.  

For a numeric variable, the variance or standard deviation---the latter is just the square root of the former---is the appropriate statistic to test a hypothesis about polarization. The research hypothesis concerns the variation of scores in two groups, for instance, young versus old voters. The statistical hypothesis would be that the variance in opinions in the population of young voters is different from the variance in the population of old voters.  

### Testing two variances in SPSS {#Levene-2groups}

#### Instructions

```{r SPSSLevene, echo=FALSE, out.width="640px", fig.cap="(ref:LeveneSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/8k-OIXiRdCc", height = "360px")
# Levene's F test for the null hypothesis that two groups have equal population variances is part of the independent-samples t test. 

# The output in Figure \@ref(fig:Levene-output) shows that the polarization (variation) in attitudes towards immigrants is not the same among young and old voters, _F_ = 4.99, _p_ = .029. Polarization is stronger among old voters (_SD_ = 2.06) than among young voters (_SD_ = 1.26). 
# 
# In a similar way, we can test if two, three or more groups have equal population variances with Levene's F test using the option _Homogeneity of variance test_ in a one-way analysis of variance (command _Analyze >  COMPARE MEANS > ONE-WAY ANOVA_).

```

#### Exercises

1. Data set [voters.sav](http://82.196.4.233:3838/data/voters.sav) contains information about the age and attitude towards immigration among a random sample of voters. Is the attitude towards immigrants equally polarized among young (under 30) and old (30+) voters? Justify your answer with a statistical test.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=age_group immigrant  
  /ORDER=ANALYSIS.  
* Independent-samples t test with Levene s test.  
T-TEST GROUPS=age_group(1 2)  
  /MISSING=ANALYSIS  
  /VARIABLES=immigrant  
  /CRITERIA=CI(.95).  
  
Check data:  
  
There are no impossible values on the variables.  
  
Check assumptions:  
  
There are no assumptions that we have to check for  
the Levene test.  
  
Interpret the results:  
  
The attitude towards immigrants is more polarized among  
older voters (SD = 2.06) than among young voters (SD = 1.26).  
The difference in variation is statistically significant,  
F = 4.99, p = .029.  
Note that SPSS does not report the (two) degrees of freedom  
of the F test, so we cannot report them either.   
SPSS, however, does report the degrees of freedom of Levene's F test in a  
one-way analysis of variance. We could have used that approach here as well.  
```

2. Use the data of Exercise 1. Create a new variable grouping voter's age with classes 18-35, 36-65, and 66+ years. Is the attitude towards immigrants equally polarized among these three age groups in the population? Justify your answer with a statistical test.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=age immigrant  
  /ORDER=ANALYSIS.  
* Group age.  
RECODE age (Lowest thru 35=1) (36 thru 65=2)  
  (66 thru Highest=3) INTO age3.  
VARIABLE LABELS  age3 'Voter ages in three groups'.  
EXECUTE.  
* Define Variable Properties.  
*age3.  
VALUE LABELS age3  
  1.00 '18-35'  
  2.00 '36-65'  
  3.00 '66+'.  
EXECUTE.  
* ANOVA with descriptives.  
ONEWAY immigrant BY age3  
  /STATISTICS DESCRIPTIVES HOMOGENEITY   
  /MISSING ANALYSIS.  
  
Check data:  
  
There are no impossible values on the variables.  
  
Check assumptions:  
  
There are no assumptions that we have to check for  
the Levene test.  
  
Interpret the results:  
  
The attitude towards immigrants seems to be more polarized   
among middle-aged (SD = 2.17) and aged voters (SD = 2.06)   
than among young voters (SD = 1.37).   
The difference in variation, however, is not statistically   
significant, F (2, 63) = 2.77, p = .070.  
This result seems to contradict the statistically significant  
test result that we found in Exercise 1, comparing only young   
to old voters. There is no contradiction, however. We merely  
see that the classification into groups can matter to the   
significance of results.  
```

### Association: relations between characteristics  
Finally, research hypotheses may address the relation between two or more variables. Relations between variables are at stake if the research hypothesis states or implies that one (type of) characteristic is related to another (type of) characteristic. The statistical name for a relation between variables is _association_.  

Take, for example, an analysis of the effect of a celebrity endorser on the willingness to donate. Here, the endorser to whom a person is exposed (one characteristic) is related to this person's willingness to donate (another characteristic). Another example: If exposure to the campaign increases willingness to donate, a person's willingness to donate is positively related to this person's exposure to the campaign.  

```{r association, eval=FALSE, echo=FALSE, fig.cap="Different types of statistical associations."}
#DROPPED

# Extend and simplify correlation-game (https://github.com/ShinyEd/ShinyEd/tree/master/correlation_game).
# Simplify user choices to association strength (none, weak, medium, strong) and association direction (positive, negative, neither). Report value(s) of appropriate measures of association/efect size. Don't show standard deviation line and ellipse. Don't include difficulty levels. Add choice of association type (nominal, ordinal, categorical * numeric, numeric * numeric) - this choice generates a new graph.
# 2 numerical variables: name them exposure (X) and willigness to donate (Y) ; report Spearman and Pearson correlation ; also generate non-linear monotonic relations (bend) and non-monotonic relations (quadratic) to train intuition for choosing appropriate test.
# Add 1 dichotomous (endorser = Clooney, Jolie, nobody) and 1 numerical variable (willigness to donate, Y): depict as jittered scatterplot with group means as horizontal line segments) ; report Cohen's d ; vary mean difference between groups (larger difference -> larger effect) and variation within group (larger variation -> smaller effect).
# Add 2 ordinal categorical variables (exposure = X (low, medium, high) and donate intention = Y (yes, maybe, no), 3 values each): depict with geom_count() and show frequency of each cell ; report Kendall's tau-b and Somers' d (and chi-squared?) ; vary cell frequencies such that positive, negative, non-monotonic, and no associations arise.
# Add 2 nominal categorical variables (endorser X (Clooney, Jolie, nobody) and know campaign = Y (Yes, No)): depict with geom_count() ; report Cramer's V and Goodman & Kruskals tau (and chi-squared?) ; vary cell frequencies such that positive, negative, non-monotonic, and no associations arise (as with the ordinal variables).

Figure \@ref(fig:association) helps you to sharpen your intuitions about statistical associations. Select the strength and direction of the association depicted in the graph and submit your answer to see if you are right. Select one of the options under _Association type:_ to generate a new plot.

1. For which type of association is association direction completely irrelevant?

2. For which pattern of scores are ordinal or numeric association measures problematic?

3. In a categorical by numeric association, what are the effects of the difference between the averages on the effect size (Cohen's d)? And what is the effect of variation of scores?
```

### Score level differences {#level-differences}

```{r anova-between-simple, fig.cap="How do group level differences express association?", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="540px"}
# Goal: Illustrate that between groups variance represents differences between group means and the grand mean. And that it is a (smaller or larger) proportion of total variance.
# App anova-means: Generate 4 random observations from a normally distributed population with mean 6.4, sd = 1 (Clooney), 4 observations from a population N(m = 6.8, sd = 1) (Jolie), and 4 observations from N(m = 3.3, sd = 1) (no endorser). Use colour for the treatment factor (3 levels). Represent observations in a dotplot, each with a separate value on the x axis, clustered by factor level (experimental condition). Display group means as horizontal line segments (coloured by factor level). Display eta^2 for the data. Allow user to change the group means and update the plot, mean (difference) lines, and eta^2.
# Extension/replacement: Add horizontal line for grand mean, vertical red solid double-sided arrows between each observation and the grand mean (total variance), vertical black solid double-sided arcs for each observation between its group mean and the grand mean (between variance), and vertical black dotted double-sided arrows for each observation between the dot and its group mean (within variance). 
knitr::include_app("http://82.196.4.233:3838/apps/anova-between-simple/", height="530px")
```

1. Figure \@ref(fig:anova-between-simple) shows the willingness to donate for twelve respondents and the celebrity they saw endorsing the fund-raising campaign. Do you think that the willingness to donate is associated with the person endorsing the fund-raiser? Motivate your answer.

```{r eval=FALSE}
Respondents who saw a campaign without celebrity endorser are on average much
less willing to donate to the fund-raiser than respondents who saw George
Clooney or Angelina Jolie. This suggests that the endorser makes a difference
to willingness to donate, so the two are associated. More specifically, it
seems to make a difference whether the endorser is a celebrity or not.
```

2. Can you create or remove an association between willingness to donate and endorsing celebrity by changing the group averages? Change the group averages to check your expectations.

```{r eval=FALSE}
If we increase the differences between the average group scores, the
association becomes stronger. Eta-squared, for example, becomes larger. But if
we decrease the differences between the average group scores, the association
becomes weaker. and eta-squared becomes smaller.
```

Association comes in two related flavors: a difference in score level between groups or the predominance of particular combinations of scores on different variables.

The relation between the endorser's identity and willingness to donate is an example of the first flavor. All people are confronted with one of the celebrities as endorser of the fund-raising campaign. This is captured by a categorical variable: the endorsing celebrity. 

The categorical variable clusters people into groups: One group is confronted with Celebrity A, another group with Celebrity B, and so on. If the celebrity matters to the willingness to donate, the general level of donation willingness should be higher in the group exposed to one celebrity than in the group exposed to another celebrity.

Thus, we return to statistics needed to test research hypotheses about score levels, namely measures of central tendency. If willingness to donate is a numeric variable, we can use group means to test the association between endorsing celebrity (grouping variable) and willingness to donate (score variable). The statistical hypothesis would then be that group means are not equal in the population of all people. 

If you closely inspect Figure \@ref(fig:choice-diagram), you will see that we prefer to use a _t_ distribution if we compare two different groups (independent-samples t test) or two repeated observations for the same group (paired-samples t test). By contrast, if we have three or more groups, we use analysis of variance with an _F_ distribution. 

### Comparing means in SPSS {#comp-means}

#### Instructions

```{r SPSSTindep, echo=FALSE, out.width="640px", fig.cap="(ref:TindepSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/g4O0oTVm-Tk", height = "360px")
# Goal: association as level differences between groups: are females more wiling to donate to a fund raiser than males?
# Example: donors.sav, outcome is willingness to donate (post), predictor (grouping variable) is sex (0, 1).
# Technique: independent-samples t test.
# SPSS menu: Compare Means ; options: check level of confidence interval.
# Paste & Run.
# Interpret output: choose right row in test table depending on Levene's F test ; test result and significance, confidence interval.
# Check assumptions: each group more than 30 observations or normally distributed (histogram panelled by sex).
```

----

```{r SPSSTpaired, echo=FALSE, out.width="640px", fig.cap="(ref:TpairedSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/7mGKG3mLqN0", height = "360px")
# Goal: Association as score level change (due to intermediary treatment).
# Example: donors.sav, outcome is willingness to donate post and prior exposure to a fund raiser campaign.
# Technique: paired-samples t test ; repeated measurements.
# SPSS menu: Compare Means ; second selected variable will be subtracted form the first, so post-pre is better combination; options: check level of confidence interval.
# Paste & Run.
# Interpret output: test result and significance, confidence interval.
# Check assumptions: each measurement more than 30 observations or normally distributed (histogram per variable).
```

----

```{r SPSS1way, echo=FALSE, out.width="640px", fig.cap="(ref:1waySPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/1-kh9_pGI9k", height = "360px")
# Execute one-way analysis of variance in SPSS with Analyze > Compare Means > One-Way ANOVA {or use general Linear Model also for one-way? (MCRS probably uses Compare Means because it only disucsses one-way ANOVA) - between & within variance not labeled as such!).
# Goal: association as level differences between three or more groups: Does the endorser matter to the level of willingness to donate to a fund raiser?
# Example: donors.sav, outcome is willingness to donate (post), predictor (grouping variable) is endorser (0, 1).
# Technique: one-way ANOVA.
# SPSS menu: Compare Means ; post hoc: Bonferroni ; options: descriptives, homogeneity of variance test, means plot.
# Paste & Run.
# Check assumptions: F test homogeneous popualtion variances or groups of equal size ; post-hoc t tests: each group more than 30 observations or normally distributed.
# Interpret output: F test on the null hypothesis that all groups have equal population means - point out between groups sum of squares? ; post-hoc t test for pairwise comparison of (population) means ; test result and significance, confidence interval.
```

#### Exercises

1. Use [donors.sav](http://82.196.4.233:3838/data/donors.sav) to test if people's willingness to donate at the end of the campaign depends on the celebrity endorsing the campaign.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=willing_post endorser  
  /ORDER=ANALYSIS.  
* One-way analysis of variance.  
ONEWAY willing_post BY endorser  
  /STATISTICS DESCRIPTIVES HOMOGENEITY   
  /PLOT MEANS  
  /MISSING ANALYSIS  
  /POSTHOC=BONFERRONI ALPHA(0.05).  
  
Check data:  
  
There are no impossible values on the variables.  
  
Check assumptions:  
  
The three groups are more or less of equal sizes:  
The largest difference is 4 participants, which is less  
than ten per cent of the smallest group (N = 45).  
Anyway, we may assume equal population variances,  
Levene F (2, 140) = .02, p = .978.  
  
Interpret the results:  
  
Willingness to donate depends on the endorsing celebrity.   
There is a statistically significant difference between average  
willingness to donate for the three endorsers, F (2, 140) = 7.44,   
p = .001.  
People are more willing to donate if they have seen Clooney   
(M = 4.99, SD = 1.64) or Jolie (M = 4.95, SD = 1.63) endorse the   
fund raiser than people who do not see a celebrity endorser   
(M = 3.87, SD = 1.47). The differences between, on the one   
hand, no celebrity endorser and, on the other hand, Clooney  
(p = .002) or Jolie (p = .004) are statistically significant.  
  
Instead of reporting the F test result in the text, the   
ANOVA table can be included.  
```

2. Is willingness to donate at the end of the campaign higher for those who remember the campaign than for those who do not remember it?

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=willing_post remember  
  /ORDER=ANALYSIS.  
* Independent-samples t test.  
T-TEST GROUPS=remember(0 1)  
  /MISSING=ANALYSIS  
  /VARIABLES=willing_post  
  /CRITERIA=CI(.95).  
  
Check data:  
  
There are no impossible values on the variables.  
  
Check assumptions:  
  
Sample sizes (N = 66 and N = 77) are of sufficient  
size not to worry about the shape of the distribution  
of willingness in the population.  
  
Interpret the results:  
  
Willingness to donate at the end of the campaign is significantly   
higher for those who remember the campaign (M = 4.94, SD = 1.60)   
than for those who do not remember it (M = 4.24, SD = 1.65),   
t (141) = -2.57, p = .011, 95%CI[-1.24, -0.16].   
Willingness is 0.16 to 1.24 points higher on a 10-point scale for   
those who remember the campaign. Considering the range of the   
scale, this is quite a small difference.  
```

3. Did willingness to donate increase in the population between the start and the end of the campaign?

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=willing_post willing_pre  
  /ORDER=ANALYSIS.  
* Paired-samples t test.  
T-TEST PAIRS=willing_pre WITH willing_post (PAIRED)  
  /CRITERIA=CI(.9500)  
  /MISSING=ANALYSIS.  
  
Check data:  
  
There are no impossible values on the two variables.  
  
Check assumptions:  
  
Sample size is sufficiently large (N = 143).  
  
Interpret the results:  
  
There is a small but statistically significant increase in  
the willingness to donate over the duration of the experiment,  
t (142) = 5.74, p < .001, 95%CI[0.08; 0.17].  
Average willingness to donate is 0.08 to 0.17 units higher  
at the end of the experiment than at the start. This is a   
very small difference on a 10-point scale.  
```

### Combinations of scores {#score-combi}
The other flavor of association represents situations in which some combinations of scores on different variables are much more common than other combinations of scores. 

Think of the hypothesis that brand awareness is related to exposure to advertisements for that brand. If the hypothesis is true, people with high exposure and high brand awareness should occur much more often than people with high exposure and low brand awareness or low exposure and high brand awareness.

The two variables here are exposure and brand awareness. One combination of scores on the two variables is high exposure combined with high brand awareness. This combination should be more common than high exposure combined with low brand awareness.  

Measures of association are statistics that put a number to the pattern of group-score levels or combinations of scores. The exact statistic that we use depends on the measurement level of the variables. For numerical variables, measured at the interval or ratio level, we use Pearson's correlation coefficient or the regression coefficient. For ordinal variables with quite a lot of different scores, we use Spearman's rank correlation. 

For categorical variables, measured at the nominal or ordinal level, chi-squared indicates whether variables are statistically associated. The larger chi-squared, the less probable it is that the variables are not associated in the population. If variables are not associated, they are said to be _statistically independent_.

Several measures exist that express the strength of the association between two categorical variables. We use Phi and Cramer's V (two nominal variables, symmetric association), Goodman & Kruskals tau (two nominal variables, asymmetric association), Kendalls tau-b (two categorical ordinal variables, symmetric association), and Somers' d (two categorical ordinal variables, asymmetric association).

### Testing associations in SPSS {#associationSPSS}

#### Instructions

```{r SPSScorrelation, echo=FALSE, out.width="640px", fig.cap="(ref:correlationSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/2g3Oyfe76h0", height = "360px")
# Goal: association as combinations of scores
# Example: consumers.sav, brand awareness by advertisement exposure
# Technique: Pearson and Spearman correlations for two numeric variables
# Check data: check linearity in scatterplot, add lines for means to explain combinations of scores that determine the correlation, add regression line to point out the influence of a bivariate outlier ; Spearman's rank correlation is less sensative to bivariate outliers
# SPSS menu: Correlate > Bivariate
# Paste & Run.
# Interpret output: size and statistical significance, no confidence interval  ; use bootstrapping for a confidence interval (see other video)
# Check assumptions: for using t distribution, Pearson: histograms with normal distribution (mention don't show), Spearman: samples size over 30 
```

----

```{r SPSSregsimple, echo=FALSE, out.width="640px", fig.cap="(ref:regsimpleSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/qgOLWulSERs", height = "360px")
# Goal: asymmetric association as prediction
# Example: consumers.sav, brand awareness by advertisement exposure
# Technique: regression with confidence intervals
# SPSS menu: regression>linear with CI under Statistics.
# Paste & Run.
# Interpret output: R2, F test, predictive effect strength (b*) and change (b) with 95% confidence interval.
# Check assumptions: in chapter on moderation with regression analysis?
```

----

```{r SPSSchisqcross, echo=FALSE, out.width="640px", fig.cap="(ref:chisqcrossSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/Nqtg8q-S0k8", height = "360px")
# Goal: association as relatively frequent/infrequent category combinations
# Example: consumers.sav, word of mouth and sex
# Technique: cross-tabulation
# SPSS menu: Descriptive Statistics > Crosstabs with option Display clustered bar charts ; Statistics: chi square ; Phi and Cramer's V (two nominal variables, symmetric association), Goodman & Kruskals tau (two nominal variables, asymmetric association), Kendalls tau-b (with lambda: two categorical ordinal variables, symmetric association), and Somers' d (two categorical ordinal variables, symmetric association) ; Cells: column percentages and expected frequencies
# Paste & Run.
# Check assumptions: 2x2 table so Fisher's exact test ; larger tables: sufficient number of expected observations in each cell, a minimum of one and at least 80% above five
# Interpret output: statistical test ; association size ; association contents
```

#### Exercises

1. In the population of all consumers, is brand awareness linked to exposure to advertisements for the brand? Use [consumers.sav](http://82.196.4.233:3838/data/consumers.sav) to answer this question.

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=ad_expo brand_aw  
  /ORDER=ANALYSIS.  
* Check if the association can be linear.  
GRAPH  
  /SCATTERPLOT(BIVAR)=ad_expo WITH brand_aw  
  /MISSING=LISTWISE.  
* Correlations.  
CORRELATIONS  
  /VARIABLES=ad_expo brand_aw  
  /PRINT=TWOTAIL NOSIG  
  /MISSING=PAIRWISE.  
NONPAR CORR  
  /VARIABLES=ad_expo brand_aw  
  /PRINT=SPEARMAN TWOTAIL NOSIG  
  /MISSING=PAIRWISE.  
  
Check data:  
  
There are no impossible values that must be changed  
into missing values.  
  
Check assumptions:  
  
Can the association be linear? If we check a scatterplot  
of the two variables, the points do not clearly display  
a curved shape. But there is one point that may distort  
a linear association because it is far away from the   
other points, namely a consumer with an exposure score  
near one. If the rank correlation is substantially  
higher than the Pearson correlation, this single  
observation may be responsible.  
  
Interpret the results:  
  
Brand awareness is statistically signficantly associated  
with exposure to advertisements for the brand, r = .46,  
p < .001. More exposure tends to go together with more  
brand awareness.  
```

2. How well can we predict brand awareness with ad exposure?

```{r eval=FALSE}
SPSS syntax:  
  
* Check data.  
FREQUENCIES VARIABLES=ad_expo brand_aw  
  /ORDER=ANALYSIS.  
* Simple regression.  
REGRESSION  
  /MISSING LISTWISE  
  /STATISTICS COEFF OUTS CI(95) R ANOVA  
  /CRITERIA=PIN(.05) POUT(.10)  
  /NOORIGIN   
  /DEPENDENT brand_aw  
  /METHOD=ENTER ad_expo.  
  
Check data:  
  
There are no impossible values.  
  
Check assumptions:  
  
* We need at least twenty observations (cases) for each predictor in the
regression model. Our model contains only one predictor, so the 62 cases in
our data set suffice for using the theoretical approximation (F and t
distribution) here.
* Other assumptions will be explained in the chapter on moderation with
regression analysis, so let us not pay attention to the assumptions yet.
  
Interpret the results:  
  
Ad exposure predicts about one fifth of the   
variation in brand awareness scores, R2 = .21,   
F (1, 60) = 15.87, p < .001.  
The predictive effect of exposure to brand advertisements   
is moderately strong (b* = 0.46). An additional  
unit of exposure increases the predicted brand awareness  
with 0.4 points, t = 3.98, p < .001, 95%[0.22; 0.67].  
```

3. Does word of mouth involve women rather than men? Interpret the contents, strength, and statistical significance of the association. Have a look at Section \@ref(SPSS-exact) if you forgot which p value to interpret.

```{r eval=FALSE}
SPSS syntax:  
  
* Contingency table with chi-squared test and measure of association.  
CROSSTABS  
  /TABLES=wom BY gender  
  /FORMAT=AVALUE TABLES  
  /STATISTICS=CHISQ PHI LAMBDA   
  /CELLS=COUNT COLUMN   
  /COUNT ROUND CELL  
  /BARCHART.  
  
Check data:  
  
There are no impossible values on the two categorical   
variables.  
  
Check assumptions:  
  
This is a 2x2 contingency table so we have to use (Fisher)  
exact test. This test makes no assumptions.  
  
Interpret the results:  
  
There is no statistically significant association between  
word of mouth and sex, p = .161 (Fisher exact), Goodman &  
Kruskal tau = .05. We cannot confidently conclude that  
either females or males experience word of mouth more  
frequently.  
  
Note that a one-sided test is possible too. The p value  
of a one-sided exact test is .080 here.  
```

## The Null and Alternative Hypothesis {#null-alt}

In the preceding section, I referred to statistical hypotheses without further qualification. There are, however, at least two different statistical hypotheses: the null hypothesis (_H_~0~) and the alternative hypothesis (_H_~1~ or _H_~A~). 

For reasons that will be explained in Section \@ref(p-value), the _null hypothesis_ must equate the test statistic to a single value. For example, the statistical hypothesis that the proportion of all households in the population reached by a television station is .5 equates the population proportion to .5. 

```{r null-alt, eval=FALSE, echo=FALSE, fig.cap="From research hypothesis to statistical ypotheses."}
# Let the student formulate/compose the null and alternative hypothesis for a given research hypothesis.
# Randomly select one research hypothesis from a hard-coded list. Present it to the user and let the user compose the null and alternative hypothesis either as a string or using list boxes. If the user presses submit, the statistical hypotheses are checked or the correct hypotheses are shown.
# The list of research hypotheses contains both one-sided and two-sided hypotheses, with and without explicit (boundary) values (e.g.,  "average media literacy is 5.5" versus "there is no difference") ; all sample statistics are continuous.
# Give feedback if the student presses the Submit button.

Use Figure \@ref(fig:null-alt) to practice formulating statistical hypotheses.

1. Is the research hypothesis usually the null hypothesis or the alternative hypothesis?
```

### Alternative hypothesis

If the research hypothesis is not the null hypothesis, it is the _alternative hypothesis_. The alternative hypothesis covers all situations not covered by the null hypothesis. The null hypothesis stating that the proportion of all households in the population reached by the television station is .5 is linked to the alternative hypothesis that states the proportion is not .5. Thus, we cover all possible outcomes.  

The research hypothesis stating that the average media literacy score of all children in the population is below 5.5, is an example of an alternative hypothesis. Most of the research hypothesis examples in the previous section are alternative hypotheses because they do not equate the statistic with a particular value:  

* If opinions about immigrants are hypothesized to be different for younger and older voters, the variances are hypothesized to be unequal in the population. But the research hypothesis does not state how unequal.  

* If the celebrity endorsing a fundraising campaign makes a difference to the willingness of people to donate, the average willingness is not hypothesized to be equal for all groups but, again, the size of the difference is not specified.  

* If we hypothesize that more exposure to brand advertisements increases brand awareness, we expect the correlation or regression coefficient to be positive, that is, larger than zero. But we have not hypothesized a particular value, so the research hypothesis represents the alternative hypothesis.  

### Research hypotheses tend to be alternative hypotheses  
It is not a coincidence that most of the research hypotheses in the examples are alternative hypotheses instead of null hypotheses. This is very common in social research, even though it is not necessarily always the case, as some statistics textbooks would have us believe. Often, our theories tell us to expect differences or changes but not the size of differences or changes.  

If the research hypothesis is the alternative hypothesis, we have to formulate the null hypothesis ourselves. This is very important, because it is this hypothesis that is actually tested as we will see in a later section. Some examples:  

* A research hypothesis stating that average media literacy of children is below 5.5 is an alternative hypothesis. The associated null hypothesis would be that average media literacy is at least 5.5.

* If the alternative hypothesis states that variances or group means are unequal in the population, the null hypothesis would be that they are equal in the population.  

* An alternative hypothesis expecting a correlation between exposure and brand awareness requires the null hypothesis to state that there is no such association in the population.  

### Nil hypothesis {#nil}
Null hypotheses are quite often stating that there is no difference or no association in the population. They equate the population statistic to zero. This type of null hypothesis is called a _nil hypothesis_ or just plainly _the nil_. 

A null hypothesis on association in the population usually is a nil hypothesis, assigning the value zero to the measure of association in the population. For example, Spearman's rho or Pearson's correlation between exposure and brand awareness are hypothesized to be zero in the population. For a measure of association, zero always means that there is no association.  

This also applies to the regression coefficient ($b$ or $b^*$ in the regression equation): If it is zero, the predictor variable is useless for predicting the outcome variable. In these cases, the null hypothesis is the nil. If statistical software does not report the null hypothesis that is being tested, you may assume that it equates the parameter of interest to zero.  

## One-Sided and Two-Sided Tests {#one-twosidedtests}

The research hypothesis stating that average media literacy is below 5.5 in the population represents the alternative hypothesis because it does not fix the hypothesized population value to one number. The accompanying null hypothesis must state that the population mean is 5.5 or higher. 

This null hypothesis is slightly different from the ones we have encountered so far, which equated the population value to a single value, usually zero. If the null hypothesis equates a parameter to a single value, the null hypothesis can be rejected if the sample statistic is either too high or too low. There are two ways of rejecting the null hypothesis, so this type of hypothesis is called _two-sided_ or two-tailed.

By contrast, the null hypothesis stating that the population mean is 5.5 or higher is a _one-sided_ or one-tailed hypothesis. It can only be rejected if the sample statistic is at one side of the spectrum: only below (left-sided) or only above (right-sided) a particular value. A test of a one-sided null hypothesis is called a _one-sided test_. 

In a left-sided test of the media literacy hypothesis, the researcher is not interested in demonstrating that average media literacy among children can be larger than 5.5. Usually, she is not interested because she rules out the possibility. For instance, she believes that average media literacy among children cannot be substantially higher than 5.5. The researcher brings prior knowledge about the world to bear that has convinced her that average media literacy among children can only be lower than 5.5 on average in the population. 

If there is a possibility that the score may also be higher than 5.5 and it is deemed important to note values well over 5.5 and values well below 5.5, the research and null hypotheses should be two-sided. Then, a sample average  well above 5.5 would also have resulted in a rejection of the null hypothesis. In a left-sided test, however, a high sample outcome cannot reject the null hypothesis. 

### Boundary value as hypothesized population value  
You may wonder how a one-sided null hypothesis equates the parameter of interest with one value as it should. The special value here is 5.5. If we can reject the null hypothesis stating that the population mean is 5.5 because our sample mean is sufficiently lower than 5.5, we can also reject any hypothesis involving population means higher than 5.5. 

In other words, if you want to know if the value is not 5.5 or more, it is enough to find that it is less than 5.5. If it's less than 5.5, then you know it's also less than any number above 5.5. Therefore, we use the boundary value of a one-sided null hypothesis as the value for a one-sided test.

### One-sided -- two-sided distinction is not always relevant  
Note that the difference between one-sided and two-sided tests is only useful if we test a statistic against a particular value or if we test the difference between two groups. In the first situation, we may rule out the possibility that the population value is higher (or lower) than the hypothesized value if we have good reasons to believe that it can only be lower (or higher). In the second situation, we may expect that one group can only score higher than the other group and not the other way around.  

In contrast, we cannot meaningfully formulate a one-sided null hypothesis if we are comparing three groups or more. Even if we expect that Group A can only score higher than Group B and Group C, what about the difference between Group B and Group C? If we can't have meaningful one-sided null hypotheses, we cannot meaningfully distinguish between one-sided and two-sided null hypotheses.  

### Formulate statistical hypotheses in advance
Statistical hypotheses are important because they specify the statistic that we use in our test and they determine whether we do a two-sided or one-sided test. We have to formulate our statistical hypotheses before we have a look at our sample. After all, we want to give the sample data a fair chance to prove our hypothesis wrong. A statistical test is useless if we already know the result in the sample and adjust our hypotheses to this information.  

Finally, you should note that scientific papers usually report the research hypothesis but not the statistical hypotheses. The statistical hypotheses of a test are supposed to be known to all researchers. Make sure you know them, as well.

## p Value and Significance Level ($\alpha$) {#p-value}

The purpose of formulating a null hypothesis is that we can use the value specified in the null hypothesis as a hypothetical population value. This saves us the trouble of looking for plausible population values, which we do if we estimate a confidence interval. 

When testing a null hypothesis, we just act as if the null hypothesis is true. We pretend that the value specified in the null hypothesis is the true population value. This allows us to create one sampling distribution that will tell us how plausible our sample is if the null hypothesis would be true.  For this reason, the null hypothesis must equate the parameter to one value. 

Let us assume that average media literacy is 3.9 in our sample. According to our null hypothesis, the population average is (at least) 5.5. If average media literacy of children in the population would really be 5.5, how plausible is it to draw a sample with 3.9 (or less) as average media literacy? We can use a hypothetical sampling distribution with 5.5 as mean value to answer this question.

```{r sign-left, fig.cap="Sampling distribution of average media literacy.", echo=FALSE, out.width="420px", screenshot.opts = list(delay = 5), dev="png"}
# Display a normal curve (M = 5.5) representing the sampling distribution of average media literacy. Colour 5% of the area situated in the left tail, add 5% as text. Add a vertical line representing the sample mean, initially 3.9, and display the percentage of the area under the curve to the left of this line.
# Let the user manipulate the sample mean (range 1-10) and the population mean according to the null hypothesis (range 1-10).
knitr::include_app("http://82.196.4.233:3838/apps/sign-left/", height="400px")
```

1. Figure \@ref(fig:sign-left) shows the hypothesized population mean, the associated sampling distribution, and the sample scores (red dots) with their mean. What does the red number directly to the left of the sample mean line mean?

```{r eval=FALSE}
* It is the probability of drawing a sample with average media literacy cores
below 3.9 from a population with 5.5 as average media literacy scores. 
* This is called a (left-sided) p value.
```

2. The significance level is 5% here. Why is it marked by the blue tail in Figure \@ref(fig:sign-left)?

```{r eval=FALSE}
* The significance level is the maximum risk that we want to take to reject a
null hypothesis that is true (Type I error). This risk is a probability, so it
is expressed by a surface under the (probability density) curve.
* Here, we are only interested in sample averages that are too low, so the
entire risk is situated in the left tail.
```

3. How low must the sample mean be to have a p value below 5% (a statistically significant test result)?

```{r eval=FALSE}
* The limit(s) of the tail that repesent the significance level mark the
rejection region. The limit is called a critical value.
* In the initial example, with average population media literacy hypothesized
to be 5.5, the rejection region contains all sample mean scores below 4.31.
Any sample with average media literacy below 4.31 has a (left-sided) p value
below 5%, so it is statistically significantly different from the hypothesized
population value.
```

4. What happens to the p value of our initial sample mean (3.9) if we change the value of our null hypothesis? Is it always possible to formulate a null hypothesis such that the sample mean is statistically significant? Take some new samples to check your answer.

```{r eval=FALSE}
* If the null hypothesis changes, the average, hence the centre of the
sampling distribution moves left or right. And so does the critical value.
* If the sample mean is outside the rejection region, that is, it is larger
than the critical value, choose a higher population mean as your null
hypothesis. If it is sufficiently higher than the sampe mean, the sample mean
ends up in the rejection region under the blue tail. Then, the test is
statistically significant.
```

### p Value
If our sample statistic is a continuous variable, for instance, average media literacy, we know that it does not make sense to calculate the probability of finding the exact sample mean that we obtained. In Section \@ref(cont-random-var), we have learned that we work with p values in this situation. For example, we use the probability of finding our sample mean or a mean that is even more distant from the hypothesized population mean.

The question now is: How large must the difference be between the value that we expect according to our null hypothesis (the hypothesized population value), and the value that we observe in our sample, before we stop believing that the null hypothesis is true? If our null hypothesis expects the average media literacy of all children to be (at least) 5.5, how small should the sample mean be before we reject the null hypothesis?  

The answer is that the difference between hypothesized and sample average media literacy must be so large that it is very improbable that we would draw a sample with the observed mean from a population with the hypothesized mean. In statistical terms, the p value of our sample outcome must be very low before we reject the null hypothesis. With a very low p value, our sample is too improbable to sustain our belief that the null hypothesis is true.

### Statistical significance and rejection region
How low should we go? A commonly accepted threshold value is .05 (5%). If the p value is .05 or less, we decide that our sample is too improbable and implausible to be drawn from a population for which the null hypothesis is true. Therefore, we reject the null hypothesis and we say that the test is _statistically significant_.

The decision about the null hypothesis is simple if you have the p value. If the reported p value is lower than the significance level, we reject the null hypothesis. Otherwise, we do not reject the null hypothesis. 

> If p is low, the null must go and the test is statistically significant. 

This is the golden rule of null hypothesis testing (although some argue that the gold of this rule is fool's gold, see Chapter \@ref(crit-discus)).  

The values for the sample statistic for which the test is statistically significant, so that the null hypothesis is rejected, is called the _rejection region_. If the sample statistic is in the rejection region of a test, we reject the null hypothesis, and the test is statistically significant. 

However, we usually do not know the rejection region in terms of the sample statistic, so we ordinarily use p values to determine if a test is statistically significant. SPSS reports p values, which are sometimes referred to as _significance_ or _Sig._. 

### Conditional probability  
It is important to remember that the p value that we calculate is a probability __under the assumption that the null hypothesis is true__. Therefore, it is a _conditional probability_. 

Compare it to the probability that we throw sixes with a dice. This probability is one out of six under the assumption that the dice is unbiased. Probabilities always rest on assumptions. If the assumptions are violated, we cannot calculate probabilities. 

If the dice is biased, we don't know the probability of throwing sixes. In the same way, we have no clue whatsoever of the probability of drawing a sample like the one we have if the null hypothesis is not true in the population. This is why specifying a null hypothesis is necessary for calculating p values.

### Significance level and Type I error {#sig-typeI}
The threshold value, conventionally .05, is called the _significance level_ of the test. If the null hypothesis---for instance, average media literacy is 5.5 in the population---is true, but we accidentally draw a sample with a mean well below 5.5, our p value is smaller than the significance level and we reject the null hypothesis.  

We have to reject the null hypothesis in this situation; those are the rules of the game. However, in concluding that the null hypothesis is wrong, we make a mistake. We don’t know and don’t believe that we make this error, but we still do. This error is called a _Type I error_: rejecting a hypothesis that is actually true.  

We cannot entirely avoid this error because samples can be very different from the population from which they are drawn, as we learned in Chapter \@ref(samp-dist). Thankfully, however, we know the probability that we make this error. This probability is the significance level.  

You should understand the exact meaning of probabilities. A significance level of .05 allows five per cent of all possible samples to be so different from the population that we reject the null hypothesis even if it is true. 

In other words, if we draw a lot of samples and decide on the null hypothesis for each sample, we would reject a true null hypothesis in five per cent of our decisions. So we have a five per cent chance of making a Type I error. 

We decide on that probability when we select the confidence level of the test. We think that .05 is an acceptable probability for making this type of error. However, we do not know whether our sample belongs to the five per cent.

### p Values in one-sided tests  

In the example of average media literacy, we have only taken into account the left tail of the sampling distribution to calculate the p value. If we expect that average media literacy is below 5.5, we only consider the chance that the sample mean is below 5.5, so we do a one-sided test.  

In this case, it makes sense to care only for the left tail of the sampling distribution. The entire probability of rejecting the null hypothesis while it is actually true is located in the left tail of the sampling distribution. 

Of course, a one-sided test may also focus solely on the right tail of the sampling distribution. For example, if we hypothesize that exposure to advertisements increases brand awareness among consumers, we expect a positive correlation or regression coefficient. Our null hypothesis specifies that the correlation or regression coefficient is at most zero and we only reject it if the sample value is well above zero.  

### Significance level in two-sided tests  

What is the situation in a two-sided test, for instance, if we hypothesize that average media literacy is not 5.5 in the population? Now, there are two ways in which we may reject the null hypothesis of 5.5 average media literacy: If the sample average media literacy is sufficiently larger than 5.5 or if it is sufficiently smaller. Each way of rejecting the null hypothesis has an associated probability if the null hypothesis is true (Type I error). Their sum is the significance level.

```{r sign-two, fig.cap="How do we obtain two-sided significance levels and p values?", echo=FALSE, out.width="420px", screenshot.opts = list(delay = 5), dev="png"}
# Display a normal curve (M = 5.5, range [1, 10]) representing the sampling distribution of average media literacy. Add one range slider (initial settings such that it cuts of 5% in the left tail and 0% at the right tail) and colour the areas under the curve to the left of the lower limit and the right of the upper limit. Add a button to draw a new sample; display the samples scores and sample mean in red. Display the significance level and the total p value if one of the critical values is set to the minimum (1) or maximum (10) value (one-sided) or if the critical values are symmetrical around the hypothesized poulation mean.
knitr::include_app("http://82.196.4.233:3838/apps/sign-two/", height="400px")
```

1. In Figure \@ref(fig:sign-two), use the _Non-rejection region_ slider to determine the rejection region for a two-sided test at 5% significance level. Hint: Click a handle and then use the left and right arrow keys on your keyboard to make tiny changes to the handle's value. 

```{r eval=FALSE}
* A two-sided significance level of .05 has .025 probability in both the left
and right tail of the distribution. Move the left slider handle to 2.95 and the
right slider handle to 8.05 to obtain this situation.
* The two-sided rejection region with 5% significance level unites all sample
mean media literacy scores below 2.95 with those above 8.05.
```

2. What is the significance level of a one-sided test using the rejection area in the right tail that you have just determined? To check your answer, change one of the slider handles such that you obtain a one-sided rejection region and read out the p value.

```{r eval=FALSE}
* A one-sided test uses only one of the two tails of the sampling distribution,
the right tail in the current example. The probability of drawing a sample
with average media literacy above 8.05 is 2.5%. So this is the one-sided
significance level.
* If you want to get rid of the left tail in the rejection region, drag the left
slider handle to 1. The left tail probability is zero now, so the right tail
probability equals the total (one-sided) significance level.
* Note that this figure simplifies the situation a little. The tails of a normal
distribution never touch the horizontal axis, so there is a theoretical chance
even for scores below 1. However, media literacy is scored on a scale from 1
to 10, so scores below 1 do not make sense. As a consequence, the probability
of a score below 1 is set to zero. In this respect, the sampling distribution
deviates from the theoretical normal distribution. The latter does not exactly
capture the sampling distribution in this example.
```

3. When does Figure \@ref(fig:sign-two) indicate that a significance level is not applicable? Why is that so?

```{r eval=FALSE}
* The figure reports that a significance level is not applicabe if the two tails
do not contain the same probability but neither of them is zero.
* If neither probability is zero, we have a two-sided test. But in a two-sided
test, the total significance level is equally divided over the left and right
tails. The two probabilities, then, must be equal in a two-sided test.
```

4. When is the two-sided p value largest and when is a one-sided p value largest? Draw several samples and inspect the p values to check your answer.

```{r eval=FALSE}
* A two-sided p value is largest if the sample mean is closest to the
hypothesized population mean. The two-sided p value doubles the smallest of
the left-side or right-side p value.
* A one-sided p value is largest if the sample mean is on the end of the
distribution opposite to the tail containing the rejection region. A
right-sided test uses the right-sided p value, a left-sided test uses the
left-sided p value.
```

5. What is special about a significance level of .05?

```{r eval=FALSE}
Nothing is special about a significance level of .05. We could have used any
other cutoff value.
```

In a two-sided test, we divide the significance level by two and use half of the probability for the left tail and the other half for the right tail of the sampling distribution. If our significance level is .05, as it usually is, we use .025 as the maximum probability of finding a sample with a statistic that is so low that we would reject a null hypothesis that is true. We use the other .025 probability for drawing a sample with a statistic that is so high that we reject the null hypothesis. Together, the two .025 probabilities constitute the .050 significance level of our two-sided test.

### p Values in two-sided tests  
Just like the significance level, the p value in a two-sided test has to take into account that we may reject the null hypothesis in two different ways.  

A p value gives us the probability of drawing a sample with the value for the sample statistic that we have found in our current sample or a more extreme value. In other words, it is the probability of drawing a sample with a statistic that is at least as distant from the hypothesized population value as our current sample result. In Figure \@ref(fig:1-2sidedpvalues), the sample mean is 5.15 and we have .042 probability to find a sample mean of 5.15 or less if the null hypothesis is true.

```{r 1-2sidedpvalues, eval=TRUE, echo=FALSE, fig.cap="Halve a two-sided p value to obtain a one-sided p value, double a one-sided p value to obtain a two-sided p value."}
knitr::include_graphics("figures/CH4.1_2sidedpvalues.png")
```

In a two-sided test, the distance can go in two directions: larger than the hypothesized value or smaller. As a consequence, the p value must cover samples at opposite sides of the sampling distribution. We should not only reckon with sample means that are less than 5.15 but also with sample means that are just as much larger than the hypothesized population value. So we have to take into account a probability of .042 for the right tail of the distribution as well in Figure \@ref(fig:1-2sidedpvalues). We can double the one-sided p value to obtain the two-sided p value. And we can halve a two-sided p value to obtain the one-sided p value.  

You do not need to worry about this if your statistics software reports the type of p value that you need: one-sided or two-sided. If there is no choice between one- and two-sided tests, the reported p value is always correct.  

However, if your software reports a one-sided p value but you need a two-sided p value, you have to double the reported p value. In contrast, if the software reports a two-sided p value while you need a one-sided p value, you have to divide the reported p value by two yourself because you do not want to take the probability in the other tail into account. Statistical software usually reports two-sided p values, so this situation may occur.

By the way, the test on the sample mean is statistically significant in the left-sided test in Figure \@ref(fig:1-2sidedpvalues) (bottom) but not in the two-sided test (top). The sample mean is in the rejection region in the left-sided test because the full 5 per cent significance level is located in the left tail of the distribution.

### Unfounded one-sided null hypotheses  

Be careful if you divide a two-sided p value to obtain a one-sided p value. If your left-sided test hypothesizes that average media literacy is below 5.5 but your sample mean is above 5.5, your left-sided test can never be significant. After all, your sample result is fully in line with the null hypothesis.  

In this situation, the two-sided p value can be significant but you should never use the two-sided p value if your null hypothesis was one-sided. Changing your null hypothesis during the analysis, even from one-sided to two-sided, is cheating (see Section \@ref(cap-chance)) because you must formulate the null hypothesis before you know the sample result. If you test the significance of null hypotheses, you have to live with the fact that you excluded outcomes from your one-sided test that perhaps should not have been excluded.  

```{r nonsig-1sided, fig.cap="Is the test statistically significant?", echo=FALSE, out.width="420px", screenshot.opts = list(delay = 5), dev="png"}
# Display the sampling distribution of average media literacy as a normal curve with 5% two-sided significance areas dark blue and 5% one-sided significance areas light blue. Generate randomly one out of six possible values for the sample mean: (from left to right) in the 0-2.5% region, 2.5%-5%, 5%-50%, 50%-95%, 95%-97.5%, 97.5%-100%. Let the user select the answers (Yes/No) to three questions: Is this sample mean significant at a 5% significance level? "Left-sided test?", "Right-sided test?",  "Two-sided test?". Give feedback when the user presses the submit button. 
knitr::include_app("http://82.196.4.233:3838/apps/nonsig-1sided/", height="310px")
```

1. Practice recognizing significant test results in Figure \@ref(fig:nonsig-1sided). Draw some samples and decide if a two-sided, right-sided, and left-sided test is statistically significant at 5% significance level.

```{r eval=FALSE}
* If the sample mean is in a dark blue tail, a two-sided test is statistically
significant.
* If the sample mean is in the light or dark blue tail at the right, a
right-sided test is statistically significant but a left-sided test is not.
* If the sample mean is in the light or dark blue tail at the left, a
left-sided test is statistically significant but a right-sided test is not.
```

2. In which situation is a one-sided test not statistically significant whereas a two-sided test is statistically significant at the 5% significance level?

```{r eval=FALSE}
* If the sample mean is in the dark blue tail at the right, a two-sided test
is statistically significant but a left-sided test is not.
* If the sample mean is in the dark blue tail at the left, a two-sided test is
statistically significant but a right-sided test is not.
```

## Test Statistic and Degrees of Freedom

A theoretical probability distribution links sample outcomes such as a sample mean to probabilities by means of a _test statistic_. A test statistic is named after the theoretical probability distribution to which it belongs: _z_ for the standard-normal or _z_ distribution, _t_ for the _t_ distribution, _F_ for the _F_ distribution and, you guessed it, chi-squared for the chi-squared distribution.  

```{r crit-df, fig.cap="Sample size and critical values in a one-sample t test.", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="420px"}
# Compare app crit-values in Ch. 3.
# Draw a t distribution with mean 5.5, standard deviation 0.4, and degrees of freedom equal to selected sample size minus 1 ; x-axis with scale and labelled "Average media literacy" ; second x axis reflecting t values ; vertical lines with values for critical values (two-sided, 5% significance level) ; colour areas outside the critical values ; add a slider to adjust sample size (range [5, 50], initial setting 25) ; update the t distribution, the critical values (vertical lines), the areas outside the critical values, and the scale of the t axis if the slider position changes.
knitr::include_app("http://82.196.4.233:3838/apps/crit-df/", height="405px")
```

Figure \@ref(fig:crit-df) uses the t distribution to approximate the sampling distribution of average media literacy in a random sample of children.

1. What is the meaning of the coloured tails?
```{r eval=FALSE}
* The coloured tails represent the probabilities of drawing a sample with a
(sample) mean that differs a lot from the (true or hypothesized) population
mean. The (true or hypothesized) population mean is represented by the centre
of the distribution.
* In this case, the probabilities of the two coloured tails sum to 5%, so the
tails represent the top five per cent of samples that are most different from
the hypothesized or true population mean.
```

2. What is the meaning of t~c~?
```{r eval=FALSE}
* t~c~ is the value of the test statistic t that separates the most unlikely
or most extreme samples from the most likely samples.
* It is called the critical value (of the test statistic).
```

3. Why does the distribution become more pointed when sample size increases?
```{r eval=FALSE}
* With a larger sample, we have more information, so we have more precise
results. More sample means are close to the true population mean.
* In technical terms, the variation of sample means decreases with larger
sample size; sample means are more alike. The standard deviation of the
sampling distribution is called the standard error. A distribution of larger
samples has a smaller standard error.
```

4. Is there a fixed relation between the t values and the values for average sample media literacy? Change the sample size to find the answer.
```{r eval=FALSE}
* No, there is not a fixed relation. A particular average media literacy
score, for example, 4.0 on the bottom scale, does not always correspond to the
same test statistic (t) value on the top scale. Just change sample size to see
this.
* The t statistic expresses the difference between the (true or hypothesized)
population value and the sample value in standard errors. It translates, for
example, the difference between 5.5 (population average of media literacy) and
4.0 (media literacy in our sample) into standard errors, for instance, 0.5,
which yields a test statstic value of 1.5 / 0.5 = 3.0.
* A larger sample yields a smaller standard error. As a result, we divide the
difference in raw scores, 1.5 in the example, by a smaller number, so the
resulting test statistic is larger.
```

5. What is the relation between sample size and critical t values?
```{r eval=FALSE}
* Smaller samples have slightly larger critical t values.
```

A test statistic is calculated from the sample statistic that we want to test, for instance, the sample proportion, mean, variance, or association, but it uses the null hypothesis as well. A test statistic more or less standardizes the difference between the sample statistic and the population value that we expect under the null hypothesis. 

The exact formula and calculation of a test statistic is not important to us. Just note that the larger the difference between the observed value (sample outcome) and the hypothesized value (hypothesized population value), the more extreme the value of the test statistic, the less likely (lower p value) it is that we draw a sample with the observed outcome or an outcome even more different from the hypothesized value, and, finally, the more likely we are to reject the null hypothesis.

Actually, we reject the null hypothesis if the test statistic is in the _rejection region_. The value of the test statistic where the rejection region starts, is called the _critical value_. In Section \@ref(crit-values), we learned that 1.96 is the critical value of z for a two-sided test at 5% significance level in a standard-normal distribution. In a z test, then, a sample z value above 1.96 or below -1.96 indicates a significant test result.

Except for the standard-normal distribution, the probability distributions that we use depend on the degrees of freedom of the test. The degrees of freedom can depend on sample size, the number of groups that we compare, or the number of rows and columns in a contingency table. We don't need to worry about this. 

The t distribution depends on the degrees of freedom of the test, for example. The degrees of freedom are determined by sample size: a larger sample yields slightly lower critical values in a t distribution. For samples that are not too small, however, the critical values of t are near 2. You may have noticed this in Figure \@ref(fig:crit-df).

APA6 requires us to report the degrees of freedom. If SPSS reports the degrees of freedom, usually in a column with the header "df", you should include it between brackets after the name of the test statistic, for instance: t (18) = 0.63. Note that the _F_ test statistic has two degrees of freedom, both of which should be reported (separated by a comma), for example, _F_ (2, 87) = 3.13. 

## Test Recipe and Rules for Reporting
Testing a null hypothesis consists of several steps, which are summarized below, much like a recipe in a cookbook.

1. Specify the statistical hypotheses.  

In the first step, translate the research hypothesis into a null and alternative hypothesis. This requires choosing the right statistics for testing the research hypothesis (Section \@ref(formulating-hypotheses)) and choosing between a one-sided or two-sided test if applicable (Section \@ref(one-twosidedtests)).  

2. Select the significance level of the test.  

Before we execute the test, we have to choose the maximum probability of rejecting the null hypothesis if it is actually true. This, as we know now, is the significance level. We almost always select .05 (5%) as the significance level. If we have a very large sample, e.g., several thousands of cases, we may select a lower significance level, for instance, 0.01. See Chapter \@ref(power) for more details.  

3. Select how the sampling distribution is going to be created.  

Are you going to use bootstrapping, an exact approach, or a theoretical probability distribution? Theoretical probability distributions are the most common choice and we focus on these, here. We have to know which theoretical probability distribution can be used for which test. If you are working with statistical software, you automatically select the correct probability distribution by selecting the correct test. For example, a test on the means of two independent samples in SPSS uses the t distribution.

4. Execute the test.  

Let your statistical software calculate the p value of the test and/or the value of the test statistic. It is important that this step comes after the first three steps. The first three steps should be made without knowledge of the results in the sample.  

5. Decide on the null hypothesis.  

Reject the null hypothesis if the p value is lower than the significance level.  

6. Report the test results.  

The ultimate goal of the test is to increase our knowledge. To this end, we have to communicate our results both to fellow scientists and to the general reader who is interested in the subject of our research.  

### Reporting to fellow scientists
Fellow scientists need to be able to see the exact statistical test results. According to APA6, we should report the test statistic, the associated degrees of freedom (if any), the value of the test statistic, the p value of the test statistic, and the confidence interval (if any). APA6 requires a particular format for presenting statistical results and it demands that the results are included at the end of a sentence.  

The statistical results for a _t_ test on one mean, for example, would be:  

<center>_t_ (67) = 2.73, p = .004, 95%CI[4.13, 4.87]</center>  

* The degrees of freedom are between parentheses directly after the name of the test statistic. Chi-squared tests add sample size to the degrees of freedom, for instance: chi-squared (12, N = 89) = 23.14, p = .027.

* The value of the test statistic is 2.73 in this example.

* The p value is .004. Note that we report all results with two decimal places except probabilities, which are reported with three decimals. We are usually interested in small probabilities---less than .05---so we need the third decimal here.

* The 95% confidence interval is 4.13 to 4.87, so with 95% confidence we state that the population mean is between 4.13 and 4.87.

Not all tests produce all results reported in the example above. For example, a _z_ test does not have degrees of freedom and _F_ or chi-squared tests do not have confidence intervals. Exact tests or bootstrap tests usually do not have a test statistic. Just report the items that your statistical software produces, and give them in the correct format.

###Reporting to the general reader
For fellow scientists and especially for the general reader, it is important to read an interpretation of the results that clarifies both the subject of the test and the test results. Make sure that you tell your reader who or what the test is about:  

* What is the population that you investigate?  

* What are the variables?  

* What are the relevant sample statistics?  

* Which comparison(s) do you make?  

* Are the results statistically significant and, if so, what are the estimates for the population?  

* If the results are significant, how large are the differences or associations?  

A test on one proportion, for instance, the proportion of all households reached by a television station, could be reported as follows:  

> "The television station reaches significantly and substantially (_p_ = .61) more than half of all households in Greece, _z_ = 4.01, p < .001."  

The interpretation of this test tells us the population ("all households in Greece"), the variable ("reaching a household") and the sample statistic of interest (_p_ for proportion). It tells us that the result is statistically significant, which a fellow scientist can check with the reported p value. 

Note that the actual p value is well below .001. If we would round it to three decimals, it would become .000. This suggests that the probability is zero but there is always some probability of rejecting the null hypothesis if it is true. For this reason, APA6 wants us to report p < .001 instead of p = .000. 

Finally, the interpretation tells us that the difference from .5 is substantial. Sometimes, we can express the difference in a number, which is called the _effect size_, and give a more precise interpretation (see Chapter \@ref(power) for more information).  

If you have the value of the test statistic but not the p value, you may report the significance level of the test instead of the p value. In this case, you either report "p < .05" if the test is significant or "n.s." if the test is <i>n</i>ot <i>s</i>ignificant.  

## Relation Between Null-Hypothesis Test and Confidence Interval {#null-ci0}

The top of Figure \@ref(fig:null-ci) shows media literacy scores in a random sample of children and their average media literacy score (red). The hypothesized average media literacy in the population of children is shown on the bottom axis. The curve represents the sampling distribution if the null hypothesis is true. The coloured areas under the curve represent 2.5% of the total area each.

```{r null-ci, fig.cap="How does null hypothesis significance relate to confidence intervals?", echo=FALSE, out.width="420px", screenshot.opts = list(delay = 5), dev="png"}
# Draw three horizontal lines, the top line labeled 'population', the middle one labeled 'sampling distribution', and the bottom line labeled 'sample'. All lines have a numerical scale (1-10). Add a normal curve to the sampling distribution axis with 2.5% of each tail area coloured and the mean indicated by a vertical line extending to the population axis and labeled there by 'Mean = <number>'. Generate a sample mean within the range [4.5, 6.5] and mark it with a number on the lower line and a vertical line from the sample to well above the sampling distribution line (so it cuts through the normal curve). Add a slider to adjust the hypothesized population mean (range [3, 7]). The slider adjusts the horizontal position of the normal curve and population mean.
knitr::include_app("http://82.196.4.233:3838/apps/null-ci/", height="410px")
```

1. Would you reject the current null hypothesis with this sample?
```{r eval=FALSE}
* If the sample mean (red line) is in the blue tails of
the sampling distribution, the null hypothesis must be
rejected.
* Note that the blue tails extend infinitely away from the
hypothesized value but the density becomes quickly so small
that you can't see the blue surface everywhere.
```

2. What are the lowest and highest sample means for which the null hypothesis is _not_ rejected?
```{r eval=FALSE}
* If the sample mean (red line) is between the blue
tails, the null hypothesis is not rejected.
* So imagine that the inner boundaries of the tails extend
down to the x axis. At which mean media literacy scores
do these boundaries intersect with the x axis?
* In the initial situation with the hypothesized population
mean at 5.5, the boundary scores are just above four and
just below seven.
```

3. Why does the sampling distribution (curve) move horizontally if you change the hypothesized population mean?
```{r eval=FALSE}
* The curve moves because it represents the sampling
distribution if the null hypothesis is true. It assumes
that the true population mean is equal to the hypothesized
population mean. 
* The population mean is the expected value of the 
sampling distribution, so it is the centre of this
distribution. If we change the hypothesized mean, we
change the centre of the distribution, so it moves
to the left or right.
```

4. Use the slider to find the 95% confidence interval for the population mean given the sample mean.
```{r eval=FALSE}
* A confidence interval contains all (hypothetical)
values of the population parameter---here: the
population mean---for which we do not reject the
null hypothesis. So all hypothesized population means
that create a sampling distribution such that the sample
mean is between the critical boundaries of the tails are
part of the confidence interval.
* Change the slider such that the boundary of the right tail
coincides with the red line of the sample mean. This
hypothesized population value is the lower bound of the
95% confidence interval.
* Make the boundary of the left tail meet the sample mean
(red line): this population value is the upper bound
of the confidence interval.
```

Null-hypothesis testing and confidence-interval estimation are related. The long and the short of it is that a 95% confidence interval contains all null hypotheses that would _not_ be rejected with the current sample at the 5% significance level, two-sided.  

Remember that the probability involved in a 95% confidence interval is not the probability that the population value is within the interval. The population value is a fixed number, not a random variable with a probability distribution (at least, not in the approach to statistical inference that we follow here). For this reason, we say that we are 95% confident that the parameter lies within the 95% confidence interval but not that the population value is included in the interval with 95% probability.  

The probability refers to the sample that we have drawn. If the population value would have a value within the interval, our sample result is among the 95% samples that are most plausible to be drawn. However, this is just another way of saying that a null hypothesis with this value for the parameter is sufficiently plausible (at a 5% significance level) considering the sample that we have drawn. So we do _not_ reject the null hypothesis if it specifies any of the parameter values included in the 95% confidence interval.  

### Testing a null hypothesis with a confidence interval  
It is easy to execute a null hypothesis test if you know the confidence interval. If the population value specified in the null hypothesis is within the confidence interval, do not reject the null hypothesis. Otherwise, reject the null hypothesis.

Let us imagine, for instance, that the 95% confidence interval for average media literacy among children is 4.11 to 4.87. Our initial null hypothesis states that the average is at least 5.5. An average of 5.5 is clearly outside this 95% confidence interval, so this sample rejects the null hypothesis at the 5% significance level. 

We have already encountered this result in a previous section. Note, however, that our original test was one-sided but a confidence interval corresponds always to a two-sided test because it allows the parameter to be both smaller and larger than the sample value.  

It is also clear that any null hypothesis specifying a population mean above 5.5 would be rejected. We could already infer that from our one-sided null hypothesis test. What we could not see there, however, is that a hypothesis specifying a population mean of 4.9 or 4.0 would also have been rejected in a two-sided test at the 5% level. A confidence interval gives more information than a single null hypothesis test because it shows us test results for a range of null hypotheses. 

### Testing a null hypothesis with bootstrapping  

Using the confidence interval is the easiest and sometimes the only way of testing a null hypothesis if we create the sampling distribution with bootstrapping. For instance, we may use the median as the preferred measure of central tendency rather than the mean, if the distribution of scores is quite skewed and the sample is not very large. In this situation, a theoretical probability distribution for the sample median is not known, so we resort to bootstrapping.

```{r null-bootstrap, eval=FALSE, echo=FALSE}
# Create a (left) skewed sample of media literacy scores (N = 30, such that the sampling dsitribution is skewed?). Generate a sampling distribution of median media literacy scores and display it as a histogram (with narrow bins). Show to vertical lines for the lower and upper limit of the confidence interval and display the percentage of cases to left/middle/right of these lines. Add a range slider, so the user can set the lower and upper limits of the 95% confidence interval for the sample median.

1. Figure \@ref(fig:null-bootstrap) shows the bootstrapped sampling distribution of sample medians for media literacy of teenagers. Use the sliders to determine the 95% confidence interval of the sample median.

2. Test the null hypothesis that teenager media literacy in the population is 6.0.
```

Bootstrapping creates an empirical sampling distribution: a lot of samples with a median calculated for each sample. A confidence interval can be created from this sampling distribution (see Section \@ref(bootstrap-confidenceinterval)). If our null hypothesis about the population median is included in the 95% confidence interval, we do not reject the null hypothesis. Otherwise, we reject it.

## Capitalization on Chance {#cap-chance}

The relation between null hypothesis testing and confidence intervals may have given the impression that one could test a range of null hypotheses using just one sample and one confidence interval. For instance, we could simultaneously test the null hypotheses that average media literacy among children is 5.5, 4.5, or 3.5. Just check if these values are inside or outside the confidence interval and you are done?  

This impression is wrong. The probabilities that we calculate using one sample assume that we only apply one test to the data. If we test the original null hypothesis that average media literacy is 5.5, we run a risk of five per cent to reject the null hypothesis if the null hypothesis is true.  

If we apply a second test to the same sample, for example, testing the null hypothesis that average media literacy is 4.5, we again run this risk of five per cent. But the risk of rejecting at least one true null hypothesis increases dramatically if we do two tests. This risk is 9.75 per cent, namely one minus the risk of rejecting no true null hypothesis, which is 1 - .95 * .95 = .0975.

The situation becomes even worse if we do three or more tests on the same sample. The total level of rejecting at least one null hypothesis that is true increases well above the significance level that we want to maintain, namely five per cent.

The phenomenon that we are dealing with probabilities of making Type I errors that are higher (_inflated Type I errors_) than the significance level that we want to use, is referred to as _capitalization on chance_. Applying more than one test to the same data is one way to capitalize on chance. If you do a lot of tests on the same data, it is very rare not to find some statistically significant results even if all null hypotheses are true.  

### Capitalization on chance in post-hoc tests  
This type of capitalization on chance may occur, for example, in an analysis of variance. To test the research hypothesis that the celebrity endorsing a fundraising campaign makes a difference to people's willingness to donate, we may organize an experiment using four versions of a video clip as the treatment, each clip featuring a different celebrity endorsing the campaign. This results in four groups of participants, each group having an average score on their willingness to donate. As a first step, we test the null hypothesis that all groups have equal population means using an _F_ test (analysis of variance).  

If this test is statistically significant, we reject the null hypothesis and conclude that at least two groups have different population means. The next question is: Which groups, that is, endorsement by which celebrities, display a different willingness to donate? To answer this question, we must do post-hoc _t_ tests on pairs of groups. With four groups, we have six pairs of groups, so we have six _t_ tests on independent means. The probability of rejecting at least one true null hypothesis of no difference is much higher than five per cent if we use a significance level of five per cent for each single _t_ test.  

### Correcting for capitalization on chance  
We can correct in several ways for this type of capitalization on chance; one such way is by applying the Bonferroni correction. This correction merely divides the significance level that we use for each test by the number of tests that we do. In our example, we do six _t_ tests, so we divide the significance level of five per cent by six. The resulting significance level for each _t_ test is .008. If a _t_ test's p value is below .008, we reject the null hypothesis, but we do not reject it otherwise.

The Bonferroni correction is a rather coarse correction, which is not entirely accurate. However, it has a simple logic that directly links to the problem of capitalization on chance. Therefore, it is a good technique to help understand the problem, which is the main goal we want to attain, here. We will skip better but more complicated alternatives to Bonferroni correction.

Note that we need not apply a correction if we specify a hypothesis beforehand about the two groups that we expect to differ. In the example of celebrity endorsement, we would not have to apply the Bonferroni correction to the _t_ test on the mean difference between participants confronted to Celebrity A and Celebrity C if we had hypothesized that the willingness to donate differs here. Of course, we could have skipped the analysis of variance and gone straight to the _t_ test with such a hypothesis.

### Specifying hypotheses afterwards  
Capitalization on chance occurs if we apply different tests to the same variables in the same sample. This occurs in exploratory research in which we do not specify hypotheses beforehand but try out different predictors or different outcome variables. 

It occurs more strongly if we first have a look at our sample data and then formulate an hypothesis. Knowing the sample outcome, it is easy to specify a null hypothesis that will be rejected. This is plain cheating and it should be avoided at all times.

### Advantages of using confidence intervals  
If we cannot use the confidence interval to test a range of null hypotheses, what, then, is the added value of a confidence interval over a single null hypothesis test? The confidence interval tells us the plausible values of the parameter rather than whether or not it is one particular value. Putting it simply, we just know more from a confidence interval than from a null hypothesis test.

We could use the additional information, if we were to do a new null hypothesis test on a new sample. We would have a better idea of plausible null hypotheses. We could, for instance, test whether media literacy is 4.49, the middle of the confidence interval in our previous research project. A confidence interval helps us to be more specific in the next study addressing our topic, whether this study is carried out by ourselves or by our colleagues.

## Test Your Understanding

```{r hypo-testing, fig.cap="Testing null hypotheses.", echo=FALSE, out.width="420px", screenshot.opts = list(delay = 5), dev="png"}
# Use app null-ci.
knitr::include_app("http://82.196.4.233:3838/apps/null-ci/", height="410px")
```

Figure \@ref(fig:hypo-testing) displays a random sample of media literacy scores (red) and a sampling distribution if the null hypothesis is true.

1. What are the null and alternative hypotheses in Figure \@ref(fig:hypo-testing)? Is the null hypothesis a nil hypothesis here?
```{r eval=FALSE, echo=FALSE}
* The null hypothesis is that average media literacy
score in the population is 5.5. This is not a nil
hypothesis because the value is not zero.
* The significance level is split between the two tails
of the sampling distribution, so a two-sided test is
intended. The alternative hypothesis of this two-sided
significance test is that the population mean is NOT
5.5; it is either higher or lower than 5.5.
```

2. What represents the p value of the sample mean that we have found in Figure \@ref(fig:hypo-testing)? Does the p value depend on the type of null hypothesis: one-sided or two-sided?
```{r eval=FALSE, echo=FALSE}
* The p value of the test is the surface of the tail of the sampling
distribution that is further away from the hypothesized value than the sample
mean. Graphically speaking, it is the surface of the tail that is cut of by
the red line.
* In a one-sided test, the p value is just the surface of this tail. In a
two-sided test, it is the twice this surface because it also includes the same
part of the tail at the other side of the distribution.
* So yes, it makes a difference to the p value whether you test one-sided or
two-sided. That is, if it makes sense to distinguish between a one-sided and
two-sided test.
```

3. What represents the significance level and rejection region in Figure \@ref(fig:hypo-testing)?
```{r eval=FALSE, echo=FALSE}
* The surface of the blue tails represent the significance level of the test.
Each tail contains 2.5% of the surface, so the significane level is 5%.
* The rejection region contains the sample statistic values under the blue
tails. These are the sample average media literacy scores that differ too much
from the hypothesized population mean to believe that the hypothesized mean is
the true population mean.
```

4. Is the test statistically significant? How do you decide?
```{r eval=FALSE, echo=FALSE}
* If the sample mean (red line) is in the blue tails (rejection region) of the
sampling distribution, the null hypothesis must be rejected.
* Note that the blue tails extend infinitely away from the hypothesized value
but the density becomes quickly so small that you can't see the blue surface
everywhere.
```

5. What happens if you change the hypothesized population mean? Check your answer by using the slider.
```{r eval=FALSE, echo=FALSE}
* The curve moves horizontally because it represents the sampling distribution
if the null hypothesis is true. It assumes that the true population mean is
equal to the hypothesized population mean.
* The population mean is the expected value of the sampling distribution, so
it is the centre of this distribution. If we change the hypothesized mean, we
change the centre of the distribution, so it moves to the left or right.
```

6. Is it OK to change your null hypothesis when you know your sample mean? Why is it OK or not OK? 
```{r eval=FALSE, echo=FALSE}
* It is NOT OK to change your null hypothesis when you know your sample mean
because you can always find a null hypothesis that is statistically
significant. Just move you null hypothesis so your sample mean (red) ends up
in the (blue) region outside the critical values.
* You don't offer the data a fair chance to proof that your null hypothesis is
wrong when you (re)formulate the null hypothesis when you know your sample.
```

## Take-Home Points  

* We use a statistical test if we want to decide on a null hypothesis: reject or not reject? Usually, this boils down to the question:  "Is there or is there not an effect (difference, association) in the population?"  

* The decision rules should be specified beforehand: decide on the direction of the test (one-sided or two-sided) and the significance level.

* The null and alternative hypotheses always concern a population statistic. Together they cover all possible outcomes for the statistic. The null hypothesis always specifies one (boundary) value for the population statistic. 

* We reject the null hypothesis if a test is statistically significant. This means that the probability of drawing a sample with the current or a more extreme outcome (even more inconsistent with the null hypothesis) for the test statistic is below the significance level.

* The 95% confidence interval includes all null hypotheses that would _not_ be rejected in a two-sided test at 5% significance level. It contains the population values that are not sufficiently contradicted by the data.

* The calculated p value is only correct if the same data is used for no more than one null hypothesis test and the null hypothesis was formulated beforehand.

* If the same data is used for more null hypotheses tests, the probability of a Type I error increases. We obtain too many significant results, which is called capitalization on chance.