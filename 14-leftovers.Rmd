# Chapter 4 leftovers

















## Testing a Null Hypothesis with a Theoretical Probability Distribution

The preceding sections taught us how to conduct a significance test. Formulate a null hypothesis that equates a population characteristic (parameter) to a particular value, which is a boundary value in the case of a one-sided test. Then construct a sampling distribution with the hypothesized (boundary) value as centre and use it to calculate a _p_ value for a sample. If the _p_ value is below the significance level ($\alpha$), the test is statistically significant, so we reject the null hypothesis.

We have not discussed yet how we construct the sampling distribution. Chapter \@ref(probmodels) presented three ways: bootstrapping, an exact approach, and approximation of the sampling distribution with a theoretical probability distribution. The last option is the most popular, so let us discuss it first. Exact approaches and bootstrapping are discussed in the next section.

A theoretical probability distribution links sample outcomes such as a sample mean to probabilities by means of a _test statistic_. A test statistic is named after the theoretical probability distribution to which it belongs: _z_ for the standard-normal or _z_ distribution, _t_ for the _t_ distribution, _F_ for the _F_ distribution and, surprise, surprise, chi-squared for the chi-squared distribution.  

```{r crit-df, fig.pos='H', fig.align='center', fig.cap="Sample size and critical values in a one-sample _t_ test.", echo=FALSE, screenshot.opts = list(delay = 5), dev="png", out.width="420px"}
# Compare app crit-values in Ch. 3.
# Draw a t distribution with mean 5.5, standard deviation 0.4, and degrees of freedom equal to selected sample size minus 1 ; x-axis with scale and labelled "Average media literacy" ; second x axis reflecting t values ; vertical lines with values for critical values (two-sided, 5% significance level) ; colour areas outside the critical values ; add a slider to adjust sample size (range [5, 50], initial setting 25) ; update the t distribution, the critical values (vertical lines), the areas outside the critical values, and the scale of the t axis if the slider position changes.
knitr::include_app("https://sharon-klinkenberg.shinyapps.io/crit-df/", height="405px")
```

Figure \@ref(fig:crit-df) uses the _t_ distribution to approximate the sampling distribution of average media literacy in a random sample of children.


A test statistic is calculated from the sample statistic that we want to test, for instance, the sample proportion, mean, variance, or association, but it uses the null hypothesis as well. A test statistic more or less standardizes the difference between the sample statistic and the population value that we expect under the null hypothesis. 

The exact formula and calculation of a test statistic is not important to us. Just note that the test statistic is usually zero if the sample outcome is equal to the hypothesized population value. In Figure \@ref(fig:crit-df), for example, the _t_ value of a sample with mean 5.5 is zero if the hypothesized population mean is 5.5. The larger the difference between the observed value (sample outcome) and the expected value (hypothesized population value), the more extreme the value of the test statistic, the less likely (lower _p_ value) it is that we draw a sample with the observed outcome or an outcome even more different from the expected value, and, finally, the more likely we are to reject the null hypothesis.

We reject the null hypothesis if the test statistic is in the _rejection region_. The value of the test statistic where the rejection region starts, is called the _critical value_ of the test statistic. In Section \@ref(crit-values), we learned that 1.96 is the critical value of z for a two-sided test at five per cent significance level in a standard-normal distribution. In a z test, then, a sample z value above 1.96 or below -1.96 indicates a statistically significant test result.



## Testing a Null Hypothesis with an Exact Approach or Bootstrapping

Exact approaches calculate probabilities for discrete outcomes. In the candy example, the number of yellow candies in a sample bag of ten candies is a discrete outcome. With the binomial formula, the exact probability of zero yellow candies can be calculated, the probability of one yellow candy, two yellow candies, and so on (see Figure \@ref(fig:exactapproachfigure2)). 

```{r exactapproachfigure2, eval=TRUE, echo=FALSE, out.width="300px", fig.pos='H', fig.align='center', fig.cap="Probabilities of a sample with a particular number of yellow candies if 20 per cent of the candies are yellow in the population."}
knitr::include_graphics("figures/exactapproach.png")
```

Let us imagine that our sample bag of ten candies contains six yellow candies and we hypothesize that twenty per cent of the candies are yellow in the population. The _p_ value of our sample outcome (six yellow candies) sums the probabilities of drawing a sample bag with six, seven, eight, nine, or ten yellow candies from a population in which twenty per cent of the candies are yellow (our null hypothesis). The _p_ value happens to be (around) .007 (.006 + .001 + 0 + 0 + 0). This is the right-sided _p_ value if we assume that our hypothesis is true.

With the _p_ value, we perform the significance test as usual. The _p_ value is well below the significance level of .05, so we reject the null hypothesis that twenty per cent of all candies in the population are yellow.

The situation is slightly more complicated if we want to execute a significance test with a sampling distribution created with bootstrapping. To understand the testing procedure with bootstrapping, we first have to discuss the relation between null-hypothesis testing and confidence intervals.

### Relation between null-hypothesis tests and confidence intervals {#null-ci0}

Figure \@ref(fig:null-ci) shows media literacy scores in a random sample of children and their average media literacy score (red). The hypothesized average media literacy in the population of children is shown on the top axis. The curve represents the sampling distribution if the null hypothesis is true.

```{r null-ci, fig.pos='H', fig.align='center', fig.cap="How does null hypothesis significance relate to confidence intervals?", echo=FALSE, out.width="775px", screenshot.opts = list(delay = 5), dev="png"}
# Draw three horizontal lines, the top line labeled 'population', the middle one labeled 'sampling distribution', and the bottom line labeled 'sample'. All lines have a numerical scale (1-10). Add a normal curve to the sampling distribution axis with 2.5% of each tail area coloured and the mean indicated by a vertical line extending to the population axis and labeled there by 'Mean = <number>'. Generate a sample mean within the range [4.5, 6.5] and mark it with a number on the lower line and a vertical line from the sample to well above the sampling distribution line (so it cuts through the normal curve). Add a slider to adjust the hypothesized population mean (range [3, 7]). The slider adjusts the horizontal position of the normal curve and population mean.
knitr::include_app("https://sharon-klinkenberg.shinyapps.io/null-ci/", height="268px")
```

<A name="question4.6.1"></A>
```{block2, type='rmdquestion', echo = Qch4}
1. What are the lowest and highest hypothesized population means for which the null hypothesis is _not_ rejected? Use the slider to find the answer. [<img src="icons/2answer.png" width=115px align="right">](#answer4.6.1){.buttonToAnswer}
```

<A name="question4.6.2"></A>
```{block2, type='rmdquestion', echo = Qch4}
2. The interval between the lowest and highest hypothesized population means of Question 1 is the 95% confidence interval (see Section \@ref(ci-parameter)). Is a null hypothesis statistically significant if the hypothesized population value is within the 95% confidence interval or outside of this interval? [<img src="icons/2answer.png" width=115px align="right">](#answer4.6.2){.buttonToAnswer}
```
  
Do you remember how we constructed a confidence interval in Chapter \@ref(param-estim), Section \@ref(imag-pop-values)? We looked for all population values for which the sample outcome is sufficiently plausible. Sufficiently plausible means that our observed sample outcome is among the sample outcomes that are closest to the population value. By convention, we use a confidence level of 95 per cent, which means that our observed sample is among the 95 per cent of all samples that have outcomes closest to the population value.

But wait a minute. If the sample outcome is among the 95 per cent of samples in the middle of the sampling distribution, it is NOT among the extreme five percent of all samples. This is simply another way of saying that the observed sample outcome is not statistically significant at the five per cent significance level. A 95% confidence interval contains all null hypothesis values for which our sample outcome is not statistically significant at the 5% significance level. Confidence levels and significance levels are related.

```{block2, type='rmdimportant'}
A 95% confidence interval contains all null hypotheses that would *not* be rejected with the current sample at the 5% significance level, two-sided.
```

If we know the 95% confidence interval, we can immediately see if our null hypothesis must be rejected or not. If the population value in our null hypothesis lies within the 95% confidence interval, the null hypothesis is NOT rejected. The sample that we have drawn is sufficiently plausible if our null hypothesis is true. In contrast, we must reject the null hypothesis if the hypothesized population value is NOT in the 95% confidence interval. 

Let us assume, for example, that average media literacy in our sample is 3.0 and that the 95% confidence interval for average media literacy ranges from 1.0 to 5.0. A null hypothesis specifying 2.5 as population average must not be rejected at the five percent significance level because 2.5 is in between 1.0 and 5.0, that is, inside the 95% confidence interval. If our null hypothesis says that average media literacy in the population is 5.5, we must reject this null hypothesis because it is outside the 95% confidence interval. The null hypothesis that average media literacy in the population is 0.0 must be rejected for the same reason. 

Note that the hypothesized value can be too high or too low for the confidence interval, so a hypothesis test using a confidence interval is two-sided.

### Testing a null hypothesis with bootstrapping  

Using the confidence interval is the easiest and sometimes the only way of testing a null hypothesis if we create the sampling distribution with bootstrapping. For instance, we may use the median as the preferred measure of central tendency rather than the mean if the distribution of scores is quite skewed and the sample is not very large. In this situation, a theoretical probability distribution for the sample median is not known, so we resort to bootstrapping.

```{r null-bootstrap, eval=FALSE, echo=FALSE}
# Create a (left) skewed sample of media literacy scores (N = 30, such that the sampling dsitribution is skewed?). Generate a sampling distribution of median media literacy scores and display it as a histogram (with narrow bins). Show to vertical lines for the lower and upper limit of the confidence interval and display the percentage of cases to left/middle/right of these lines. Add a range slider, so the user can set the lower and upper limits of the 95% confidence interval for the sample median.

1. Figure \@ref(fig:null-bootstrap) shows the bootstrapped sampling distribution of sample medians for media literacy of teenagers. Use the sliders to determine the 95% confidence interval of the sample median.

2. Test the null hypothesis that teenager media literacy in the population is 6.0.
```

Bootstrapping creates an empirical sampling distribution: a lot of samples with a median calculated for each sample. A confidence interval can be created from this sampling distribution (see Section \@ref(bootstrap-confidenceinterval)). If our null hypothesis about the population median is included in the 95% confidence interval, we do not reject the null hypothesis. Otherwise, we reject it.

For the lovers of details, we add a disclaimer. A confidence interval contains all null hypotheses not rejected by our current sample if we use a normal distribution [@RefWorks:3929: 376-378], but this is not always the case if we calculate the confidence interval with the critical values of a _t_ distribution [see, for example, @smithsonCorrectConfidenceIntervals2001] or if we use a bootstrapped confidence interval. In these cases, null hypothesis values near the interval boundaries may or may not be rejected by our current sample; we do not know. A confidence interval gives us an approximate range of null hypotheses that are not rejected by our sample rather than an exact range.

```{html, echo=ch4} 
### Answers {-} 
```

<A name="answer4.6.1"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 1. 

* Change the slider such that the boundary of the right tail coincides with
the red line of the sample mean. This hypothesized population value is the
lower bound of the 95% confidence interval. It is the lowest hypothesized
population value for which the observed sample mean is not statistically
significant.
* Make the boundary of the left tail meet the sample mean (red line): this
population value is the upper bound of the confidence interval. It is the
highest hypothesized population value for which the observed sample mean is
not statistically significant. [<img src="icons/2question.png" width=161px align="right">](#question4.6.1)
```

<A name="answer4.6.2"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 2. 

* A confidence interval contains all population values---here: the population mean---for which the observed sample is among the 95% most likely samples to be drawn from such a population.
* We reject a null hypothesis if the sample is NOT among the 95% most likely samples if the null hypothesis is true. In other words, if it is among the 5% most unlikely samples. 
* So we reject null hypotheses if the hypothesized population value is OUTSIDE the confidence interval. If it is INSIDE the confidence interval, we do NOT reject the null hypothesis. [<img src="icons/2question.png" width=161px align="right">](#question4.6.2)
```
  
## Test Recipe and Rules for Reporting

Testing a null hypothesis consists of several steps, which are summarized below, much like a recipe in a cookbook.

1. Specify the statistical hypotheses.  

In the first step, translate the research hypothesis into a null and alternative hypothesis. This requires choosing the right statistic for testing the research hypothesis (Section \@ref(nullhypothesis)) and choosing between a one-sided or two-sided test if applicable (Section \@ref(one-twosidedtests)).  

2. Select the significance level of the test.  

Before we execute the test, we have to choose the maximum probability of rejecting the null hypothesis if it is actually true. This is the significance level of the test. We almost always select .05 (5%) as the significance level. If we have a very large sample, e.g., several thousands of cases, we may select a lower significance level, for instance, 0.01. See Chapter \@ref(power) for more details.  

3. Select how the sampling distribution is created.  

Are you going to use bootstrapping, an exact approach, or a theoretical probability distribution? Theoretical probability distributions are the most common choice. If you are working with statistical software, you automatically select the correct probability distribution by selecting the correct test. For example, a test on the means of two independent samples in SPSS uses the _t_ distribution.

4. Execute the test.  

Let your statistical software calculate the _p_ value of the test and/or the value of the test statistic. It is important that this step comes after the first three steps. The first three steps should be made without knowledge of the results in the sample (see Section \@ref(cap-chance)).  

5. Decide on the null hypothesis.  

Reject the null hypothesis if the _p_ value is lower than the significance level or if the sample outcome is outside the confidence interval.  

6. Report the test results.  

The ultimate goal of the test is to increase our knowledge. To this end, we have to communicate our results both to fellow scientists and to the general reader who is interested in the subject of our research.  



## Specifying Null Hypotheses in SPSS {#nullSPSS}

```{r flowchart, echo=FALSE, fig.width=9, fig.pos='H', fig.align='center', fig.cap="Flow chart for selecting a test in SPSS."}
#Create and draw flow chart for selecting tests in SPSS.
source("flowchart.R")
#Show plot.
p
#Cleanup.
rm(p, x, y)
```

Statistics such as means, proportions, variances, and correlations are calculated on variables. For translating a research hypothesis into a statistical hypothesis, the researcher has to recognize the dependent and independent variables addressed by the research hypothesis and their variable types. The main  distinction is between dichotomies (two groups), (other) categorical variables (three or more groups), and numerical variables. Once you have identified the variables, the flow chart in Figure \@ref(fig:flowchart) helps you to identify the right statistical test.

If possible, SPSS uses a theoretical probability distribution to approximate the sampling distribution. It will select the appropriate sampling distribution. In some cases, such as a test on a contingency table with two rows and two columns, SPSS automatically includes an exact test because the theoretical approximation cannot be relied on.

SPSS does not allow the user to specify the null hypothesis of a test if the test involves two or more variables. If you cannot specify the null hypothesis, SPSS uses the nil hypothesis that the population value of interest is zero. For example, SPSS tests the null hypothesis that males and females have the same average willingness to donate to a charity, that is, the mean difference is zero, if we apply an independent samples _t_ test. 

Imagine that we know from previous research that females tend to score one point higher on the willingness scale than males. It would not be very interesting to reject the nil hypothesis. Instead, we would like to test the null hypothesis that the average difference between females and males is 1.00. We cannot change the null hypothesis of a _t_ test in SPSS, but we can use the confidence interval to test this null hypothesis as explained in Section \@ref(null-ci0). 

In SPSS, the analyst has to specify the null hypothesis in tests on one variable, namely tests on one proportion, one mean, or one categorical variable. The following instructions explain how to do this.

### Specify null for binomial test

A proportion is the statistic best suited to test research hypotheses addressing the share of a category in the population. The hypothesis that a television station reaches half of all households in a country provides an example. All households in the country constitute the population. The share of the television station is the proportion or percentage of all households watching this television station.  

If we have a data set for a sample of households containing a variable indicating whether or not a household watches the television station, we can test the research hypothesis with a binomial test. The statistical null hypothesis is that the proportion of households watching the television station is 0.5 in the population.

```{r SPSSbinomial, echo=FALSE, out.width="640px", fig.pos='H', fig.align='center', fig.cap="(ref:binomialSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/tmdZpOSObco", height = "360px")

# A binomial test on a single proportion can be executed in SPSS with the command _Analyze > Nonparametric Tests > Binomial_. In the dialog , you have to enter a variable. 
# 
# If this variable is a dichotomy (it has only two values), you can leave the _Define Dichotomy_ option at "Get from data". SPSS will use the first category score that it encounters in the data set as the test category. This is tricky. It will test the sample proportion of this value against the test proportion that you specify elsewhere in this dialog.
# 
# If the variable has more than two categories or you want to be sure about the category that you use for the test, use the "Cut point" option under _Define Dichotomy_ to divide all scores into two groups. The lowest score up to and including the cut point are used as the test category.
# 
# The statistics under Options are not interesting if you just want to test a proportion.
#
# Figure shows the output. The sample proportion does not differ significantly from 0.5. In this example, we would report: "We cannot reject the hypothesis that the TV station reaches half of all households, _p_ = .784."
#
# The one-sided versus two-sided output is not discussed in the video.
```

We can also be interested in more than one category, for instance, in which regions are the households located: in the north, east, south, and west of the country? This translates into a statistical hypothesis containing two or more proportions in the population. If 30% of households in the population are situated in the west, 25 % in the south and east, and 20% in the north, we would expect these proportions in the sample if all regions are equally well-represented. Our statistical hypothesis is actually a relative frequency distribution, such as, for instance, in Table \@ref(tab:hypo-freq).

```{r hypo-freq, echo=FALSE}
knitr::kable(data.frame(Region = c("North", "East", "South", "West"), HP = c(0.20, 0.25, 0.25, 0.30)), digits = 2, caption = "Statistical hypothesis about four proportions as a frequency table.", col.names = c("Region", "Hypothesized Proportion"), booktabs = TRUE) %>%
  kable_styling(font_size = 12, full_width = F, position = "float_right",
                latex_options = c("HOLD_position"))
```

A test for this type of statistical hypothesis is called a one-sample chi-squared test. It is up to the researcher to specify the hypothesized proportions for all categories. This is not a simple task: What reasons do we have to expect particular values, say a region's share of thirty per cent of all households instead of twenty-five per cent?

The test is mainly used if researchers know the true proportions of the categories in the population from which they aimed to draw their sample. If we try to draw a sample from all citizens of a country, we usually know the frequency distribution of sex, age, educational level, and so on for all citizens from the national bureau of statistics. With the bureau's information, we can test if the respondents in our sample have the same distribution with respect to sex, age, or educational level as the population from which we tried to draw the sample; just use the official population proportions in the null hypothesis. 

If the proportions in the sample do not differ more from the known proportions in the population than we expect based on chance, the sample is _representative_ of the population _in the statistical sense_ (see Section \@ref(representative)). As always, we use the _p_ value of the test as the probability of obtaining our sample or a sample that is even more different from the null hypothesis, if the null hypothesis is true. Note that the null hypothesis now represents the (distribution in) the population from which we tried to draw our sample. We conclude that the sample is representative of this population in the statistical sense if we can _not_ reject the null hypothesis, that is, if the _p_ value is _larger_ than .05. Not rejecting the null hypothesis means that we have sufficient probability that our sample was drawn from the population that we wanted to investigate. We can now be more confident that our sample results generalize to the population that we meant to investigate.

```{r SPSSchisq1, echo=FALSE, out.width="640px", fig.pos='H', fig.align='center', fig.cap="(ref:chisq1SPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/8DAau9jFUhA", height = "360px")
# If we want to test a frequency distribution against a known or hypothesized population distribution, we must use a one-sample chi-squared test. This test is available in SPSS with the command _ANALYZE > NONPARAMETRIC TESTS > LEGACY DIALOGS > CHI SQUARE_. Select the categorical variable for which you want to test the distribution under _Test variable List_.
# 
# Select the option _All categories equal_ under _Expected Values_ if you hypothesize that all categories have the same proportions in the population. In the example, we hypothesize that households are equally distributed over the four regions. This is a plausible hypothesis if the four regions are known to contain a quarter of all households in the country or if the sample was stratified by region, that is, every region was meant to deliver the same number of households to the sample.
# 
# If the hypothesized distribution is not equal over all categories, specify the expected proportions, percentages, or sample frequencies under _Values_. You must specify an expected value for each category in the exact order in which the categories are coded. Be careful not to make mistakes. 
# 
# Although the frequencies of the four regions are not exactly the same in the sample, the hypothesis of equal population frequencies cannot be rejected, Chi-square (3) = 3.27, _p_ = .352.

```

Finally, we have the significance test on one mean, which we have used in the example of average media literacy throughout this chapter. For a numeric (interval or ratio measurement level) variable such as the 10-point scale in this example, the mean is a good measure of the distribution's center. Our statistical hypothesis would be that average media literacy score of all children in the population is (below) 5.5.  

```{r SPSS1mean, echo=FALSE, out.width="640px", fig.pos='H', fig.align='center', fig.cap="(ref:1meanSPSS)", dev="png", screenshot.opts = list(delay = 5)}
knitr::include_url("https://www.youtube.com/embed/Gupx40D5bLY", height = "360px")
# To execute a one sample t test in SPSS, use the command _ANALYZE > COMPARE MEANS > ONE SAMPLE T TEST_. Select a numeric variable in the dialog and enter the hypothesized population mean under _Test Value_. 
# 
# Media literacy is measured on a ten point scale. Is average media literacy (in the population) equal to 5.5? A one sample t test tells us that average media literacy in our sample (_M_ = 4.47, _SD_ = 1.64) is statistically significantly different from 5.5, _t_ (86) = -5.87, _p_ < .001, 95% CI [-1.38, -0.68]. We are confident that the population average media literacy score is 0.68 to 1.38 below 5.5, so somewhere between 4.12 and 4.82.

```

```{html, echo = Qch4}
### Exercises
```

<A name="question4.8.1"></A>
```{block2, type='rmdquestion', echo = Qch4}
1. Use the data set [households.sav](https://shklinkenberg.github.io/Statistical-Inference/data/households.sav) to test the research hypothesis that the TV station reaches 50 per cent of all households in the population. [<img src="icons/2answer.png" width=115px align="right">](#answer4.8.1){.buttonToAnswer}
```

<A name="question4.8.2"></A>
```{block2, type='rmdquestion', echo = Qch4}
2. Test the hypothesis that the TV station reaches at least 55 per cent of all households in the population. [<img src="icons/2answer.png" width=115px align="right">](#answer4.8.2){.buttonToAnswer}
```

<A name="question4.8.3"></A>
```{block2, type='rmdquestion', echo = Qch4}
3. Does half of the households have an income of at most 40,000? [<img src="icons/2answer.png" width=115px align="right">](#answer4.8.3){.buttonToAnswer}
```

<A name="question4.8.4"></A>
```{block2, type='rmdquestion', echo = Qch4}
4. According to information from the National Bureau of Statistics, 20 per cent of all households have incomes up to 30,000, 50 per cent have incomes between 30,000 and 50,000, and 30 per cent has incomes over 50,000. Use a test to decide if our sample is representative with respect to income. Hint: recode income first. [<img src="icons/2answer.png" width=115px align="right">](#answer4.8.4){.buttonToAnswer}
```

<A name="question4.8.5"></A>
```{block2, type='rmdquestion', echo = Qch4}
5. Use the data set [children.sav](https://shklinkenberg.github.io/Statistical-Inference/data/children.sav) to test the hypothesis that average parental supervision of the child's media use is 5.5 (on a scale from 1 to 10) in the population. [<img src="icons/2answer.png" width=115px align="right">](#answer4.8.5){.buttonToAnswer}
```

<A name="question4.8.6"></A>
```{block2, type='rmdquestion', echo = Qch4}
6. If you would test the hypothesis that average parental supervision in the population is 4.5, would the test be statistically significant according to the confidence interval reported for Exercise 5? Check your answer by carrying out the test. [<img src="icons/2answer.png" width=115px align="right">](#answer4.8.6){.buttonToAnswer}
```

```{html, echo=ch4} 
### Answers {-} 
```

<A name="answer4.8.1"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Exercise 1. 

SPSS syntax:  
  
\* Check data.  
FREQUENCIES VARIABLES=tv_reach  
  /ORDER=ANALYSIS.  
\* Binomial test.  
NPAR TESTS  
  /BINOMIAL (0.50)=tv_reach (1) 
  /MISSING ANALYSIS.  
  
Check data & assumptions:   
  
Variable tv_reach is a dichotomy as it should be for this test.  
    
Interpret the results:  
  
The proportion of households not reached in the sample (p = 0.48; double-click the binomial test output and a proportion within it to see more than one decimal place) is below fifty per cent. However, we cannot reject the null hypothesis that the TV station reaches fifty per cent of all households, _p_ = .784.

Note that SPSS applies a two-sided test if the proportion in the null
hypothesis is .50. [<img src="icons/2question.png" width=161px align="right">](#question4.8.1)
```

<A name="answer4.8.2"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Exercise 2. 

SPSS syntax:  
  
\* Check data.  
FREQUENCIES VARIABLES=tv_reach  
  /ORDER=ANALYSIS.  
\* Binomial test.  
\* Hint: Test the proportion of households not reached because   
  this is the first category: 1 - 0.55 = 0.45.  
NPAR TESTS  
  /BINOMIAL (0.45)=tv_reach (1) 
  /MISSING ANALYSIS.  
  
Check data & assumptions:   
  
Variable tv_reach is a dichotomy as it should be for this test.  
    
Interpret the results:  
  
With 52 per cent of all households reached in the sample, we cannot reject the
null hypothesis that the TV station reaches at least 55 per cent of all
households, *p* = .260 (one-sided). Perhaps, the TV station reaches at least 55 per cent in the population.

Note that SPSS applies a two-sided test if the proportion in the null
hypothesis is .50, but it applies a one-sided test in all other situations.
This is a right-sided test if the sample proportion is larger than the
hypothesized proportion (null hypothesis: the proportion in the population is at most the test proportion) and it is a left-sided test if the sample proportion is smaller than the hypothesized proportion (null hypothesis: the proportion in the population is at least the test proportion). We have the first situation here. We test if the TV station does NOT reach at most 45 per cent of all households. This is the same as testing that it reaches at least 55 per cent.  [<img src="icons/2question.png" width=161px align="right">](#question4.8.2)
```

<A name="answer4.8.3"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Exercise 3. 

SPSS syntax:  
  
\* Check data.  
FREQUENCIES VARIABLES=income  
  /ORDER=ANALYSIS.  
\* Binomial test.  
\* Use the cut off option in the binomial test.  
NPAR TESTS  
  /BINOMIAL (0.50)=income (40000)  
  /MISSING ANALYSIS.  
  
Check data:  
  
* There are no apparent impossible income values.  
  
Check assumptions:  
  
There are no assumptions for the binomial test.  
  
Interpret the results:  
  
The proportion of households with an income of at most 40,000 is significantly less than fifty per cent, *p* = .022. It is 39 per cent in the sample. [<img src="icons/2question.png" width=161px align="right">](#question4.8.3)
```

<A name="answer4.8.4"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Exercise 4. 

SPSS syntax:  
  
\* Check data.  
FREQUENCIES VARIABLES=income  
  /ORDER=ANALYSIS.  
\* Recoding income into groups.  
RECODE income (Lowest thru 30000=1) (30000  thru 50000=2)  
  (50000 thru Highest=3) INTO income_group.  
VARIABLE LABELS  income_group 'Grouped income'.  
EXECUTE.  
\* Define Variable Properties.  
\*income_group.  
VALUE LABELS income_group  
  1.00 'low'  
  2.00 'medium'  
  3.00 'high'.  
EXECUTE.  
\* one-sample chi-squared test.  
NPAR TESTS  
  /CHISQUARE=income_group  
  /EXPECTED=20 50 30  
  /MISSING ANALYSIS.  
  
There are no apparent impossible income values. 

In the RECODE command, the lower limit of a category (e.g., 30,000) equals the upper limit of the preceding category (30,000) instead of one more (30,001). In this way, values between 30,000 and 30,000, for example, 30,000.5, are not skipped. The first category includes all values up to and including 30,000. The second category includes all values larger than 30,000, for instance, 30,000.5, up to and including 50,000.
  
Check assumptions:  
  
Assumptions for the chi-squared test:

* Expected frequencies never below 1 and max 20% below 5: OK, the SPSS table
note says: "0 cells (0.0%) have expected frequencies less than 5. The minimum
expected cell frequency is 24.0."
* In this case, 0% of the cells have expected frequencies below 5, which is
less than the allowed maximum of 20%. In other words, at least 80% of the
cells have expected frequencies of 5 or more.
   
Interpret the results:  
  
The distribution of incomes over income groups in the sample does not differ
in a statistically significant way from the distribution in the population
according to the National Bureau of Statistics, *chi-squared* (2, _N_ = 120) = 3.40, *p* =
.182.
In other words, we do not have sufficient reason to believe that our sample is not
representative of the population with regards to income. [<img src="icons/2question.png" width=161px align="right">](#question4.8.4)
```

<A name="answer4.8.5"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Exercise 5. 

SPSS syntax:  
  
\* Check data.  
FREQUENCIES VARIABLES=supervision  
  /ORDER=ANALYSIS.  
\* Set imposible value (25) to missing.  
\* Define Variable Properties.  
\*supervision.  
MISSING VALUES supervision(25.00).  
EXECUTE.  
\* One-sample t test.  
T-TEST  
  /TESTVAL=5.5  
  /MISSING=ANALYSIS  
  /VARIABLES=supervision  
  /CRITERIA=CI(.95).  
  
Check data:  
      
There is one impossible value for parental supervision, namely 25. This value must be made missing.  
  
Check assumptions:  
  
Sample size (N = 86) is well over 30, so we do not have to worry about the (normal) shape of the distribution of the variable in the population.  
    
Interpret the results:  
  
In the sample, average parental supervision is 5.36 (*SD* = 1.94) on a scale from 1 to 10. We are rather confident that the true average supervision score in the population is around 5.5, more specifically, between 4.94 and 5.77. A test is not statistically significant, *t* (85) = -0.68, *p* = .498, 95% CI [4.94; 5.77].
  
Note that we have to add the confidence interval limits to the test value
(here: 5.5) to obtain the confidence interval for the population mean. SPSS
reports the confidence interval for the difference between the hypothesized
population mean and the sample mean. [<img src="icons/2question.png" width=161px align="right">](#question4.8.5)
```

<A name="answer4.8.6"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Exercise 6. 

The confidence interval reported in Exercise 5 is 95% CI [4.94; 5.77].
It tells us that all null hypotheses with a hypothesized population mean
between 4.94 and 5.77 would NOT be rejected with the current sample. All
hypotheses with population values outside this confidence interval would be
rejected by the current sample in a two-sided test with five per cent
significance level.
The null hypothesis that average parental supervision in the population is 4.5
is outside the confidence interval, so it would be rejected by the current
sample. A test with this null hypothesis would be statistically significant.
We can check this be executing the test. But note that this is only a check; we
already know the result.

SPSS syntax:  
  
\* Check data.  
FREQUENCIES VARIABLES=supervision  
  /ORDER=ANALYSIS.  
\* Set imposible value (25) to missing.  
\* Define Variable Properties.  
\*supervision.  
MISSING VALUES supervision(25.00).  
EXECUTE.  
\* One-sample t test.  
T-TEST  
  /TESTVAL=4.5  
  /MISSING=ANALYSIS  
  /VARIABLES=supervision  
  /CRITERIA=CI(.95).  
  
Check data:  
  
There is one impossible value for parental supervision, namely 25. This value must be made missing.  
  
Check assumptions:  
  
Sample size (N = 86) is well over 30, so we need not worry about the shape of the variable distribution in the population.  
  
Interpret the results:  
  
The average parental supervision score in the sample (*M* = 5.36, *SD* = 1.94) makes us doubt that the true average supervision score in the population is 4.5 or thereabouts, *t* (85) = 4.10, *p* < .001, 95% CI [4.94; 5.77].
  
Note that we obtain the same confidence interval as in Exercise 1. The confidence interval does not depend on the null hypothesis, whereas the significance test does. [<img src="icons/2question.png" width=161px align="right">](#question4.8.6)
```

## Capitalization on Chance {#cap-chance}

The relation between null hypothesis testing and confidence intervals (Section \@ref(null-ci0)) may have given the impression that we can test a range of null hypotheses using just one sample and one confidence interval. For instance, we could simultaneously test the null hypotheses that average media literacy among children is 5.5, 4.5, or 3.5. Just check if these values are inside or outside the confidence interval and we are done, right?  

This impression is wrong. The probabilities that we calculate using one sample assume that we only apply one test to the data. If we test the original null hypothesis that average media literacy is 5.5, we run a risk of five per cent to reject the null hypothesis if the null hypothesis is true. The significance level is the probability of making a Type I error (Section \@ref(sig-typeI)). 

If we apply a second test to the same sample, for example, testing the null hypothesis that average media literacy is 4.5, we again run this risk of five per cent. The probability of not rejecting a true null hypothesis is .95, so the probability of not rejecting two true null hypotheses is .95 * .95  = 0.9025. The risk of rejecting at least one true null hypothesis in two tests is 1 - 0.9025 = .0975. This risk is dramatically higher than the significance level (.05) that we want to use. The situation becomes even worse if we do three or more tests on the same sample. 

The phenomenon that we are dealing with probabilities of making Type I errors that are higher (*inflated Type I errors*) than the significance level that we want to use, is called _capitalization on chance_. Applying more than one test to the same data is one way to capitalize on chance. If you do a lot of tests on the same data, you are likely to find some statistically significant results even if all null hypotheses are true.  

### Example of capitalization on chance

This type of capitalization on chance may occur, for example, if we want to compare average media literacy among three groups: second, fourth, and sixth grade students. We can use a _t_ test to test if average media literacy among fourth grade students is higher than among second grade students. We need a second _t_ test to compare average media literacy of sixth grade students to second grade students, and a third one to compare sixth to fourth grade students.

If we execute three tests, the probability of rejecting at least one true null hypothesis of no difference is much higher than five per cent if we use a significance level of five per cent for each single _t_ test. In other words, we are more likely to obtain at least one statistically significant result than we want. 

### Correcting for capitalization on chance  

We can correct in several ways for this type of capitalization on chance; one such way is the Bonferroni correction. This correction divides the significance level that we use for each test by the number of tests that we do. In our example, we do three _t_ tests on pairs of groups, so we divide the significance level of five per cent by three. The resulting significance level for each _t_ test is .0167. If a _t_ test's _p_ value is below .0167, we reject the null hypothesis, but we do not reject it otherwise.

The Bonferroni correction is a rather coarse correction, which is not entirely accurate. However, it has a simple logic that directly links to the problem of capitalization on chance. Therefore, it is a good technique to help understand the problem, which is the main goal we want to attain, here. We will skip better, but more complicated alternatives to Bonferroni correction.

It has been argued that we do not have to apply a correction for capitalization on chance if we specify a hypothesis beforehand for each test that we execute. Formulating hypotheses does not solve the problem of capitalization on chance. The probability of rejecting at least one true null hypothesis still increases with the number of tests that we execute. If all hypotheses and associated tests are reported [(as recommended in @wassersteinASAStatementPValues2016], however, the reader of the report can evaluate capitalization on chance. If one out of twenty tests at five per cent significance level turns out to be statistically significant, this is what we would expect based on chance if all null hypotheses are true. The evidence for rejecting this null hypothesis is less convincing than if only one test was applied and that test turned out to be statistically significant.

### Specifying hypotheses afterwards

Capitalization on chance occurs if we apply different tests to the same variables in the same sample. This occurs in exploratory research in which we do not specify hypotheses beforehand but try out different independent variables or different dependent variables. 

It occurs more strongly if we first have a look at our sample data and then formulate the hypothesis. Knowing the sample outcome, it is easy to specify a null hypothesis that will be rejected. This is plain cheating and it must be avoided at all times.

<!-- ## Test Your Understanding

```{r hypo-testing, fig.pos='H', fig.align='center', fig.cap="Testing null hypotheses.", echo=FALSE, out.width="775px", screenshot.opts = list(delay = 5), dev="png"}
# Use app null-ci.
knitr::include_app("https://sharon-klinkenberg.shinyapps.io/null-ci/", height="268px")
```

Figure \@ref(fig:hypo-testing) displays a random sample of media literacy scores (red) and a sampling distribution if the null hypothesis is true.

<A name="question4.10.1"></A>
```{block2, type='rmdquestion', echo = Qch4}
1. What are the null and alternative hypotheses in Figure \@ref(fig:hypo-testing)? Is the null hypothesis a nil hypothesis here? [<img src="icons/2answer.png" width=115px align="right">](#answer4.10.1){.buttonToAnswer}
```

<A name="question4.10.2"></A>
```{block2, type='rmdquestion', echo = Qch4}
2. What represents the _p_ value of the sample mean that we have found in Figure \@ref(fig:hypo-testing)? Does the _p_ value depend on the type of null hypothesis: one-sided or two-sided? [<img src="icons/2answer.png" width=115px align="right">](#answer4.10.2){.buttonToAnswer}
```

<A name="question4.10.3"></A>
```{block2, type='rmdquestion', echo = Qch4}
3. Which parts of Figure \@ref(fig:hypo-testing) represent the significance level and rejection region? [<img src="icons/2answer.png" width=115px align="right">](#answer4.10.3){.buttonToAnswer}
```

<A name="question4.10.4"></A>
```{block2, type='rmdquestion', echo = Qch4}
4. Is the test statistically significant? How do you decide? [<img src="icons/2answer.png" width=115px align="right">](#answer4.10.4){.buttonToAnswer}
```

<A name="question4.10.5"></A>
```{block2, type='rmdquestion', echo = Qch4}
5. What happens if you change the hypothesized population mean? Check your answer by using the slider. [<img src="icons/2answer.png" width=115px align="right">](#answer4.10.5){.buttonToAnswer}
```

<A name="question4.10.6"></A>
```{block2, type='rmdquestion', echo = Qch4}
6. Is it OK to change your null hypothesis when you know your sample mean? Why is it OK or not OK? [<img src="icons/2answer.png" width=115px align="right">](#answer4.10.6){.buttonToAnswer}
```

```{html, echo=ch4} 
### Answers {-} 
```

```{block2, type='rmdanswer', echo=ch4}
Answers to the Test Your Understanding questions will be shown in the web book when the last tutor group has discussed this chapter.
```

<A name="answer4.10.1"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 1. 

* The null hypothesis is that average media literacy score in the population is 5.5. This is not a nil hypothesis because the hypothesized value is not zero.
* The significance level is split between the two tails of the sampling distribution, so a two-sided test is intended. The alternative hypothesis of this two-sided significance test is that the population mean is NOT
5.5; it is either higher or lower than 5.5. [<img src="icons/2question.png" width=161px align="right">](#question4.10.1)
```

<A name="answer4.10.2"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 2. 

* The _p_ value of the test is the surface of the tail of the sampling
distribution that is further away from the hypothesized value than the sample
mean. Graphically speaking, it is the surface of the tail that is cut of by
the red line.
* In a one-sided test, the _p_ value is just the surface of this tail. In a
two-sided test, it is the twice this surface because it also includes the same
part of the tail at the other side of the distribution.
* So yes, it makes a difference to the _p_ value whether you test one-sided or
two-sided. [<img src="icons/2question.png" width=161px align="right">](#question4.10.2)
```

<A name="answer4.10.3"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 3. 

* The surface of the blue tails represent the significance level of the test. Each tail contains 2.5% of the surface, so the significance level is 5%.
* The rejection region contains the sample statistic values under the blue tails. Graphically speaking, the rejection region consists of the segments of the horizontal axis under the blue tails. These are the sample average media literacy scores that differ too much from the hypothesized population mean to believe that the hypothesized mean is the true population mean. [<img src="icons/2question.png" width=161px align="right">](#question4.10.3)
```

<A name="answer4.10.4"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 4. 

* If the sample mean (red line) falls within a blue tail (rejection region)
of the sampling distribution, the null hypothesis must be rejected.
* Note that the blue tails extend infinitely away from the hypothesized value
but the density becomes quickly so small that you can not see the blue surface
everywhere. [<img src="icons/2question.png" width=161px align="right">](#question4.10.4)
```

<A name="answer4.10.5"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 5. 

* The curve moves horizontally because it represents the sampling distribution
if the null hypothesis is true. It assumes that the true population mean is
equal to the hypothesized population mean.
* The population mean is the expected value of the sampling distribution, so
it is the centre of this distribution. If we change the hypothesized mean, we
change the centre of the distribution, so it moves to the left or right. [<img src="icons/2question.png" width=161px align="right">](#question4.10.5)
```

<A name="answer4.10.6"></A>
```{block2, type='rmdanswer', echo=ch4}
Answer to Question 6. 

* It is **not** OK to change your null hypothesis when you know your sample mean
because you can always find a null hypothesis that is statistically
significant. Just move your null hypothesis so your sample mean (red) ends up
in the (blue) region outside the critical values.
* You do not offer the data a fair chance to prove that your null hypothesis is
wrong if you (re)formulate the null hypothesis when you know your sample.
* This would be an example of capitalization on chance. [<img src="icons/2question.png" width=161px align="right">](#question4.10.6)
```

-->

## Take-Home Points  

* We use a statistical test if we want to decide on a null hypothesis: reject or not reject?   

* The decision rules should be specified beforehand: Decide on the direction of the test (one-sided or two-sided) and the significance level.

* The null and alternative hypotheses always concern a population statistic. Together they cover all possible outcomes for the statistic. The null hypothesis always specifies one (boundary) value for the population statistic. 

* We reject the null hypothesis if a test is statistically significant. This means that the probability of drawing a sample with the current or a more extreme outcome (even more inconsistent with the null hypothesis) if the null hypothesis is true (conditional probability) is below the significance level.

* A statistically significant test does not prove that the null hypothesis is false. We can make a Type I error: rejecting a true null hypothesis.

* The 95% confidence interval includes all null hypotheses that would _not_ be rejected by our current sample in a two-sided test at five per cent significance level. It contains the population values that are not sufficiently contradicted by the sample data.

* The calculated _p_ value is only correct if the data is used for no more than one null hypothesis test and the null hypothesis was formulated beforehand.

* If the same data is used for more null hypotheses tests, the probability of a Type I error increases. We obtain too many significant results, which is called capitalization on chance.